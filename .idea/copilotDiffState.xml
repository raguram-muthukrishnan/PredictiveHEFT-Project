<?xml version="1.0" encoding="UTF-8"?>
<project version="4">
  <component name="CopilotDiffPersistence">
    <option name="pendingDiffs">
      <map>
        <entry key="$PROJECT_DIR$/WorkflowSim-1.0/examples/org/workflowsim/examples/HEFTExperiment.java">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/WorkflowSim-1.0/examples/org/workflowsim/examples/HEFTExperiment.java" />
              <option name="originalContent" value="package org.workflowsim.examples;&#10;&#10;import org.cloudbus.cloudsim.Cloudlet;&#10;import org.cloudbus.cloudsim.DatacenterCharacteristics;&#10;import org.cloudbus.cloudsim.Host;&#10;import org.cloudbus.cloudsim.Log;&#10;import org.cloudbus.cloudsim.Pe;&#10;import org.cloudbus.cloudsim.Storage;&#10;import org.cloudbus.cloudsim.VmAllocationPolicySimple;&#10;import org.cloudbus.cloudsim.VmSchedulerTimeShared;&#10;import org.cloudbus.cloudsim.core.CloudSim;&#10;import org.cloudbus.cloudsim.provisioners.BwProvisionerSimple;&#10;import org.cloudbus.cloudsim.provisioners.PeProvisionerSimple;&#10;import org.cloudbus.cloudsim.provisioners.RamProvisionerSimple;&#10;import org.workflowsim.*;&#10;import org.workflowsim.utils.ClusteringParameters;&#10;import org.workflowsim.utils.OverheadParameters;&#10;import org.workflowsim.utils.Parameters;&#10;import org.workflowsim.utils.ReplicaCatalog;&#10;&#10;import java.io.File;&#10;import java.text.DecimalFormat;&#10;import java.util.ArrayList;&#10;import java.util.Calendar;&#10;import java.util.Collections;&#10;import java.util.LinkedList;&#10;import java.util.List;&#10;&#10;/**&#10; * Standard HEFT Algorithm Experiment&#10; * This version includes the critical fix to manually pass the planner's&#10; * scheduled list to the workflow engine, ensuring all tasks are executed.&#10; */&#10;public class HEFTExperiment {&#10;&#10;    private static final String DAX_PATH = &quot;WorkflowSim-1.0/config/dax/Montage_100.xml&quot;;&#10;    private static final int VM_NUM = 5;&#10;&#10;    public static void main(String[] args) {&#10;        try {&#10;            Log.printLine(&quot;====================================================================&quot;);&#10;            Log.printLine(&quot;                    STANDARD HEFT ALGORITHM EXPERIMENT&quot;);&#10;            Log.printLine(&quot;====================================================================&quot;);&#10;            runHEFTExperiment();&#10;        } catch (Exception e) {&#10;            Log.printLine(&quot;HEFT experiment terminated due to an unexpected error.&quot;);&#10;            e.printStackTrace();&#10;        }&#10;    }&#10;&#10;    private static void runHEFTExperiment() {&#10;        try {&#10;            // 1) CloudSim initialization&#10;            CloudSim.init(1, Calendar.getInstance(), false);&#10;&#10;            // 2) Validate DAX file&#10;            File daxFile = new File(DAX_PATH);&#10;            if (!daxFile.exists()) {&#10;                Log.printLine(&quot;!ERROR!: DAX file not found at &quot; + daxFile.getAbsolutePath());&#10;                return;&#10;            }&#10;&#10;            // 3) Configure WorkflowSim parameters&#10;            OverheadParameters op = new OverheadParameters(0, null, null, null, null, 0);&#10;            ClusteringParameters cp = new ClusteringParameters(0, 0, ClusteringParameters.ClusteringMethod.NONE, null);&#10;            ReplicaCatalog.FileSystem fileSystem = ReplicaCatalog.FileSystem.SHARED;&#10;&#10;            // CORRECTED HEFT CONFIGURATION: Follow official HEFT example pattern&#10;            Log.printLine(&quot;Configuring PURE HEFT algorithm (following official pattern)...&quot;);&#10;            Parameters.init(&#10;                    VM_NUM,&#10;                    DAX_PATH,&#10;                    null,&#10;                    null,&#10;                    op,&#10;                    cp,&#10;                    Parameters.SchedulingAlgorithm.STATIC,      // CRITICAL: Use STATIC (not INVALID)&#10;                    Parameters.PlanningAlgorithm.HEFT,          // Use HEFT planning algorithm&#10;                    null,&#10;                    0&#10;            );&#10;&#10;            ReplicaCatalog.init(ReplicaCatalog.FileSystem.LOCAL); // Use LOCAL file system&#10;&#10;            // 4) Create infrastructure&#10;            WorkflowDatacenter datacenter = createDatacenter(&quot;HEFT_Datacenter&quot;);&#10;&#10;            // 5) Create workflow planner and engine FIRST (like official example)&#10;            WorkflowPlanner wfPlanner = new WorkflowPlanner(&quot;planner_0&quot;, 1);&#10;            WorkflowEngine wfEngine = wfPlanner.getWorkflowEngine();&#10;&#10;            // 6) Create VMs with correct scheduler ID (like official example)&#10;            List&lt;CondorVM&gt; vmList = createVMs(wfEngine.getSchedulerId(0), VM_NUM);&#10;            Log.printLine(&quot;Created &quot; + vmList.size() + &quot; VMs with scheduler ID: &quot; + wfEngine.getSchedulerId(0));&#10;&#10;            // 7) Submit VMs and bind datacenter (following official pattern)&#10;            wfEngine.submitVmList(vmList, 0);&#10;            wfEngine.bindSchedulerDatacenter(datacenter.getId(), 0);&#10;&#10;            // 8) Run the simulation&#10;            Log.printLine(&quot;Starting HEFT simulation...&quot;);&#10;            long startTime = System.nanoTime();&#10;&#10;            CloudSim.startSimulation();&#10;            List&lt;Job&gt; finishedJobs = wfEngine.getJobsReceivedList();&#10;            CloudSim.stopSimulation();&#10;&#10;            long endTime = System.nanoTime();&#10;            Log.printLine(&quot;HEFT simulation completed!&quot;);&#10;&#10;            // 9) Analyze and display results&#10;            analyzeHEFTResults(finishedJobs, endTime - startTime);&#10;&#10;        } catch (Exception e) {&#10;            Log.printLine(&quot;Error during HEFT experiment execution:&quot;);&#10;            e.printStackTrace();&#10;        }&#10;    }&#10;&#10;    private static void analyzeHEFTResults(List&lt;Job&gt; finishedJobs, long simulationTimeNanos) {&#10;        Log.printLine(&quot;\n====================================================================&quot;);&#10;        Log.printLine(&quot;                        HEFT ALGORITHM RESULTS&quot;);&#10;        Log.printLine(&quot;====================================================================&quot;);&#10;&#10;        if (finishedJobs == null || finishedJobs.isEmpty()) {&#10;            Log.printLine(&quot;!ERROR!: No jobs completed in HEFT simulation&quot;);&#10;            return;&#10;        }&#10;&#10;        DecimalFormat df = new DecimalFormat(&quot;###.##&quot;);&#10;        &#10;        // Basic metrics calculation&#10;        double makespan = 0.0;&#10;        double totalCost = 0.0;&#10;        double totalTurnaroundTime = 0.0;&#10;        double totalWaitingTime = 0.0;&#10;        double totalCpuTime = 0.0;&#10;&#10;        for (Job job : finishedJobs) {&#10;            if (job.getFinishTime() &gt; makespan) {&#10;                makespan = job.getFinishTime();&#10;            }&#10;            totalCost += job.getCostPerSec() * job.getActualCPUTime();&#10;            &#10;            // Performance &amp; Responsiveness Metrics&#10;            totalTurnaroundTime += (job.getFinishTime() - job.getArrivalTime());&#10;            totalWaitingTime += job.getWaitingTime();&#10;            totalCpuTime += job.getActualCPUTime();&#10;        }&#10;&#10;        // Calculate VM utilization metrics&#10;        double[] vmUtilizations = new double[VM_NUM];&#10;        double totalAvailableCpuTime = 0.0;&#10;        &#10;        // Calculate total available CPU time from all VMs&#10;        for (int i = 0; i &lt; VM_NUM; i++) {&#10;            totalAvailableCpuTime += makespan; // Each VM was available for the entire makespan&#10;        }&#10;        &#10;        // Calculate individual VM utilizations&#10;        for (Job job : finishedJobs) {&#10;            int vmId = job.getVmId();&#10;            if (vmId &gt;= 0 &amp;&amp; vmId &lt; VM_NUM) {&#10;                vmUtilizations[vmId] += job.getActualCPUTime();&#10;            }&#10;        }&#10;        &#10;        // Convert to utilization percentages and calculate standard deviation&#10;        double sumUtilization = 0.0;&#10;        for (int i = 0; i &lt; VM_NUM; i++) {&#10;            vmUtilizations[i] = (vmUtilizations[i] / makespan) * 100.0; // Convert to percentage&#10;            sumUtilization += vmUtilizations[i];&#10;        }&#10;        &#10;        double meanUtilization = sumUtilization / VM_NUM;&#10;        double sumSquaredDifferences = 0.0;&#10;        for (int i = 0; i &lt; VM_NUM; i++) {&#10;            double difference = vmUtilizations[i] - meanUtilization;&#10;            sumSquaredDifferences += difference * difference;&#10;        }&#10;        double stdDevUtilization = Math.sqrt(sumSquaredDifferences / VM_NUM);&#10;&#10;        // Display comprehensive metrics&#10;        Log.printLine(&quot;BASIC PERFORMANCE METRICS:&quot;);&#10;        Log.printLine(&quot;- Total jobs completed: &quot; + finishedJobs.size());&#10;        Log.printLine(&quot;- Makespan: &quot; + df.format(makespan) + &quot; seconds&quot;);&#10;        Log.printLine(&quot;- Total cost: $&quot; + df.format(totalCost));&#10;        &#10;        Log.printLine(&quot;\n1) PERFORMANCE &amp; RESPONSIVENESS METRICS:&quot;);&#10;        Log.printLine(&quot;- Average Task Turnaround Time: &quot; + df.format(totalTurnaroundTime / finishedJobs.size()) + &quot; seconds&quot;);&#10;        Log.printLine(&quot;- Average Task Waiting Time: &quot; + df.format(totalWaitingTime / finishedJobs.size()) + &quot; seconds&quot;);&#10;        &#10;        Log.printLine(&quot;\n2) RESOURCE UTILIZATION METRICS:&quot;);&#10;        Log.printLine(&quot;- Overall CPU Utilization: &quot; + df.format((totalCpuTime / totalAvailableCpuTime) * 100.0) + &quot;%&quot;);&#10;        Log.printLine(&quot;- Standard Deviation of VM Utilization: &quot; + df.format(stdDevUtilization) + &quot;%&quot;);&#10;        &#10;        // Display individual VM utilizations for debugging&#10;        Log.printLine(&quot;- Individual VM Utilizations:&quot;);&#10;        for (int i = 0; i &lt; VM_NUM; i++) {&#10;            Log.printLine(&quot;  VM &quot; + i + &quot;: &quot; + df.format(vmUtilizations[i]) + &quot;%&quot;);&#10;        }&#10;        &#10;        Log.printLine(&quot;\n3) COST &amp; EFFICIENCY METRICS:&quot;);&#10;        Log.printLine(&quot;- Scheduling Overhead (Planning Time): &quot; + (simulationTimeNanos / 1_000_000) + &quot; ms&quot;);&#10;        Log.printLine(&quot;- Algorithm Used: Standard HEFT&quot;);&#10;&#10;        Log.printLine(&quot;====================================================================&quot;);&#10;    }&#10;&#10;    // Infrastructure creation methods&#10;    private static WorkflowDatacenter createDatacenter(String name) throws Exception {&#10;        List&lt;Host&gt; hostList = new ArrayList&lt;&gt;();&#10;        int hostNum = Math.max(VM_NUM * 2, 10);&#10;        int pesPerHost = 4;&#10;        int mips = 2500;&#10;        int ram = 16384;&#10;        long storage = 1_000_000;&#10;        int bw = 10_000;&#10;&#10;        for (int i = 0; i &lt; hostNum; i++) {&#10;            List&lt;Pe&gt; peList = new ArrayList&lt;&gt;();&#10;            for (int j = 0; j &lt; pesPerHost; j++) {&#10;                peList.add(new Pe(j, new PeProvisionerSimple(mips)));&#10;            }&#10;            hostList.add(new Host(i, new RamProvisionerSimple(ram), new BwProvisionerSimple(bw), storage, peList, new VmSchedulerTimeShared(peList)));&#10;        }&#10;&#10;        DatacenterCharacteristics characteristics = new DatacenterCharacteristics(&quot;x86&quot;, &quot;Linux&quot;, &quot;Xen&quot;, hostList, 10.0, 3.0, 0.05, 0.001, 0.0);&#10;        return new WorkflowDatacenter(name, characteristics, new VmAllocationPolicySimple(hostList), new LinkedList&lt;Storage&gt;(), 0);&#10;    }&#10;&#10;    protected static List&lt;CondorVM&gt; createVMs(int userId, int vms) {&#10;        List&lt;CondorVM&gt; list = new ArrayList&lt;&gt;();&#10;        long size = 10_000;&#10;        int ram = 2048;&#10;        long bw = 1000;&#10;        int pesNumber = 2;&#10;        String vmm = &quot;Xen&quot;;&#10;&#10;        // Create VMs with varying MIPS ratings&#10;        int[] mipsRatings = {500, 1000, 1500, 2000, 2500}; // Low, Medium, High power&#10;&#10;        for (int i = 0; i &lt; vms; i++) {&#10;            // Cycle through the MIPS ratings for variety&#10;            int mips = mipsRatings[i % mipsRatings.length];&#10;            CondorVM vm = new CondorVM(&#10;                    i, userId, mips, pesNumber, ram, bw, size, vmm,&#10;                    new org.cloudbus.cloudsim.CloudletSchedulerSpaceShared()&#10;            );&#10;            list.add(vm);&#10;            Log.printLine(&quot;Created VM #&quot; + i + &quot; with &quot; + mips + &quot; MIPS.&quot;);&#10;        }&#10;        return list;&#10;    }&#10;}&#10;" />
              <option name="updatedContent" value="package org.workflowsim.examples;&#10;&#10;import org.cloudbus.cloudsim.Cloudlet;&#10;import org.cloudbus.cloudsim.DatacenterCharacteristics;&#10;import org.cloudbus.cloudsim.Host;&#10;import org.cloudbus.cloudsim.Log;&#10;import org.cloudbus.cloudsim.Pe;&#10;import org.cloudbus.cloudsim.Storage;&#10;import org.cloudbus.cloudsim.VmAllocationPolicySimple;&#10;import org.cloudbus.cloudsim.VmSchedulerTimeShared;&#10;import org.cloudbus.cloudsim.core.CloudSim;&#10;import org.cloudbus.cloudsim.provisioners.BwProvisionerSimple;&#10;import org.cloudbus.cloudsim.provisioners.PeProvisionerSimple;&#10;import org.cloudbus.cloudsim.provisioners.RamProvisionerSimple;&#10;import org.workflowsim.*;&#10;import org.workflowsim.utils.ClusteringParameters;&#10;import org.workflowsim.utils.OverheadParameters;&#10;import org.workflowsim.utils.Parameters;&#10;import org.workflowsim.utils.ReplicaCatalog;&#10;&#10;import java.io.File;&#10;import java.text.DecimalFormat;&#10;import java.util.ArrayList;&#10;import java.util.Calendar;&#10;import java.util.Collections;&#10;import java.util.LinkedList;&#10;import java.util.List;&#10;&#10;/**&#10; * Standard HEFT Algorithm Experiment&#10; * This version includes the critical fix to manually pass the planner's&#10; * scheduled list to the workflow engine, ensuring all tasks are executed.&#10; */&#10;public class HEFTExperiment {&#10;&#10;    private static final String DAX_PATH = &quot;WorkflowSim-1.0/config/dax/Montage_100.xml&quot;;&#10;    private static final int VM_NUM = 5;&#10;&#10;    public static void main(String[] args) {&#10;        try {&#10;            Log.printLine(&quot;====================================================================&quot;);&#10;            Log.printLine(&quot;                    STANDARD HEFT ALGORITHM EXPERIMENT&quot;);&#10;            Log.printLine(&quot;====================================================================&quot;);&#10;            runHEFTExperiment();&#10;        } catch (Exception e) {&#10;            Log.printLine(&quot;HEFT experiment terminated due to an unexpected error.&quot;);&#10;            e.printStackTrace();&#10;        }&#10;    }&#10;&#10;    private static void runHEFTExperiment() {&#10;        try {&#10;            // 1) CloudSim initialization&#10;            CloudSim.init(1, Calendar.getInstance(), false);&#10;&#10;            // 2) Validate DAX file&#10;            File daxFile = new File(DAX_PATH);&#10;            if (!daxFile.exists()) {&#10;                Log.printLine(&quot;!ERROR!: DAX file not found at &quot; + daxFile.getAbsolutePath());&#10;                return;&#10;            }&#10;&#10;            // 3) Configure WorkflowSim parameters&#10;            OverheadParameters op = new OverheadParameters(0, null, null, null, null, 0);&#10;            ClusteringParameters cp = new ClusteringParameters(0, 0, ClusteringParameters.ClusteringMethod.NONE, null);&#10;            ReplicaCatalog.FileSystem fileSystem = ReplicaCatalog.FileSystem.SHARED;&#10;&#10;            // CORRECTED HEFT CONFIGURATION: Follow official HEFT example pattern&#10;            Log.printLine(&quot;Configuring PURE HEFT algorithm (following official pattern)...&quot;);&#10;            Parameters.init(&#10;                    VM_NUM,&#10;                    DAX_PATH,&#10;                    null,&#10;                    null,&#10;                    op,&#10;                    cp,&#10;                    Parameters.SchedulingAlgorithm.STATIC,      // CRITICAL: Use STATIC (not INVALID)&#10;                    Parameters.PlanningAlgorithm.HEFT,          // Use HEFT planning algorithm&#10;                    null,&#10;                    0&#10;            );&#10;&#10;            ReplicaCatalog.init(ReplicaCatalog.FileSystem.LOCAL); // Use LOCAL file system&#10;&#10;            // 4) Create infrastructure&#10;            WorkflowDatacenter datacenter = createDatacenter(&quot;HEFT_Datacenter&quot;);&#10;&#10;            // 5) Create workflow planner and engine FIRST (like official example)&#10;            WorkflowPlanner wfPlanner = new WorkflowPlanner(&quot;planner_0&quot;, 1);&#10;            WorkflowEngine wfEngine = wfPlanner.getWorkflowEngine();&#10;&#10;            // 6) Create VMs with correct scheduler ID (like official example)&#10;            List&lt;CondorVM&gt; vmList = createVMs(wfEngine.getSchedulerId(0), VM_NUM);&#10;            Log.printLine(&quot;Created &quot; + vmList.size() + &quot; VMs with scheduler ID: &quot; + wfEngine.getSchedulerId(0));&#10;&#10;            // 7) Submit VMs and bind datacenter (following official pattern)&#10;            wfEngine.submitVmList(vmList, 0);&#10;            wfEngine.bindSchedulerDatacenter(datacenter.getId(), 0);&#10;&#10;            // 8) Run the simulation&#10;            Log.printLine(&quot;Starting HEFT simulation...&quot;);&#10;            long startTime = System.nanoTime();&#10;&#10;            CloudSim.startSimulation();&#10;            List&lt;Job&gt; finishedJobs = wfEngine.getJobsReceivedList();&#10;            CloudSim.stopSimulation();&#10;&#10;            long endTime = System.nanoTime();&#10;            Log.printLine(&quot;HEFT simulation completed!&quot;);&#10;&#10;            // 9) Analyze and display results&#10;            analyzeHEFTResults(finishedJobs, endTime - startTime);&#10;&#10;        } catch (Exception e) {&#10;            Log.printLine(&quot;Error during HEFT experiment execution:&quot;);&#10;            e.printStackTrace();&#10;        }&#10;    }&#10;&#10;    private static void analyzeHEFTResults(List&lt;Job&gt; finishedJobs, long simulationTimeNanos) {&#10;        Log.printLine(&quot;\n====================================================================&quot;);&#10;        Log.printLine(&quot;                        HEFT ALGORITHM RESULTS&quot;);&#10;        Log.printLine(&quot;====================================================================&quot;);&#10;&#10;        if (finishedJobs == null || finishedJobs.isEmpty()) {&#10;            Log.printLine(&quot;!ERROR!: No jobs completed in HEFT simulation&quot;);&#10;            return;&#10;        }&#10;&#10;        DecimalFormat df = new DecimalFormat(&quot;###.##&quot;);&#10;        &#10;        // Basic metrics calculation&#10;        double makespan = 0.0;&#10;        double totalCost = 0.0;&#10;        double totalTurnaroundTime = 0.0;&#10;        double totalWaitingTime = 0.0;&#10;        double totalCpuTime = 0.0;&#10;&#10;        for (Job job : finishedJobs) {&#10;            if (job.getFinishTime() &gt; makespan) {&#10;                makespan = job.getFinishTime();&#10;            }&#10;            totalCost += job.getCostPerSec() * job.getActualCPUTime();&#10;            &#10;            // Performance &amp; Responsiveness Metrics&#10;            totalTurnaroundTime += (job.getFinishTime() - job.getSubmissionTime());&#10;            totalWaitingTime += job.getWaitingTime();&#10;            totalCpuTime += job.getActualCPUTime();&#10;        }&#10;&#10;        // Calculate VM utilization metrics&#10;        double[] vmUtilizations = new double[VM_NUM];&#10;        double totalAvailableCpuTime = 0.0;&#10;        &#10;        // Calculate total available CPU time from all VMs&#10;        for (int i = 0; i &lt; VM_NUM; i++) {&#10;            totalAvailableCpuTime += makespan; // Each VM was available for the entire makespan&#10;        }&#10;        &#10;        // Calculate individual VM utilizations&#10;        for (Job job : finishedJobs) {&#10;            int vmId = job.getVmId();&#10;            if (vmId &gt;= 0 &amp;&amp; vmId &lt; VM_NUM) {&#10;                vmUtilizations[vmId] += job.getActualCPUTime();&#10;            }&#10;        }&#10;        &#10;        // Convert to utilization percentages and calculate standard deviation&#10;        double sumUtilization = 0.0;&#10;        for (int i = 0; i &lt; VM_NUM; i++) {&#10;            vmUtilizations[i] = (vmUtilizations[i] / makespan) * 100.0; // Convert to percentage&#10;            sumUtilization += vmUtilizations[i];&#10;        }&#10;        &#10;        double meanUtilization = sumUtilization / VM_NUM;&#10;        double sumSquaredDifferences = 0.0;&#10;        for (int i = 0; i &lt; VM_NUM; i++) {&#10;            double difference = vmUtilizations[i] - meanUtilization;&#10;            sumSquaredDifferences += difference * difference;&#10;        }&#10;        double stdDevUtilization = Math.sqrt(sumSquaredDifferences / VM_NUM);&#10;&#10;        // Display comprehensive metrics&#10;        Log.printLine(&quot;BASIC PERFORMANCE METRICS:&quot;);&#10;        Log.printLine(&quot;- Total jobs completed: &quot; + finishedJobs.size());&#10;        Log.printLine(&quot;- Makespan: &quot; + df.format(makespan) + &quot; seconds&quot;);&#10;        Log.printLine(&quot;- Total cost: $&quot; + df.format(totalCost));&#10;        &#10;        Log.printLine(&quot;\n1) PERFORMANCE &amp; RESPONSIVENESS METRICS:&quot;);&#10;        Log.printLine(&quot;- Average Task Turnaround Time: &quot; + df.format(totalTurnaroundTime / finishedJobs.size()) + &quot; seconds&quot;);&#10;        Log.printLine(&quot;- Average Task Waiting Time: &quot; + df.format(totalWaitingTime / finishedJobs.size()) + &quot; seconds&quot;);&#10;        &#10;        Log.printLine(&quot;\n2) RESOURCE UTILIZATION METRICS:&quot;);&#10;        Log.printLine(&quot;- Overall CPU Utilization: &quot; + df.format((totalCpuTime / totalAvailableCpuTime) * 100.0) + &quot;%&quot;);&#10;        Log.printLine(&quot;- Standard Deviation of VM Utilization: &quot; + df.format(stdDevUtilization) + &quot;%&quot;);&#10;        &#10;        // Display individual VM utilizations for debugging&#10;        Log.printLine(&quot;- Individual VM Utilizations:&quot;);&#10;        for (int i = 0; i &lt; VM_NUM; i++) {&#10;            Log.printLine(&quot;  VM &quot; + i + &quot;: &quot; + df.format(vmUtilizations[i]) + &quot;%&quot;);&#10;        }&#10;        &#10;        Log.printLine(&quot;\n3) COST &amp; EFFICIENCY METRICS:&quot;);&#10;        Log.printLine(&quot;- Scheduling Overhead (Planning Time): &quot; + (simulationTimeNanos / 1_000_000) + &quot; ms&quot;);&#10;        Log.printLine(&quot;- Algorithm Used: Standard HEFT&quot;);&#10;&#10;        Log.printLine(&quot;====================================================================&quot;);&#10;    }&#10;&#10;    // Infrastructure creation methods&#10;    private static WorkflowDatacenter createDatacenter(String name) throws Exception {&#10;        List&lt;Host&gt; hostList = new ArrayList&lt;&gt;();&#10;        int hostNum = Math.max(VM_NUM * 2, 10);&#10;        int pesPerHost = 4;&#10;        int mips = 2500;&#10;        int ram = 16384;&#10;        long storage = 1_000_000;&#10;        int bw = 10_000;&#10;&#10;        for (int i = 0; i &lt; hostNum; i++) {&#10;            List&lt;Pe&gt; peList = new ArrayList&lt;&gt;();&#10;            for (int j = 0; j &lt; pesPerHost; j++) {&#10;                peList.add(new Pe(j, new PeProvisionerSimple(mips)));&#10;            }&#10;            hostList.add(new Host(i, new RamProvisionerSimple(ram), new BwProvisionerSimple(bw), storage, peList, new VmSchedulerTimeShared(peList)));&#10;        }&#10;&#10;        DatacenterCharacteristics characteristics = new DatacenterCharacteristics(&quot;x86&quot;, &quot;Linux&quot;, &quot;Xen&quot;, hostList, 10.0, 3.0, 0.05, 0.001, 0.0);&#10;        return new WorkflowDatacenter(name, characteristics, new VmAllocationPolicySimple(hostList), new LinkedList&lt;Storage&gt;(), 0);&#10;    }&#10;&#10;    protected static List&lt;CondorVM&gt; createVMs(int userId, int vms) {&#10;        List&lt;CondorVM&gt; list = new ArrayList&lt;&gt;();&#10;        long size = 10_000;&#10;        int ram = 2048;&#10;        long bw = 1000;&#10;        int pesNumber = 2;&#10;        String vmm = &quot;Xen&quot;;&#10;&#10;        // Create VMs with varying MIPS ratings&#10;        int[] mipsRatings = {500, 1000, 1500, 2000, 2500}; // Low, Medium, High power&#10;&#10;        for (int i = 0; i &lt; vms; i++) {&#10;            // Cycle through the MIPS ratings for variety&#10;            int mips = mipsRatings[i % mipsRatings.length];&#10;            CondorVM vm = new CondorVM(&#10;                    i, userId, mips, pesNumber, ram, bw, size, vmm,&#10;                    new org.cloudbus.cloudsim.CloudletSchedulerSpaceShared()&#10;            );&#10;            list.add(vm);&#10;            Log.printLine(&quot;Created VM #&quot; + i + &quot; with &quot; + mips + &quot; MIPS.&quot;);&#10;        }&#10;        return list;&#10;    }&#10;}" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/WorkflowSim-1.0/examples/org/workflowsim/examples/PredictiveHEFTExperiment.java">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/WorkflowSim-1.0/examples/org/workflowsim/examples/PredictiveHEFTExperiment.java" />
              <option name="originalContent" value="package org.workflowsim.examples;&#10;&#10;import org.cloudbus.cloudsim.Cloudlet;&#10;import org.cloudbus.cloudsim.DatacenterCharacteristics;&#10;import org.cloudbus.cloudsim.Host;&#10;import org.cloudbus.cloudsim.Log;&#10;import org.cloudbus.cloudsim.Pe;&#10;import org.cloudbus.cloudsim.Storage;&#10;import org.cloudbus.cloudsim.VmAllocationPolicySimple;&#10;import org.cloudbus.cloudsim.VmSchedulerTimeShared;&#10;import org.cloudbus.cloudsim.core.CloudSim;&#10;import org.cloudbus.cloudsim.provisioners.BwProvisionerSimple;&#10;import org.cloudbus.cloudsim.provisioners.PeProvisionerSimple;&#10;import org.cloudbus.cloudsim.provisioners.RamProvisionerSimple;&#10;import org.workflowsim.*;&#10;import org.workflowsim.utils.ClusteringParameters;&#10;import org.workflowsim.utils.OverheadParameters;&#10;import org.workflowsim.utils.Parameters;&#10;import org.workflowsim.utils.ReplicaCatalog;&#10;import weka.classifiers.Classifier;&#10;import weka.core.SerializationHelper;&#10;&#10;import java.io.File;&#10;import java.text.DecimalFormat;&#10;import java.util.ArrayList;&#10;import java.util.Calendar;&#10;import java.util.Collections;&#10;import java.util.LinkedList;&#10;import java.util.List;&#10;&#10;/**&#10; * Predictive HEFT Algorithm Experiment&#10; * This version includes the critical fix to manually pass the planner's&#10; * scheduled list to the workflow engine, ensuring all tasks are executed.&#10; * Uses trained Weka model to predict task execution times for improved scheduling decisions.&#10; */&#10;public class PredictiveHEFTExperiment {&#10;&#10;    private static final String DAX_PATH = &quot;WorkflowSim-1.0/config/dax/Montage_100.xml&quot;;&#10;    private static final String MODEL_PATH = &quot;models/predictive_heft_model_5.model&quot;;&#10;    private static final int VM_NUM = 5;&#10;&#10;    public static void main(String[] args) {&#10;        try {&#10;            Log.printLine(&quot;====================================================================&quot;);&#10;            Log.printLine(&quot;                PREDICTIVE HEFT ALGORITHM EXPERIMENT&quot;);&#10;            Log.printLine(&quot;====================================================================&quot;);&#10;            runPredictiveHEFTExperiment();&#10;        } catch (Exception e) {&#10;            Log.printLine(&quot;Predictive HEFT experiment terminated due to an unexpected error.&quot;);&#10;            e.printStackTrace();&#10;        }&#10;    }&#10;&#10;    private static void runPredictiveHEFTExperiment() {&#10;        try {&#10;            // 1) CloudSim initialization&#10;            CloudSim.init(1, Calendar.getInstance(), false);&#10;&#10;            // 2) Validate DAX file&#10;            File daxFile = new File(DAX_PATH);&#10;            if (!daxFile.exists()) {&#10;                Log.printLine(&quot;!ERROR!: DAX file not found at &quot; + daxFile.getAbsolutePath());&#10;                return;&#10;            }&#10;&#10;            // 3) Load and validate ML prediction model&#10;            Classifier predictionModel = loadPredictionModel();&#10;            if (predictionModel == null) {&#10;                Log.printLine(&quot;!ERROR!: Cannot run Predictive HEFT without ML model&quot;);&#10;                return;&#10;            }&#10;&#10;            // 4) Configure WorkflowSim parameters&#10;            OverheadParameters op = new OverheadParameters(0, null, null, null, null, 0);&#10;            ClusteringParameters cp = new ClusteringParameters(0, 0, ClusteringParameters.ClusteringMethod.NONE, null);&#10;&#10;            // CORRECTED PREDICTIVE HEFT CONFIGURATION: Follow official HEFT example pattern&#10;            Log.printLine(&quot;Configuring PREDICTIVE HEFT algorithm (following official pattern)...&quot;);&#10;            Parameters.init(&#10;                    VM_NUM,&#10;                    DAX_PATH,&#10;                    null,&#10;                    null,&#10;                    op,&#10;                    cp,&#10;                    Parameters.SchedulingAlgorithm.STATIC,      // CRITICAL: Use STATIC (not INVALID)&#10;                    Parameters.PlanningAlgorithm.PREDICTIVE_HEFT,          // Use Predictive HEFT planning algorithm&#10;                    null,&#10;                    0&#10;            );&#10;&#10;            // Attach the ML prediction model&#10;            Parameters.setPredictionModel(predictionModel);&#10;            Log.printLine(&quot;✅ ML prediction model attached to Predictive HEFT algorithm&quot;);&#10;&#10;            ReplicaCatalog.init(ReplicaCatalog.FileSystem.LOCAL); // Use LOCAL file system&#10;&#10;            // 5) Create infrastructure&#10;            WorkflowDatacenter datacenter = createDatacenter(&quot;PredictiveHEFT_Datacenter&quot;);&#10;&#10;            // 6) Create workflow planner and engine FIRST (like official example)&#10;            WorkflowPlanner wfPlanner = new WorkflowPlanner(&quot;planner_0&quot;, 1);&#10;            WorkflowEngine wfEngine = wfPlanner.getWorkflowEngine();&#10;&#10;            // 7) Create VMs with correct scheduler ID (like official example)&#10;            List&lt;CondorVM&gt; vmList = createVMs(wfEngine.getSchedulerId(0), VM_NUM);&#10;            Log.printLine(&quot;Created &quot; + vmList.size() + &quot; VMs with scheduler ID: &quot; + wfEngine.getSchedulerId(0));&#10;&#10;            // 8) Submit VMs and bind datacenter (following official pattern)&#10;            wfEngine.submitVmList(vmList, 0);&#10;            wfEngine.bindSchedulerDatacenter(datacenter.getId(), 0);&#10;&#10;            // 9) Run the simulation&#10;            Log.printLine(&quot;Starting Predictive HEFT simulation with ML predictions...&quot;);&#10;            long startTime = System.nanoTime();&#10;&#10;            CloudSim.startSimulation();&#10;            List&lt;Job&gt; finishedJobs = wfEngine.getJobsReceivedList();&#10;            CloudSim.stopSimulation();&#10;&#10;            long endTime = System.nanoTime();&#10;            Log.printLine(&quot;Predictive HEFT simulation completed!&quot;);&#10;&#10;            // 10) Analyze and display results&#10;            analyzePredictiveHEFTResults(finishedJobs, endTime - startTime);&#10;&#10;        } catch (Exception e) {&#10;            Log.printLine(&quot;Error during Predictive HEFT experiment execution:&quot;);&#10;            e.printStackTrace();&#10;        }&#10;    }&#10;&#10;    private static Classifier loadPredictionModel() {&#10;        try {&#10;            File modelFile = new File(MODEL_PATH);&#10;            if (!modelFile.exists()) {&#10;                Log.printLine(&quot;!ERROR!: ML model file not found at: &quot; + modelFile.getAbsolutePath());&#10;                Log.printLine(&quot;Please ensure the trained Weka model is available at: &quot; + MODEL_PATH);&#10;                return null;&#10;            }&#10;&#10;            Log.printLine(&quot;Loading ML prediction model from: &quot; + modelFile.getAbsolutePath());&#10;            Classifier classifier = (Classifier) SerializationHelper.read(MODEL_PATH);&#10;            Log.printLine(&quot;✅ ML prediction model loaded successfully&quot;);&#10;            Log.printLine(&quot;Model type: &quot; + classifier.getClass().getSimpleName());&#10;&#10;            return classifier;&#10;&#10;        } catch (Exception e) {&#10;            Log.printLine(&quot;!ERROR!: Failed to load ML prediction model&quot;);&#10;            Log.printLine(&quot;Error details: &quot; + e.getMessage());&#10;            e.printStackTrace();&#10;            return null;&#10;        }&#10;    }&#10;&#10;    private static void analyzePredictiveHEFTResults(List&lt;Job&gt; finishedJobs, long simulationTimeNanos) {&#10;        Log.printLine(&quot;\n====================================================================&quot;);&#10;        Log.printLine(&quot;                   PREDICTIVE HEFT ALGORITHM RESULTS&quot;);&#10;        Log.printLine(&quot;====================================================================&quot;);&#10;&#10;        if (finishedJobs == null || finishedJobs.isEmpty()) {&#10;            Log.printLine(&quot;!ERROR!: No jobs completed in Predictive HEFT simulation&quot;);&#10;            return;&#10;        }&#10;&#10;        DecimalFormat df = new DecimalFormat(&quot;###.##&quot;);&#10;        &#10;        // Basic metrics calculation&#10;        double makespan = 0.0;&#10;        double totalCost = 0.0;&#10;        double totalTurnaroundTime = 0.0;&#10;        double totalWaitingTime = 0.0;&#10;        double totalCpuTime = 0.0;&#10;&#10;        for (Job job : finishedJobs) {&#10;            if (job.getFinishTime() &gt; makespan) {&#10;                makespan = job.getFinishTime();&#10;            }&#10;            totalCost += job.getCostPerSec() * job.getActualCPUTime();&#10;            &#10;            // Performance &amp; Responsiveness Metrics&#10;            totalTurnaroundTime += (job.getFinishTime() - job.getArrivalTime());&#10;            totalWaitingTime += job.getWaitingTime();&#10;            totalCpuTime += job.getActualCPUTime();&#10;        }&#10;&#10;        // Calculate VM utilization metrics&#10;        double[] vmUtilizations = new double[VM_NUM];&#10;        double totalAvailableCpuTime = 0.0;&#10;        &#10;        // Calculate total available CPU time from all VMs&#10;        for (int i = 0; i &lt; VM_NUM; i++) {&#10;            totalAvailableCpuTime += makespan; // Each VM was available for the entire makespan&#10;        }&#10;        &#10;        // Calculate individual VM utilizations&#10;        for (Job job : finishedJobs) {&#10;            int vmId = job.getVmId();&#10;            if (vmId &gt;= 0 &amp;&amp; vmId &lt; VM_NUM) {&#10;                vmUtilizations[vmId] += job.getActualCPUTime();&#10;            }&#10;        }&#10;        &#10;        // Convert to utilization percentages and calculate standard deviation&#10;        double sumUtilization = 0.0;&#10;        for (int i = 0; i &lt; VM_NUM; i++) {&#10;            vmUtilizations[i] = (vmUtilizations[i] / makespan) * 100.0; // Convert to percentage&#10;            sumUtilization += vmUtilizations[i];&#10;        }&#10;        &#10;        double meanUtilization = sumUtilization / VM_NUM;&#10;        double sumSquaredDifferences = 0.0;&#10;        for (int i = 0; i &lt; VM_NUM; i++) {&#10;            double difference = vmUtilizations[i] - meanUtilization;&#10;            sumSquaredDifferences += difference * difference;&#10;        }&#10;        double stdDevUtilization = Math.sqrt(sumSquaredDifferences / VM_NUM);&#10;&#10;        // Display comprehensive metrics&#10;        Log.printLine(&quot;BASIC PERFORMANCE METRICS:&quot;);&#10;        Log.printLine(&quot;- Total jobs completed: &quot; + finishedJobs.size());&#10;        Log.printLine(&quot;- Makespan: &quot; + df.format(makespan) + &quot; seconds&quot;);&#10;        Log.printLine(&quot;- Total cost: $&quot; + df.format(totalCost));&#10;        &#10;        Log.printLine(&quot;\n1) PERFORMANCE &amp; RESPONSIVENESS METRICS:&quot;);&#10;        Log.printLine(&quot;- Average Task Turnaround Time: &quot; + df.format(totalTurnaroundTime / finishedJobs.size()) + &quot; seconds&quot;);&#10;        Log.printLine(&quot;- Average Task Waiting Time: &quot; + df.format(totalWaitingTime / finishedJobs.size()) + &quot; seconds&quot;);&#10;        &#10;        Log.printLine(&quot;\n2) RESOURCE UTILIZATION METRICS:&quot;);&#10;        Log.printLine(&quot;- Overall CPU Utilization: &quot; + df.format((totalCpuTime / totalAvailableCpuTime) * 100.0) + &quot;%&quot;);&#10;        Log.printLine(&quot;- Standard Deviation of VM Utilization: &quot; + df.format(stdDevUtilization) + &quot;%&quot;);&#10;        &#10;        // Display individual VM utilizations for debugging&#10;        Log.printLine(&quot;- Individual VM Utilizations:&quot;);&#10;        for (int i = 0; i &lt; VM_NUM; i++) {&#10;            Log.printLine(&quot;  VM &quot; + i + &quot;: &quot; + df.format(vmUtilizations[i]) + &quot;%&quot;);&#10;        }&#10;        &#10;        Log.printLine(&quot;\n3) COST &amp; EFFICIENCY METRICS:&quot;);&#10;        Log.printLine(&quot;- Scheduling Overhead (Planning Time): &quot; + (simulationTimeNanos / 1_000_000) + &quot; ms&quot;);&#10;        Log.printLine(&quot;- Algorithm Used: Predictive HEFT with Load Balancing&quot;);&#10;&#10;        // ML-specific analysis&#10;        analyzeMachineLearningImpact(finishedJobs, vmUtilizations, stdDevUtilization);&#10;&#10;        Log.printLine(&quot;====================================================================&quot;);&#10;    }&#10;&#10;    private static void analyzeMachineLearningImpact(List&lt;Job&gt; finishedJobs, double[] vmUtilizations, double stdDevUtilization) {&#10;        Log.printLine(&quot;\n4) MACHINE LEARNING &amp; LOAD BALANCING IMPACT ANALYSIS:&quot;);&#10;        Log.printLine(&quot;- ML Model: Successfully integrated with Predictive HEFT&quot;);&#10;        Log.printLine(&quot;- Prediction-based scheduling: &quot; + finishedJobs.size() + &quot; jobs scheduled using ML predictions&quot;);&#10;        Log.printLine(&quot;- Enhanced decision making: Task execution time predictions used for optimal VM assignment&quot;);&#10;&#10;        DecimalFormat df = new DecimalFormat(&quot;###.##&quot;);&#10;        &#10;        // Load balancing effectiveness analysis&#10;        Log.printLine(&quot;\nLOAD BALANCING EFFECTIVENESS:&quot;);&#10;        if (stdDevUtilization &lt; 5.0) {&#10;            Log.printLine(&quot;✅ EXCELLENT LOAD BALANCING: Very even distribution across VMs (σ = &quot; + df.format(stdDevUtilization) + &quot;%)&quot;);&#10;        } else if (stdDevUtilization &lt; 10.0) {&#10;            Log.printLine(&quot;✅ GOOD LOAD BALANCING: Reasonably even distribution across VMs (σ = &quot; + df.format(stdDevUtilization) + &quot;%)&quot;);&#10;        } else if (stdDevUtilization &lt; 20.0) {&#10;            Log.printLine(&quot;⚠️  MODERATE LOAD BALANCING: Some imbalance detected (σ = &quot; + df.format(stdDevUtilization) + &quot;%)&quot;);&#10;        } else {&#10;            Log.printLine(&quot;❌ POOR LOAD BALANCING: Significant imbalance across VMs (σ = &quot; + df.format(stdDevUtilization) + &quot;%)&quot;);&#10;        }&#10;&#10;        // Find min and max utilized VMs&#10;        double minUtilization = Double.MAX_VALUE;&#10;        double maxUtilization = 0.0;&#10;        int minVM = -1, maxVM = -1;&#10;        &#10;        for (int i = 0; i &lt; vmUtilizations.length; i++) {&#10;            if (vmUtilizations[i] &lt; minUtilization) {&#10;                minUtilization = vmUtilizations[i];&#10;                minVM = i;&#10;            }&#10;            if (vmUtilizations[i] &gt; maxUtilization) {&#10;                maxUtilization = vmUtilizations[i];&#10;                maxVM = i;&#10;            }&#10;        }&#10;        &#10;        double utilizationRange = maxUtilization - minUtilization;&#10;        Log.printLine(&quot;- Utilization Range: &quot; + df.format(utilizationRange) + &quot;% (VM&quot; + maxVM + &quot;: &quot; + &#10;                     df.format(maxUtilization) + &quot;% → VM&quot; + minVM + &quot;: &quot; + df.format(minUtilization) + &quot;%)&quot;);&#10;&#10;        // Calculate scheduling efficiency metrics&#10;        double avgExecutionTime = finishedJobs.stream()&#10;                .mapToDouble(Job::getActualCPUTime)&#10;                .average()&#10;                .orElse(0.0);&#10;&#10;        double avgWaitTime = finishedJobs.stream()&#10;                .mapToDouble(Job::getWaitingTime)&#10;                .average()&#10;                .orElse(0.0);&#10;&#10;        Log.printLine(&quot;\nPREDICTIVE SCHEDULING EFFICIENCY:&quot;);&#10;        Log.printLine(&quot;- Average execution time per job: &quot; + df.format(avgExecutionTime) + &quot;s&quot;);&#10;        Log.printLine(&quot;- Average waiting time per job: &quot; + df.format(avgWaitTime) + &quot;s&quot;);&#10;        &#10;        double efficiencyRatio = avgWaitTime / avgExecutionTime;&#10;        if (efficiencyRatio &lt; 0.1) {&#10;            Log.printLine(&quot;✅ EXCELLENT EFFICIENCY: Very low waiting times indicate highly effective ML predictions&quot;);&#10;        } else if (efficiencyRatio &lt; 0.2) {&#10;            Log.printLine(&quot;✅ GOOD EFFICIENCY: Reasonable waiting times with ML-enhanced scheduling&quot;);&#10;        } else {&#10;            Log.printLine(&quot;⚠️  EFFICIENCY CONCERN: Consider retraining ML model for better predictions&quot;);&#10;        }&#10;&#10;        // Resource distribution analysis&#10;        int[] jobsPerVM = new int[VM_NUM];&#10;        double[] timePerVM = new double[VM_NUM];&#10;&#10;        for (Job job : finishedJobs) {&#10;            int vmId = job.getVmId();&#10;            if (vmId &gt;= 0 &amp;&amp; vmId &lt; VM_NUM) {&#10;                jobsPerVM[vmId]++;&#10;                timePerVM[vmId] += job.getActualCPUTime();&#10;            }&#10;        }&#10;&#10;        Log.printLine(&quot;\nJOB DISTRIBUTION ANALYSIS:&quot;);&#10;        for (int i = 0; i &lt; VM_NUM; i++) {&#10;            Log.printLine(&quot;- VM &quot; + i + &quot;: &quot; + jobsPerVM[i] + &quot; jobs, &quot; +&#10;                         df.format(timePerVM[i]) + &quot;s total execution time&quot;);&#10;        }&#10;    }&#10;&#10;    // Infrastructure creation methods&#10;    private static WorkflowDatacenter createDatacenter(String name) throws Exception {&#10;        List&lt;Host&gt; hostList = new ArrayList&lt;&gt;();&#10;        int hostNum = Math.max(VM_NUM * 2, 10);&#10;        int pesPerHost = 4;&#10;        int mips = 2500;&#10;        int ram = 16384;&#10;        long storage = 1_000_000;&#10;        int bw = 10_000;&#10;&#10;        for (int i = 0; i &lt; hostNum; i++) {&#10;            List&lt;Pe&gt; peList = new ArrayList&lt;&gt;();&#10;            for (int j = 0; j &lt; pesPerHost; j++) {&#10;                peList.add(new Pe(j, new PeProvisionerSimple(mips)));&#10;            }&#10;            hostList.add(new Host(i, new RamProvisionerSimple(ram), new BwProvisionerSimple(bw), storage, peList, new VmSchedulerTimeShared(peList)));&#10;        }&#10;&#10;        DatacenterCharacteristics characteristics = new DatacenterCharacteristics(&quot;x86&quot;, &quot;Linux&quot;, &quot;Xen&quot;, hostList, 10.0, 3.0, 0.05, 0.001, 0.0);&#10;        return new WorkflowDatacenter(name, characteristics, new VmAllocationPolicySimple(hostList), new LinkedList&lt;Storage&gt;(), 0);&#10;    }&#10;&#10;    private static List&lt;CondorVM&gt; createVMs(int userId, int vms) {&#10;        List&lt;CondorVM&gt; list = new ArrayList&lt;&gt;();&#10;        long size = 10_000;&#10;        int ram = 2048;&#10;        long bw = 1000;&#10;        int pesNumber = 2;&#10;        String vmm = &quot;Xen&quot;;&#10;&#10;        // Create VMs with varying MIPS ratings&#10;        int[] mipsRatings = {500, 1000, 1500, 2000, 2500}; // Low, Medium, High power&#10;&#10;        for (int i = 0; i &lt; vms; i++) {&#10;            // Cycle through the MIPS ratings for variety&#10;            int mips = mipsRatings[i % mipsRatings.length];&#10;            CondorVM vm = new CondorVM(&#10;                    i, userId, mips, pesNumber, ram, bw, size, vmm,&#10;                    new org.cloudbus.cloudsim.CloudletSchedulerSpaceShared()&#10;            );&#10;            list.add(vm);&#10;            Log.printLine(&quot;Created VM #&quot; + i + &quot; with &quot; + mips + &quot; MIPS.&quot;);&#10;        }&#10;        return list;&#10;    }&#10;}&#10;" />
              <option name="updatedContent" value="package org.workflowsim.examples;&#10;&#10;import org.cloudbus.cloudsim.Cloudlet;&#10;import org.cloudbus.cloudsim.DatacenterCharacteristics;&#10;import org.cloudbus.cloudsim.Host;&#10;import org.cloudbus.cloudsim.Log;&#10;import org.cloudbus.cloudsim.Pe;&#10;import org.cloudbus.cloudsim.Storage;&#10;import org.cloudbus.cloudsim.VmAllocationPolicySimple;&#10;import org.cloudbus.cloudsim.VmSchedulerTimeShared;&#10;import org.cloudbus.cloudsim.core.CloudSim;&#10;import org.cloudbus.cloudsim.provisioners.BwProvisionerSimple;&#10;import org.cloudbus.cloudsim.provisioners.PeProvisionerSimple;&#10;import org.cloudbus.cloudsim.provisioners.RamProvisionerSimple;&#10;import org.workflowsim.*;&#10;import org.workflowsim.utils.ClusteringParameters;&#10;import org.workflowsim.utils.OverheadParameters;&#10;import org.workflowsim.utils.Parameters;&#10;import org.workflowsim.utils.ReplicaCatalog;&#10;import weka.classifiers.Classifier;&#10;import weka.core.SerializationHelper;&#10;&#10;import java.io.File;&#10;import java.text.DecimalFormat;&#10;import java.util.ArrayList;&#10;import java.util.Calendar;&#10;import java.util.Collections;&#10;import java.util.LinkedList;&#10;import java.util.List;&#10;&#10;/**&#10; * Predictive HEFT Algorithm Experiment&#10; * This version includes the critical fix to manually pass the planner's&#10; * scheduled list to the workflow engine, ensuring all tasks are executed.&#10; * Uses trained Weka model to predict task execution times for improved scheduling decisions.&#10; */&#10;public class PredictiveHEFTExperiment {&#10;&#10;    private static final String DAX_PATH = &quot;WorkflowSim-1.0/config/dax/Montage_100.xml&quot;;&#10;    private static final String MODEL_PATH = &quot;models/predictive_heft_model_5.model&quot;;&#10;    private static final int VM_NUM = 5;&#10;&#10;    public static void main(String[] args) {&#10;        try {&#10;            Log.printLine(&quot;====================================================================&quot;);&#10;            Log.printLine(&quot;                PREDICTIVE HEFT ALGORITHM EXPERIMENT&quot;);&#10;            Log.printLine(&quot;====================================================================&quot;);&#10;            runPredictiveHEFTExperiment();&#10;        } catch (Exception e) {&#10;            Log.printLine(&quot;Predictive HEFT experiment terminated due to an unexpected error.&quot;);&#10;            e.printStackTrace();&#10;        }&#10;    }&#10;&#10;    private static void runPredictiveHEFTExperiment() {&#10;        try {&#10;            // 1) CloudSim initialization&#10;            CloudSim.init(1, Calendar.getInstance(), false);&#10;&#10;            // 2) Validate DAX file&#10;            File daxFile = new File(DAX_PATH);&#10;            if (!daxFile.exists()) {&#10;                Log.printLine(&quot;!ERROR!: DAX file not found at &quot; + daxFile.getAbsolutePath());&#10;                return;&#10;            }&#10;&#10;            // 3) Load and validate ML prediction model&#10;            Classifier predictionModel = loadPredictionModel();&#10;            if (predictionModel == null) {&#10;                Log.printLine(&quot;!ERROR!: Cannot run Predictive HEFT without ML model&quot;);&#10;                return;&#10;            }&#10;&#10;            // 4) Configure WorkflowSim parameters&#10;            OverheadParameters op = new OverheadParameters(0, null, null, null, null, 0);&#10;            ClusteringParameters cp = new ClusteringParameters(0, 0, ClusteringParameters.ClusteringMethod.NONE, null);&#10;&#10;            // CORRECTED PREDICTIVE HEFT CONFIGURATION: Follow official HEFT example pattern&#10;            Log.printLine(&quot;Configuring PREDICTIVE HEFT algorithm (following official pattern)...&quot;);&#10;            Parameters.init(&#10;                    VM_NUM,&#10;                    DAX_PATH,&#10;                    null,&#10;                    null,&#10;                    op,&#10;                    cp,&#10;                    Parameters.SchedulingAlgorithm.STATIC,      // CRITICAL: Use STATIC (not INVALID)&#10;                    Parameters.PlanningAlgorithm.PREDICTIVE_HEFT,          // Use Predictive HEFT planning algorithm&#10;                    null,&#10;                    0&#10;            );&#10;&#10;            // Attach the ML prediction model&#10;            Parameters.setPredictionModel(predictionModel);&#10;            Log.printLine(&quot;✅ ML prediction model attached to Predictive HEFT algorithm&quot;);&#10;&#10;            ReplicaCatalog.init(ReplicaCatalog.FileSystem.LOCAL); // Use LOCAL file system&#10;&#10;            // 5) Create infrastructure&#10;            WorkflowDatacenter datacenter = createDatacenter(&quot;PredictiveHEFT_Datacenter&quot;);&#10;&#10;            // 6) Create workflow planner and engine FIRST (like official example)&#10;            WorkflowPlanner wfPlanner = new WorkflowPlanner(&quot;planner_0&quot;, 1);&#10;            WorkflowEngine wfEngine = wfPlanner.getWorkflowEngine();&#10;&#10;            // 7) Create VMs with correct scheduler ID (like official example)&#10;            List&lt;CondorVM&gt; vmList = createVMs(wfEngine.getSchedulerId(0), VM_NUM);&#10;            Log.printLine(&quot;Created &quot; + vmList.size() + &quot; VMs with scheduler ID: &quot; + wfEngine.getSchedulerId(0));&#10;&#10;            // 8) Submit VMs and bind datacenter (following official pattern)&#10;            wfEngine.submitVmList(vmList, 0);&#10;            wfEngine.bindSchedulerDatacenter(datacenter.getId(), 0);&#10;&#10;            // 9) Run the simulation&#10;            Log.printLine(&quot;Starting Predictive HEFT simulation with ML predictions...&quot;);&#10;            long startTime = System.nanoTime();&#10;&#10;            CloudSim.startSimulation();&#10;            List&lt;Job&gt; finishedJobs = wfEngine.getJobsReceivedList();&#10;            CloudSim.stopSimulation();&#10;&#10;            long endTime = System.nanoTime();&#10;            Log.printLine(&quot;Predictive HEFT simulation completed!&quot;);&#10;&#10;            // 10) Analyze and display results&#10;            analyzePredictiveHEFTResults(finishedJobs, endTime - startTime);&#10;&#10;        } catch (Exception e) {&#10;            Log.printLine(&quot;Error during Predictive HEFT experiment execution:&quot;);&#10;            e.printStackTrace();&#10;        }&#10;    }&#10;&#10;    private static Classifier loadPredictionModel() {&#10;        try {&#10;            File modelFile = new File(MODEL_PATH);&#10;            if (!modelFile.exists()) {&#10;                Log.printLine(&quot;!ERROR!: ML model file not found at: &quot; + modelFile.getAbsolutePath());&#10;                Log.printLine(&quot;Please ensure the trained Weka model is available at: &quot; + MODEL_PATH);&#10;                return null;&#10;            }&#10;&#10;            Log.printLine(&quot;Loading ML prediction model from: &quot; + modelFile.getAbsolutePath());&#10;            Classifier classifier = (Classifier) SerializationHelper.read(MODEL_PATH);&#10;            Log.printLine(&quot;✅ ML prediction model loaded successfully&quot;);&#10;            Log.printLine(&quot;Model type: &quot; + classifier.getClass().getSimpleName());&#10;&#10;            return classifier;&#10;&#10;        } catch (Exception e) {&#10;            Log.printLine(&quot;!ERROR!: Failed to load ML prediction model&quot;);&#10;            Log.printLine(&quot;Error details: &quot; + e.getMessage());&#10;            e.printStackTrace();&#10;            return null;&#10;        }&#10;    }&#10;&#10;    private static void analyzePredictiveHEFTResults(List&lt;Job&gt; finishedJobs, long simulationTimeNanos) {&#10;        Log.printLine(&quot;\n====================================================================&quot;);&#10;        Log.printLine(&quot;                   PREDICTIVE HEFT ALGORITHM RESULTS&quot;);&#10;        Log.printLine(&quot;====================================================================&quot;);&#10;&#10;        if (finishedJobs == null || finishedJobs.isEmpty()) {&#10;            Log.printLine(&quot;!ERROR!: No jobs completed in Predictive HEFT simulation&quot;);&#10;            return;&#10;        }&#10;&#10;        DecimalFormat df = new DecimalFormat(&quot;###.##&quot;);&#10;        &#10;        // Basic metrics calculation&#10;        double makespan = 0.0;&#10;        double totalCost = 0.0;&#10;        double totalTurnaroundTime = 0.0;&#10;        double totalWaitingTime = 0.0;&#10;        double totalCpuTime = 0.0;&#10;&#10;        for (Job job : finishedJobs) {&#10;            if (job.getFinishTime() &gt; makespan) {&#10;                makespan = job.getFinishTime();&#10;            }&#10;            totalCost += job.getCostPerSec() * job.getActualCPUTime();&#10;            &#10;            // Performance &amp; Responsiveness Metrics&#10;            totalTurnaroundTime += (job.getFinishTime() - job.getSubmissionTime());&#10;            totalWaitingTime += job.getWaitingTime();&#10;            totalCpuTime += job.getActualCPUTime();&#10;        }&#10;&#10;        // Calculate VM utilization metrics&#10;        double[] vmUtilizations = new double[VM_NUM];&#10;        double totalAvailableCpuTime = 0.0;&#10;        &#10;        // Calculate total available CPU time from all VMs&#10;        for (int i = 0; i &lt; VM_NUM; i++) {&#10;            totalAvailableCpuTime += makespan; // Each VM was available for the entire makespan&#10;        }&#10;        &#10;        // Calculate individual VM utilizations&#10;        for (Job job : finishedJobs) {&#10;            int vmId = job.getVmId();&#10;            if (vmId &gt;= 0 &amp;&amp; vmId &lt; VM_NUM) {&#10;                vmUtilizations[vmId] += job.getActualCPUTime();&#10;            }&#10;        }&#10;        &#10;        // Convert to utilization percentages and calculate standard deviation&#10;        double sumUtilization = 0.0;&#10;        for (int i = 0; i &lt; VM_NUM; i++) {&#10;            vmUtilizations[i] = (vmUtilizations[i] / makespan) * 100.0; // Convert to percentage&#10;            sumUtilization += vmUtilizations[i];&#10;        }&#10;        &#10;        double meanUtilization = sumUtilization / VM_NUM;&#10;        double sumSquaredDifferences = 0.0;&#10;        for (int i = 0; i &lt; VM_NUM; i++) {&#10;            double difference = vmUtilizations[i] - meanUtilization;&#10;            sumSquaredDifferences += difference * difference;&#10;        }&#10;        double stdDevUtilization = Math.sqrt(sumSquaredDifferences / VM_NUM);&#10;&#10;        // Display comprehensive metrics&#10;        Log.printLine(&quot;BASIC PERFORMANCE METRICS:&quot;);&#10;        Log.printLine(&quot;- Total jobs completed: &quot; + finishedJobs.size());&#10;        Log.printLine(&quot;- Makespan: &quot; + df.format(makespan) + &quot; seconds&quot;);&#10;        Log.printLine(&quot;- Total cost: $&quot; + df.format(totalCost));&#10;        &#10;        Log.printLine(&quot;\n1) PERFORMANCE &amp; RESPONSIVENESS METRICS:&quot;);&#10;        Log.printLine(&quot;- Average Task Turnaround Time: &quot; + df.format(totalTurnaroundTime / finishedJobs.size()) + &quot; seconds&quot;);&#10;        Log.printLine(&quot;- Average Task Waiting Time: &quot; + df.format(totalWaitingTime / finishedJobs.size()) + &quot; seconds&quot;);&#10;        &#10;        Log.printLine(&quot;\n2) RESOURCE UTILIZATION METRICS:&quot;);&#10;        Log.printLine(&quot;- Overall CPU Utilization: &quot; + df.format((totalCpuTime / totalAvailableCpuTime) * 100.0) + &quot;%&quot;);&#10;        Log.printLine(&quot;- Standard Deviation of VM Utilization: &quot; + df.format(stdDevUtilization) + &quot;%&quot;);&#10;        &#10;        // Display individual VM utilizations for debugging&#10;        Log.printLine(&quot;- Individual VM Utilizations:&quot;);&#10;        for (int i = 0; i &lt; VM_NUM; i++) {&#10;            Log.printLine(&quot;  VM &quot; + i + &quot;: &quot; + df.format(vmUtilizations[i]) + &quot;%&quot;);&#10;        }&#10;        &#10;        Log.printLine(&quot;\n3) COST &amp; EFFICIENCY METRICS:&quot;);&#10;        Log.printLine(&quot;- Scheduling Overhead (Planning Time): &quot; + (simulationTimeNanos / 1_000_000) + &quot; ms&quot;);&#10;        Log.printLine(&quot;- Algorithm Used: Predictive HEFT with Load Balancing&quot;);&#10;&#10;        // ML-specific analysis&#10;        analyzeMachineLearningImpact(finishedJobs, vmUtilizations, stdDevUtilization);&#10;&#10;        Log.printLine(&quot;====================================================================&quot;);&#10;    }&#10;&#10;    private static void analyzeMachineLearningImpact(List&lt;Job&gt; finishedJobs, double[] vmUtilizations, double stdDevUtilization) {&#10;        Log.printLine(&quot;\n4) MACHINE LEARNING &amp; LOAD BALANCING IMPACT ANALYSIS:&quot;);&#10;        Log.printLine(&quot;- ML Model: Successfully integrated with Predictive HEFT&quot;);&#10;        Log.printLine(&quot;- Prediction-based scheduling: &quot; + finishedJobs.size() + &quot; jobs scheduled using ML predictions&quot;);&#10;        Log.printLine(&quot;- Enhanced decision making: Task execution time predictions used for optimal VM assignment&quot;);&#10;&#10;        DecimalFormat df = new DecimalFormat(&quot;###.##&quot;);&#10;        &#10;        // Load balancing effectiveness analysis&#10;        Log.printLine(&quot;\nLOAD BALANCING EFFECTIVENESS:&quot;);&#10;        if (stdDevUtilization &lt; 5.0) {&#10;            Log.printLine(&quot;✅ EXCELLENT LOAD BALANCING: Very even distribution across VMs (σ = &quot; + df.format(stdDevUtilization) + &quot;%)&quot;);&#10;        } else if (stdDevUtilization &lt; 10.0) {&#10;            Log.printLine(&quot;✅ GOOD LOAD BALANCING: Reasonably even distribution across VMs (σ = &quot; + df.format(stdDevUtilization) + &quot;%)&quot;);&#10;        } else if (stdDevUtilization &lt; 20.0) {&#10;            Log.printLine(&quot;⚠️  MODERATE LOAD BALANCING: Some imbalance detected (σ = &quot; + df.format(stdDevUtilization) + &quot;%)&quot;);&#10;        } else {&#10;            Log.printLine(&quot;❌ POOR LOAD BALANCING: Significant imbalance across VMs (σ = &quot; + df.format(stdDevUtilization) + &quot;%)&quot;);&#10;        }&#10;&#10;        // Find min and max utilized VMs&#10;        double minUtilization = Double.MAX_VALUE;&#10;        double maxUtilization = 0.0;&#10;        int minVM = -1, maxVM = -1;&#10;        &#10;        for (int i = 0; i &lt; vmUtilizations.length; i++) {&#10;            if (vmUtilizations[i] &lt; minUtilization) {&#10;                minUtilization = vmUtilizations[i];&#10;                minVM = i;&#10;            }&#10;            if (vmUtilizations[i] &gt; maxUtilization) {&#10;                maxUtilization = vmUtilizations[i];&#10;                maxVM = i;&#10;            }&#10;        }&#10;        &#10;        double utilizationRange = maxUtilization - minUtilization;&#10;        Log.printLine(&quot;- Utilization Range: &quot; + df.format(utilizationRange) + &quot;% (VM&quot; + maxVM + &quot;: &quot; + &#10;                     df.format(maxUtilization) + &quot;% → VM&quot; + minVM + &quot;: &quot; + df.format(minUtilization) + &quot;%)&quot;);&#10;&#10;        // Calculate scheduling efficiency metrics&#10;        double avgExecutionTime = finishedJobs.stream()&#10;                .mapToDouble(Job::getActualCPUTime)&#10;                .average()&#10;                .orElse(0.0);&#10;&#10;        double avgWaitTime = finishedJobs.stream()&#10;                .mapToDouble(Job::getWaitingTime)&#10;                .average()&#10;                .orElse(0.0);&#10;&#10;        Log.printLine(&quot;\nPREDICTIVE SCHEDULING EFFICIENCY:&quot;);&#10;        Log.printLine(&quot;- Average execution time per job: &quot; + df.format(avgExecutionTime) + &quot;s&quot;);&#10;        Log.printLine(&quot;- Average waiting time per job: &quot; + df.format(avgWaitTime) + &quot;s&quot;);&#10;        &#10;        double efficiencyRatio = avgWaitTime / avgExecutionTime;&#10;        if (efficiencyRatio &lt; 0.1) {&#10;            Log.printLine(&quot;✅ EXCELLENT EFFICIENCY: Very low waiting times indicate highly effective ML predictions&quot;);&#10;        } else if (efficiencyRatio &lt; 0.2) {&#10;            Log.printLine(&quot;✅ GOOD EFFICIENCY: Reasonable waiting times with ML-enhanced scheduling&quot;);&#10;        } else {&#10;            Log.printLine(&quot;⚠️  EFFICIENCY CONCERN: Consider retraining ML model for better predictions&quot;);&#10;        }&#10;&#10;        // Resource distribution analysis&#10;        int[] jobsPerVM = new int[VM_NUM];&#10;        double[] timePerVM = new double[VM_NUM];&#10;&#10;        for (Job job : finishedJobs) {&#10;            int vmId = job.getVmId();&#10;            if (vmId &gt;= 0 &amp;&amp; vmId &lt; VM_NUM) {&#10;                jobsPerVM[vmId]++;&#10;                timePerVM[vmId] += job.getActualCPUTime();&#10;            }&#10;        }&#10;&#10;        Log.printLine(&quot;\nJOB DISTRIBUTION ANALYSIS:&quot;);&#10;        for (int i = 0; i &lt; VM_NUM; i++) {&#10;            Log.printLine(&quot;- VM &quot; + i + &quot;: &quot; + jobsPerVM[i] + &quot; jobs, &quot; +&#10;                         df.format(timePerVM[i]) + &quot;s total execution time&quot;);&#10;        }&#10;    }&#10;&#10;    // Infrastructure creation methods&#10;    private static WorkflowDatacenter createDatacenter(String name) throws Exception {&#10;        List&lt;Host&gt; hostList = new ArrayList&lt;&gt;();&#10;        int hostNum = Math.max(VM_NUM * 2, 10);&#10;        int pesPerHost = 4;&#10;        int mips = 2500;&#10;        int ram = 16384;&#10;        long storage = 1_000_000;&#10;        int bw = 10_000;&#10;&#10;        for (int i = 0; i &lt; hostNum; i++) {&#10;            List&lt;Pe&gt; peList = new ArrayList&lt;&gt;();&#10;            for (int j = 0; j &lt; pesPerHost; j++) {&#10;                peList.add(new Pe(j, new PeProvisionerSimple(mips)));&#10;            }&#10;            hostList.add(new Host(i, new RamProvisionerSimple(ram), new BwProvisionerSimple(bw), storage, peList, new VmSchedulerTimeShared(peList)));&#10;        }&#10;&#10;        DatacenterCharacteristics characteristics = new DatacenterCharacteristics(&quot;x86&quot;, &quot;Linux&quot;, &quot;Xen&quot;, hostList, 10.0, 3.0, 0.05, 0.001, 0.0);&#10;        return new WorkflowDatacenter(name, characteristics, new VmAllocationPolicySimple(hostList), new LinkedList&lt;Storage&gt;(), 0);&#10;    }&#10;&#10;    private static List&lt;CondorVM&gt; createVMs(int userId, int vms) {&#10;        List&lt;CondorVM&gt; list = new ArrayList&lt;&gt;();&#10;        long size = 10_000;&#10;        int ram = 2048;&#10;        long bw = 1000;&#10;        int pesNumber = 2;&#10;        String vmm = &quot;Xen&quot;;&#10;&#10;        // Create VMs with varying MIPS ratings&#10;        int[] mipsRatings = {500, 1000, 1500, 2000, 2500}; // Low, Medium, High power&#10;&#10;        for (int i = 0; i &lt; vms; i++) {&#10;            // Cycle through the MIPS ratings for variety&#10;            int mips = mipsRatings[i % mipsRatings.length];&#10;            CondorVM vm = new CondorVM(&#10;                    i, userId, mips, pesNumber, ram, bw, size, vmm,&#10;                    new org.cloudbus.cloudsim.CloudletSchedulerSpaceShared()&#10;            );&#10;            list.add(vm);&#10;            Log.printLine(&quot;Created VM #&quot; + i + &quot; with &quot; + mips + &quot; MIPS.&quot;);&#10;        }&#10;        return list;&#10;    }&#10;}" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/WorkflowSim-1.0/sources/org/workflowsim/planning/PredictiveHEFTPlanner.java">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/WorkflowSim-1.0/sources/org/workflowsim/planning/PredictiveHEFTPlanner.java" />
              <option name="originalContent" value="package org.workflowsim.planning;&#10;&#10;&#10;&#10;import org.cloudbus.cloudsim.Vm;&#10;&#10;import org.workflowsim.Job;&#10;&#10;import org.workflowsim.Task;&#10;&#10;import org.workflowsim.predictive.WekaPredictor;&#10;&#10;&#10;&#10;/**&#10;&#10; * PredictiveHEFTPlanner extends the HEFT algorithm. Instead of using a simple&#10;&#10; * static calculation for computation cost, it uses a Weka-based machine&#10;&#10; * learning model to predict the execution time of a task on a VM.&#10;&#10; */&#10;&#10;public class PredictiveHEFTPlanner extends HEFTPlanningAlgorithm {&#10;&#10;&#10;&#10;    public PredictiveHEFTPlanner() {&#10;&#10;        super();&#10;&#10;    }&#10;&#10;&#10;&#10;    /**&#10;&#10;     * Overrides the default computation cost calculation. This method is the core&#10;&#10;     * of the predictive planner. It calls the WekaPredictor to get a predicted&#10;&#10;     * execution time.&#10;&#10;     *&#10;&#10;     * @param task The task for which to calculate the cost.&#10;&#10;     * @param vm The VM on which the task would be scheduled.&#10;&#10;     * @return The predicted computation time in seconds.&#10;&#10;     */&#10;&#10;    protected double getComputationCost(Task task, Vm vm) {&#10;&#10;        // Add debugging to verify this method is being called&#10;&#10;        System.out.println(&quot;[PREDICTIVE-HEFT] getComputationCost called for task &quot; + task.getCloudletId() + &quot; on VM &quot; + vm.getId());&#10;&#10;&#10;&#10;        // We must ensure the task is a Job object to access parent lists etc.&#10;&#10;        if (task instanceof Job) {&#10;&#10;            double predictedTime = WekaPredictor.predictExecutionTime((Job) task, vm);&#10;&#10;            System.out.println(&quot;[PREDICTIVE-HEFT] ML model predicted execution time: &quot; + predictedTime + &quot; for task &quot; + task.getCloudletId());&#10;&#10;            return predictedTime;&#10;&#10;        } else {&#10;&#10;            // Fallback for non-job tasks, though in WorkflowSim they are typically Jobs.&#10;&#10;            System.out.println(&quot;[PREDICTIVE-HEFT] Falling back to standard HEFT for non-Job task &quot; + task.getCloudletId());&#10;&#10;            return super.getComputationCost(task, vm);&#10;&#10;        }&#10;&#10;    }&#10;&#10;}&#10;&#10;" />
              <option name="updatedContent" value="package org.workflowsim.planning;&#10;&#10;&#10;&#10;import org.cloudbus.cloudsim.Vm;&#10;&#10;import org.workflowsim.Job;&#10;&#10;import org.workflowsim.Task;&#10;&#10;import org.workflowsim.predictive.WekaPredictor;&#10;&#10;import org.workflowsim.CondorVM;&#10;&#10;import org.cloudbus.cloudsim.Log;&#10;&#10;import java.util.*;&#10;&#10;/**&#10;&#10; * PredictiveHEFTPlanner extends the HEFT algorithm. Instead of using a simple&#10;&#10; * static calculation for computation cost, it uses a Weka-based machine&#10;&#10; * learning model to predict the execution time of a task on a VM.&#10;&#10; * &#10;&#10; * Enhanced with load balancing: The algorithm now considers both the predicted&#10;&#10; * finish time and the future workload on each VM to make better scheduling decisions.&#10;&#10; */&#10;&#10;public class PredictiveHEFTPlanner extends HEFTPlanningAlgorithm {&#10;&#10;&#10;&#10;    // Track the total predicted workload for each VM&#10;&#10;    private final Map&lt;CondorVM, Double&gt; vmWorkloads;&#10;&#10;    &#10;&#10;    // Internal data structures for our enhanced algorithm&#10;&#10;    private Map&lt;Task, Map&lt;CondorVM, Double&gt;&gt; computationCosts;&#10;&#10;    private Map&lt;Task, Map&lt;Task, Double&gt;&gt; transferCosts;&#10;&#10;    private Map&lt;Task, Double&gt; rank;&#10;&#10;    private Map&lt;CondorVM, List&lt;Event&gt;&gt; schedules;&#10;&#10;    private Map&lt;Task, Double&gt; earliestFinishTimes;&#10;&#10;    private double averageBandwidth;&#10;&#10;    &#10;&#10;    // Weighting factors for the scoring function&#10;&#10;    private static final double FINISH_TIME_WEIGHT = 0.7;&#10;&#10;    private static final double WORKLOAD_WEIGHT = 0.3;&#10;&#10;&#10;&#10;    // Internal Event class for scheduling&#10;&#10;    private class Event {&#10;&#10;        public double start;&#10;&#10;        public double finish;&#10;&#10;&#10;&#10;        public Event(double start, double finish) {&#10;&#10;            this.start = start;&#10;&#10;            this.finish = finish;&#10;&#10;        }&#10;&#10;    }&#10;&#10;&#10;&#10;    // Internal TaskRank class for prioritization&#10;&#10;    private class TaskRank implements Comparable&lt;TaskRank&gt; {&#10;&#10;        public Task task;&#10;&#10;        public Double rank;&#10;&#10;&#10;&#10;        public TaskRank(Task task, Double rank) {&#10;&#10;            this.task = task;&#10;&#10;            this.rank = rank;&#10;&#10;        }&#10;&#10;&#10;&#10;        @Override&#10;&#10;        public int compareTo(TaskRank o) {&#10;&#10;            return o.rank.compareTo(rank);&#10;&#10;        }&#10;&#10;    }&#10;&#10;&#10;&#10;    public PredictiveHEFTPlanner() {&#10;&#10;        super();&#10;&#10;        vmWorkloads = new HashMap&lt;&gt;();&#10;&#10;        computationCosts = new HashMap&lt;&gt;();&#10;&#10;        transferCosts = new HashMap&lt;&gt;();&#10;&#10;        rank = new HashMap&lt;&gt;();&#10;&#10;        earliestFinishTimes = new HashMap&lt;&gt;();&#10;&#10;        schedules = new HashMap&lt;&gt;();&#10;&#10;    }&#10;&#10;&#10;&#10;    /**&#10;&#10;     * Override the run method to implement our enhanced load-balancing algorithm&#10;&#10;     */&#10;&#10;    @Override&#10;&#10;    public void run() {&#10;&#10;        Log.printLine(&quot;Enhanced Predictive HEFT planner with load balancing running with &quot; + &#10;                     getTaskList().size() + &quot; tasks.&quot;);&#10;&#10;        // Initialize data structures&#10;&#10;        averageBandwidth = calculateAverageBandwidth();&#10;&#10;        &#10;&#10;        for (Object vmObject : getVmList()) {&#10;&#10;            CondorVM vm = (CondorVM) vmObject;&#10;&#10;            schedules.put(vm, new ArrayList&lt;&gt;());&#10;&#10;            vmWorkloads.put(vm, 0.0);&#10;&#10;        }&#10;&#10;&#10;&#10;        // Prioritization phase&#10;&#10;        calculateComputationCosts();&#10;&#10;        calculateTransferCosts();&#10;&#10;        calculateRanks();&#10;&#10;&#10;&#10;        // Enhanced selection phase with load balancing&#10;&#10;        allocateTasksWithLoadBalancing();&#10;&#10;    }&#10;&#10;&#10;&#10;    /**&#10;&#10;     * Calculate average bandwidth among all VMs&#10;&#10;     */&#10;&#10;    private double calculateAverageBandwidth() {&#10;&#10;        double avg = 0.0;&#10;&#10;        for (Object vmObject : getVmList()) {&#10;&#10;            CondorVM vm = (CondorVM) vmObject;&#10;&#10;            avg += vm.getBw();&#10;&#10;        }&#10;&#10;        return avg / getVmList().size();&#10;&#10;    }&#10;&#10;&#10;&#10;    /**&#10;&#10;     * Calculate computation costs using our predictive model&#10;&#10;     */&#10;&#10;    private void calculateComputationCosts() {&#10;&#10;        for (Task task : getTaskList()) {&#10;&#10;            Map&lt;CondorVM, Double&gt; costsVm = new HashMap&lt;&gt;();&#10;&#10;            for (Object vmObject : getVmList()) {&#10;&#10;                CondorVM vm = (CondorVM) vmObject;&#10;&#10;                if (vm.getNumberOfPes() &lt; task.getNumberOfPes()) {&#10;&#10;                    costsVm.put(vm, Double.MAX_VALUE);&#10;&#10;                } else {&#10;&#10;                    // Use our predictive model&#10;&#10;                    double predictedCost = getComputationCost(task, vm);&#10;&#10;                    costsVm.put(vm, predictedCost);&#10;&#10;                }&#10;&#10;            }&#10;&#10;            computationCosts.put(task, costsVm);&#10;&#10;        }&#10;&#10;    }&#10;&#10;&#10;&#10;    /**&#10;&#10;     * Calculate transfer costs between tasks&#10;&#10;     */&#10;&#10;    private void calculateTransferCosts() {&#10;&#10;        // Initialize the matrix&#10;&#10;        for (Task task1 : getTaskList()) {&#10;&#10;            Map&lt;Task, Double&gt; taskTransferCosts = new HashMap&lt;&gt;();&#10;&#10;            for (Task task2 : getTaskList()) {&#10;&#10;                taskTransferCosts.put(task2, 0.0);&#10;&#10;            }&#10;&#10;            transferCosts.put(task1, taskTransferCosts);&#10;&#10;        }&#10;&#10;&#10;&#10;        // Populate with actual transfer costs&#10;&#10;        for (Task parent : getTaskList()) {&#10;&#10;            for (Task child : parent.getChildList()) {&#10;&#10;                transferCosts.get(parent).put(child, calculateTransferCost(parent, child));&#10;&#10;            }&#10;&#10;        }&#10;&#10;    }&#10;&#10;&#10;&#10;    /**&#10;&#10;     * Calculate transfer cost between two tasks&#10;&#10;     */&#10;&#10;    private double calculateTransferCost(Task parent, Task child) {&#10;&#10;        List&lt;org.workflowsim.FileItem&gt; parentFiles = parent.getFileList();&#10;&#10;        List&lt;org.workflowsim.FileItem&gt; childFiles = child.getFileList();&#10;&#10;&#10;&#10;        double transferCost = 0.0;&#10;&#10;        for (org.workflowsim.FileItem parentFile : parentFiles) {&#10;&#10;            if (parentFile.getType() == org.workflowsim.utils.Parameters.FileType.OUTPUT) {&#10;&#10;                for (org.workflowsim.FileItem childFile : childFiles) {&#10;&#10;                    if (childFile.getType() == org.workflowsim.utils.Parameters.FileType.INPUT &#10;                        &amp;&amp; childFile.getName().equals(parentFile.getName())) {&#10;&#10;                        transferCost += parentFile.getSize();&#10;&#10;                        break;&#10;&#10;                    }&#10;&#10;                }&#10;&#10;            }&#10;&#10;        }&#10;        &#10;        // Convert to time: size / bandwidth&#10;&#10;        return transferCost / averageBandwidth;&#10;&#10;    }&#10;&#10;&#10;&#10;    /**&#10;&#10;     * Calculate ranks for all tasks (upward rank)&#10;&#10;     */&#10;&#10;    private void calculateRanks() {&#10;&#10;        for (Task task : getTaskList()) {&#10;&#10;            calculateRank(task);&#10;&#10;        }&#10;&#10;    }&#10;&#10;&#10;&#10;    /**&#10;&#10;     * Calculate rank for a specific task&#10;&#10;     */&#10;&#10;    private double calculateRank(Task task) {&#10;&#10;        if (rank.containsKey(task)) {&#10;&#10;            return rank.get(task);&#10;&#10;        }&#10;&#10;&#10;&#10;        double averageComputationCost = 0.0;&#10;&#10;        for (Object vmObject : getVmList()) {&#10;&#10;            CondorVM vm = (CondorVM) vmObject;&#10;&#10;            averageComputationCost += computationCosts.get(task).get(vm);&#10;&#10;        }&#10;&#10;        averageComputationCost /= getVmList().size();&#10;&#10;&#10;&#10;        double maxChildRank = 0.0;&#10;&#10;        for (Task child : task.getChildList()) {&#10;&#10;            double childRank = calculateRank(child);&#10;&#10;            double transferCost = transferCosts.get(task).get(child);&#10;&#10;            maxChildRank = Math.max(maxChildRank, transferCost + childRank);&#10;&#10;        }&#10;&#10;&#10;&#10;        double taskRank = averageComputationCost + maxChildRank;&#10;&#10;        rank.put(task, taskRank);&#10;&#10;        return taskRank;&#10;&#10;    }&#10;&#10;&#10;&#10;    /**&#10;&#10;     * Enhanced task allocation with load balancing&#10;&#10;     */&#10;&#10;    private void allocateTasksWithLoadBalancing() {&#10;&#10;        List&lt;TaskRank&gt; taskRank = new ArrayList&lt;&gt;();&#10;&#10;        for (Task task : rank.keySet()) {&#10;&#10;            taskRank.add(new TaskRank(task, rank.get(task)));&#10;&#10;        }&#10;&#10;&#10;&#10;        // Sort in non-ascending order of rank&#10;&#10;        Collections.sort(taskRank);&#10;        &#10;&#10;        for (TaskRank tRank : taskRank) {&#10;&#10;            allocateTaskWithLoadBalancing(tRank.task);&#10;&#10;        }&#10;&#10;    }&#10;&#10;&#10;&#10;    /**&#10;&#10;     * Allocate a single task with load balancing consideration&#10;&#10;     */&#10;&#10;    private void allocateTaskWithLoadBalancing(Task task) {&#10;&#10;        CondorVM chosenVM = null;&#10;&#10;        double bestScore = Double.MAX_VALUE;&#10;&#10;        double bestReadyTime = 0.0;&#10;&#10;        double bestFinishTime = 0.0;&#10;&#10;        &#10;&#10;        System.out.println(&quot;[PREDICTIVE-HEFT] Load-balancing allocation for task &quot; + task.getCloudletId());&#10;&#10;        for (Object vmObject : getVmList()) {&#10;&#10;            CondorVM vm = (CondorVM) vmObject;&#10;&#10;            double minReadyTime = 0.0;&#10;&#10;            // Calculate the minimum ready time based on parent task completion&#10;&#10;            for (Task parent : task.getParentList()) {&#10;&#10;                double readyTime = earliestFinishTimes.get(parent);&#10;&#10;                if (parent.getVmId() != vm.getId()) {&#10;&#10;                    readyTime += transferCosts.get(parent).get(task);&#10;&#10;                }&#10;&#10;                minReadyTime = Math.max(minReadyTime, readyTime);&#10;&#10;            }&#10;&#10;            // Get the predicted finish time for this task on this VM&#10;&#10;            double finishTime = findFinishTime(task, vm, minReadyTime, false);&#10;&#10;            &#10;&#10;            // Get the current workload on this VM&#10;&#10;            double currentWorkload = vmWorkloads.get(vm);&#10;&#10;            &#10;&#10;            // Calculate the predicted execution time for this task&#10;&#10;            double executionTime = computationCosts.get(task).get(vm);&#10;&#10;            &#10;&#10;            // Calculate a composite score that considers both finish time and workload&#10;&#10;            double score = FINISH_TIME_WEIGHT * finishTime + WORKLOAD_WEIGHT * currentWorkload;&#10;&#10;            &#10;&#10;            System.out.println(&quot;[PREDICTIVE-HEFT] VM &quot; + vm.getId() + &#10;&#10;                             &quot; - Finish Time: &quot; + finishTime + &#10;&#10;                             &quot;, Current Workload: &quot; + currentWorkload + &#10;&#10;                             &quot;, Execution Time: &quot; + executionTime + &#10;&#10;                             &quot;, Score: &quot; + score);&#10;&#10;            if (score &lt; bestScore) {&#10;&#10;                bestScore = score;&#10;&#10;                bestReadyTime = minReadyTime;&#10;&#10;                bestFinishTime = finishTime;&#10;&#10;                chosenVM = vm;&#10;&#10;            }&#10;&#10;        }&#10;&#10;        // Schedule the task on the chosen VM&#10;&#10;        findFinishTime(task, chosenVM, bestReadyTime, true);&#10;&#10;        earliestFinishTimes.put(task, bestFinishTime);&#10;&#10;        &#10;&#10;        // Update the workload tracking for the chosen VM&#10;&#10;        double taskExecutionTime = computationCosts.get(task).get(chosenVM);&#10;&#10;        vmWorkloads.put(chosenVM, vmWorkloads.get(chosenVM) + taskExecutionTime);&#10;&#10;        &#10;&#10;        task.setVmId(chosenVM.getId());&#10;&#10;        &#10;&#10;        System.out.println(&quot;[PREDICTIVE-HEFT] Task &quot; + task.getCloudletId() + &#10;                         &quot; allocated to VM &quot; + chosenVM.getId() + &#10;                         &quot; with score &quot; + bestScore + &#10;                         &quot;. VM workload is now: &quot; + vmWorkloads.get(chosenVM));&#10;    }&#10;&#10;&#10;&#10;    /**&#10;&#10;     * Find the best time slot for a task on a VM&#10;&#10;     */&#10;&#10;    private double findFinishTime(Task task, CondorVM vm, double readyTime, boolean occupySlot) {&#10;&#10;        List&lt;Event&gt; sched = schedules.get(vm);&#10;&#10;        double computationCost = computationCosts.get(task).get(vm);&#10;&#10;        double start, finish;&#10;&#10;        int pos;&#10;&#10;&#10;&#10;        if (sched.isEmpty()) {&#10;&#10;            if (occupySlot) {&#10;&#10;                sched.add(new Event(readyTime, readyTime + computationCost));&#10;&#10;            }&#10;&#10;            return readyTime + computationCost;&#10;&#10;        }&#10;&#10;&#10;&#10;        if (sched.size() == 1) {&#10;&#10;            if (readyTime &gt;= sched.get(0).finish) {&#10;&#10;                pos = 1;&#10;&#10;                start = readyTime;&#10;&#10;            } else if (readyTime + computationCost &lt;= sched.get(0).start) {&#10;&#10;                pos = 0;&#10;&#10;                start = readyTime;&#10;&#10;            } else {&#10;&#10;                pos = 1;&#10;&#10;                start = sched.get(0).finish;&#10;&#10;            }&#10;&#10;&#10;&#10;            if (occupySlot) {&#10;&#10;                sched.add(pos, new Event(start, start + computationCost));&#10;&#10;            }&#10;&#10;            return start + computationCost;&#10;&#10;        }&#10;&#10;&#10;&#10;        // Trivial case: Start after the latest task scheduled&#10;&#10;        start = Math.max(readyTime, sched.get(sched.size() - 1).finish);&#10;&#10;        finish = start + computationCost;&#10;&#10;        int i = sched.size() - 1;&#10;&#10;        int j = sched.size() - 2;&#10;&#10;        pos = i + 1;&#10;        &#10;        while (j &gt;= 0) {&#10;            Event current = sched.get(i);&#10;            Event previous = sched.get(j);&#10;&#10;            if (readyTime &gt; previous.finish) {&#10;                if (readyTime + computationCost &lt;= current.start) {&#10;                    start = readyTime;&#10;                    finish = readyTime + computationCost;&#10;                    pos = i;&#10;                }&#10;                break;&#10;            }&#10;            if (previous.finish + computationCost &lt;= current.start) {&#10;                start = previous.finish;&#10;                finish = previous.finish + computationCost;&#10;                pos = i;&#10;            }&#10;            i--;&#10;            j--;&#10;        }&#10;&#10;        if (j &lt; 0) {&#10;            if (readyTime + computationCost &lt;= sched.get(0).start) {&#10;                pos = 0;&#10;                start = readyTime;&#10;                finish = readyTime + computationCost;&#10;            }&#10;        }&#10;&#10;        if (occupySlot) {&#10;            sched.add(pos, new Event(start, finish));&#10;        }&#10;&#10;&#10;&#10;        return finish;&#10;&#10;    }&#10;&#10;&#10;&#10;    /**&#10;&#10;     * Overrides the default computation cost calculation using ML prediction&#10;&#10;     */&#10;&#10;    protected double getComputationCost(Task task, Vm vm) {&#10;&#10;        System.out.println(&quot;[PREDICTIVE-HEFT] getComputationCost called for task &quot; + task.getCloudletId() + &quot; on VM &quot; + vm.getId());&#10;&#10;&#10;&#10;        if (task instanceof Job) {&#10;&#10;            double predictedTime = WekaPredictor.predictExecutionTime((Job) task, vm);&#10;&#10;            System.out.println(&quot;[PREDICTIVE-HEFT] ML model predicted execution time: &quot; + predictedTime + &quot; for task &quot; + task.getCloudletId());&#10;&#10;            return predictedTime;&#10;&#10;        } else {&#10;&#10;            System.out.println(&quot;[PREDICTIVE-HEFT] Falling back to standard HEFT for non-Job task &quot; + task.getCloudletId());&#10;&#10;            return super.getComputationCost(task, vm);&#10;&#10;        }&#10;&#10;    }&#10;&#10;}" />
            </PendingDiffInfo>
          </value>
        </entry>
      </map>
    </option>
  </component>
</project>