<?xml version="1.0" encoding="UTF-8"?>
<project version="4">
  <component name="CopilotDiffPersistence">
    <option name="pendingDiffs">
      <map>
        <entry key="$PROJECT_DIR$/WorkflowSim-1.0/examples/org/workflowsim/examples/ArffDataGenerator.java">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/WorkflowSim-1.0/examples/org/workflowsim/examples/ArffDataGenerator.java" />
              <option name="originalContent" value="package org.workflowsim.examples;&#10;&#10;import org.cloudbus.cloudsim.Cloudlet;&#10;import org.cloudbus.cloudsim.CloudletSchedulerSpaceShared;&#10;import org.cloudbus.cloudsim.DatacenterCharacteristics;&#10;import org.cloudbus.cloudsim.Host;&#10;import org.cloudbus.cloudsim.Log;&#10;import org.cloudbus.cloudsim.Pe;&#10;import org.cloudbus.cloudsim.Storage;&#10;import org.cloudbus.cloudsim.Vm;&#10;import org.cloudbus.cloudsim.VmAllocationPolicySimple;&#10;import org.cloudbus.cloudsim.VmSchedulerTimeShared;&#10;import org.cloudbus.cloudsim.core.CloudSim;&#10;import org.cloudbus.cloudsim.provisioners.BwProvisionerSimple;&#10;import org.cloudbus.cloudsim.provisioners.PeProvisionerSimple;&#10;import org.cloudbus.cloudsim.provisioners.RamProvisionerSimple;&#10;import org.workflowsim.*;&#10;import org.workflowsim.utils.ClusteringParameters;&#10;import org.workflowsim.utils.OverheadParameters;&#10;import org.workflowsim.utils.Parameters;&#10;import org.workflowsim.utils.ReplicaCatalog;&#10;&#10;import java.io.File;&#10;import java.io.FileWriter;&#10;import java.text.DecimalFormat;&#10;import java.util.ArrayList;&#10;import java.util.Calendar;&#10;import java.util.LinkedList;&#10;import java.util.List;&#10;&#10;/**&#10; * This class is the definitive data generator for the Predictive HEFT project.&#10; * It correctly initializes the WorkflowSim environment to avoid common pitfalls&#10; * like the &quot;super-job&quot; clustering anomaly and the startup race condition,&#10; * ensuring that a complete and accurate ARFF file is generated from the simulation.&#10; */&#10;public class ArffDataGenerator {&#10;&#10;    /** The list of virtual machines. */&#10;    private static List&lt;CondorVM&gt; vmList;&#10;    /** The list of finished tasks. */&#10;    private static List&lt;Cloudlet&gt; finishedList;&#10;&#10;    /**&#10;     * The main method that starts the simulation and data generation process.&#10;     */&#10;    public static void main(String[] args) {&#10;        try {&#10;            Log.printLine(&quot;Initialising...&quot;);&#10;            runSimulation();&#10;            if (finishedList != null &amp;&amp; !finishedList.isEmpty()) {&#10;                generateArffFile(finishedList);&#10;            } else {&#10;                Log.printLine(&quot;!ERROR!: Simulation did not produce any finished cloudlets. ARFF file not generated.&quot;);&#10;            }&#10;        } catch (Exception e) {&#10;            Log.printLine(&quot;The simulation has been terminated due to an unexpected error&quot;);&#10;            e.printStackTrace();&#10;        }&#10;    }&#10;&#10;    /**&#10;     * Configures and runs the WorkflowSim simulation.&#10;     */&#10;    private static void runSimulation() {&#10;        try {&#10;            int num_user = 1;&#10;            Calendar calendar = Calendar.getInstance();&#10;            boolean trace_flag = false;&#10;&#10;            CloudSim.init(num_user, calendar, trace_flag);&#10;&#10;            // Use a DAX file that exists in your system&#10;            // String daxPath = &quot;WorkflowSim-1.0/config/dax/Montage_1000.xml&quot;;&#10;            String daxPath = &quot;WorkflowSim-1.0/config/dax/Montage_100.xml&quot;; // Try with a smaller workflow first&#10;            File daxFile = new File(daxPath);&#10;            if (!daxFile.exists()) {&#10;                Log.printLine(&quot;!ERROR!: Dax file not found at &quot; + daxFile.getAbsolutePath());&#10;                Log.printLine(&quot;Current working directory: &quot; + new File(&quot;.&quot;).getAbsolutePath());&#10;                return;&#10;            } else {&#10;                Log.printLine(&quot;Found DAX file at: &quot; + daxFile.getAbsolutePath());&#10;            }&#10;&#10;            Parameters.SchedulingAlgorithm sch_method = Parameters.SchedulingAlgorithm.STATIC;&#10;            Parameters.PlanningAlgorithm pln_method = Parameters.PlanningAlgorithm.HEFT;&#10;            ReplicaCatalog.FileSystem file_system = ReplicaCatalog.FileSystem.SHARED;&#10;&#10;            // Set all overheads to 0 for simplicity&#10;            OverheadParameters op = new OverheadParameters(0, null, null, null, null, 0);&#10;&#10;            // No clustering to avoid complications&#10;            ClusteringParameters.ClusteringMethod method = ClusteringParameters.ClusteringMethod.NONE;&#10;            ClusteringParameters cp = new ClusteringParameters(0, 0, method, null);&#10;&#10;            // Use fewer VMs to start with&#10;            int vmNum = 5;&#10;            Parameters.init(vmNum, daxPath, null, null, op, cp, sch_method, pln_method, null, 0);&#10;            ReplicaCatalog.init(file_system);&#10;&#10;            // Create VMs&#10;            vmList = createVM(0, Parameters.getVmNum());&#10;&#10;            Log.printLine(&quot;VM Configuration:&quot;);&#10;            for (CondorVM vm : vmList) {&#10;                Log.printLine(&quot;VM #&quot; + vm.getId() + &quot; MIPS: &quot; + vm.getMips() + &quot; PEs: &quot; + vm.getNumberOfPes());&#10;            }&#10;&#10;            // Create Datacenter&#10;            WorkflowDatacenter datacenter0 = createDatacenter(&quot;Datacenter_0&quot;);&#10;&#10;            Log.printLine(&quot;Host Configuration:&quot;);&#10;            for (Host host : datacenter0.getHostList()) {&#10;                Log.printLine(&quot;Host #&quot; + host.getId() + &quot; RAM: &quot; + host.getRamProvisioner().getRam() +&#10;                        &quot; PEs: &quot; + host.getNumberOfPes());&#10;            }&#10;&#10;            // Create workflow planner, engine, etc.&#10;            WorkflowPlanner wfPlanner = new WorkflowPlanner(&quot;planner_0&quot;, 1);&#10;            WorkflowEngine wfEngine = wfPlanner.getWorkflowEngine();&#10;&#10;            // Submit VMs to the engine&#10;            wfEngine.submitVmList(vmList);&#10;            wfEngine.bindSchedulerDatacenter(datacenter0.getId(), 0);&#10;&#10;            Log.printLine(&quot;Submitting VMs and starting simulation...&quot;);&#10;            Log.printLine(&quot;Number of VMs: &quot; + vmList.size());&#10;&#10;            // Start the simulation&#10;            CloudSim.startSimulation();&#10;&#10;            // Get the finished jobs from the workflow engine&#10;            finishedList = wfEngine.getJobsReceivedList();&#10;&#10;            // If no jobs received from engine, try from clustering engine&#10;            if (finishedList == null || finishedList.isEmpty()) {&#10;                // Need explicit cast to resolve type incompatibility&#10;                finishedList = (List&lt;Cloudlet&gt;) (List&lt;?&gt;) wfPlanner.getClusteringEngine().getJobList();&#10;                Log.printLine(&quot;Trying to get jobs from clustering engine instead.&quot;);&#10;            }&#10;&#10;            CloudSim.stopSimulation();&#10;&#10;            Log.printLine(&quot;Simulation finished!&quot;);&#10;            if (finishedList != null) {&#10;                Log.printLine(&quot;Number of jobs received: &quot; + finishedList.size());&#10;                int successCount = 0;&#10;                for (Cloudlet cl : finishedList) {&#10;                    if (cl.getCloudletStatus() == Cloudlet.SUCCESS) {&#10;                        successCount++;&#10;                    }&#10;                }&#10;                Log.printLine(&quot;Number of finished (SUCCESS) cloudlets: &quot; + successCount);&#10;            } else {&#10;                Log.printLine(&quot;No jobs received. Check DAX parsing and job submission.&quot;);&#10;            }&#10;&#10;        } catch (Exception e) {&#10;            Log.printLine(&quot;An error occurred during simulation: &quot; + e.getMessage());&#10;            e.printStackTrace();&#10;        }&#10;    }&#10;&#10;    /**&#10;     * Generates the training_data.arff file from the simulation results.&#10;     * @param list The list of finished cloudlets.&#10;     */&#10;    private static void generateArffFile(List&lt;Cloudlet&gt; list) {&#10;        try (FileWriter writer = new FileWriter(&quot;training_data.arff&quot;)) {&#10;            writer.write(&quot;@RELATION task_execution_prediction\n\n&quot;);&#10;            writer.write(&quot;@ATTRIBUTE task_length NUMERIC\n&quot;);&#10;            writer.write(&quot;@ATTRIBUTE num_parents NUMERIC\n&quot;);&#10;            writer.write(&quot;@ATTRIBUTE vm_mips NUMERIC\n&quot;);&#10;            writer.write(&quot;@ATTRIBUTE vm_pes NUMERIC\n&quot;);&#10;            writer.write(&quot;@ATTRIBUTE actual_execution_time NUMERIC\n\n&quot;);&#10;            writer.write(&quot;@DATA\n&quot;);&#10;&#10;            DecimalFormat dft = new DecimalFormat(&quot;###.##&quot;);&#10;            int validInstances = 0;&#10;&#10;            for (Cloudlet cloudlet : list) {&#10;                if (cloudlet.getCloudletStatus() != Cloudlet.SUCCESS) {&#10;                    continue;&#10;                }&#10;                Job job = (Job) cloudlet;&#10;                String taskLength = dft.format(job.getCloudletLength());&#10;                String numParents = Integer.toString(job.getParentList().size());&#10;                int vmId = cloudlet.getVmId();&#10;                Vm vm = null;&#10;                for (Vm v : vmList) {&#10;                    if (v.getId() == vmId) {&#10;                        vm = v;&#10;                        break;&#10;                    }&#10;                }&#10;                if (vm == null) {&#10;                    continue;&#10;                }&#10;&#10;                // Make sure execution time is valid&#10;                double execTime = job.getFinishTime() - job.getExecStartTime();&#10;                if (execTime &lt;= 0) {&#10;                    continue;&#10;                }&#10;&#10;                String vmMips = dft.format(vm.getMips());&#10;                String vmPes = Integer.toString(vm.getNumberOfPes());&#10;                String actualTime = dft.format(execTime);&#10;                writer.write(taskLength + &quot;,&quot; + numParents + &quot;,&quot; + vmMips + &quot;,&quot; + vmPes + &quot;,&quot; + actualTime + &quot;\n&quot;);&#10;                validInstances++;&#10;            }&#10;            Log.printLine(&quot;Successfully created training_data.arff file with &quot; + validInstances + &quot; valid instances.&quot;);&#10;        } catch (Exception e) {&#10;            Log.printLine(&quot;Error creating ARFF file: &quot; + e.getMessage());&#10;            e.printStackTrace();&#10;        }&#10;    }&#10;&#10;    /**&#10;     * Creates a Datacenter.&#10;     * @param name The name of the datacenter.&#10;     * @return The created WorkflowDatacenter.&#10;     */&#10;    protected static WorkflowDatacenter createDatacenter(String name) {&#10;        List&lt;Host&gt; hostList = new ArrayList&lt;&gt;();&#10;        int mips = 1000;&#10;        int ram = 16384; // Increased RAM per host&#10;        long storage = 1000000;&#10;        int bw = 10000;&#10;        int hostNum = Math.max(Parameters.getVmNum() * 2, 10); // More hosts than VMs&#10;        int pesPerHost = 4; // Multiple cores per host&#10;&#10;        for (int i = 0; i &lt; hostNum; i++) {&#10;            List&lt;Pe&gt; peList = new ArrayList&lt;&gt;();&#10;            for (int j = 0; j &lt; pesPerHost; j++) {&#10;                peList.add(new Pe(j, new PeProvisionerSimple(mips)));&#10;            }&#10;            hostList.add(new Host(i, new RamProvisionerSimple(ram), new BwProvisionerSimple(bw), storage, peList, new VmSchedulerTimeShared(peList)));&#10;        }&#10;        String arch = &quot;x86&quot;;&#10;        String os = &quot;Linux&quot;;&#10;        String vmm = &quot;Xen&quot;;&#10;        double time_zone = 10.0;&#10;        double cost = 3.0;&#10;        double costPerMem = 0.05;&#10;        double costPerStorage = 0.001;&#10;        double costPerBw = 0.0;&#10;        DatacenterCharacteristics characteristics = new DatacenterCharacteristics(arch, os, vmm, hostList, time_zone, cost, costPerMem, costPerStorage, costPerBw);&#10;        WorkflowDatacenter datacenter = null;&#10;        try {&#10;            datacenter = new WorkflowDatacenter(name, characteristics, new VmAllocationPolicySimple(hostList), new LinkedList&lt;&gt;(), 0);&#10;        } catch (Exception e) {&#10;            e.printStackTrace();&#10;        }&#10;        return datacenter;&#10;    }&#10;&#10;    /**&#10;     * Creates a list of virtual machines.&#10;     * @param userId The user ID.&#10;     * @param vms The number of VMs.&#10;     * @return A list of CondorVMs.&#10;     */&#10;    protected static List&lt;CondorVM&gt; createVM(int userId, int vms) {&#10;        LinkedList&lt;CondorVM&gt; list = new LinkedList&lt;&gt;();&#10;        long size = 10000;&#10;        int ram = 2048; // Increased RAM per VM&#10;        int mips = 1000;&#10;        long bw = 1000;&#10;        int pesNumber = 2; // Give each VM 2 processing elements&#10;        String vmm = &quot;Xen&quot;;&#10;        for (int i = 0; i &lt; vms; i++) {&#10;            double ratio = 1.0;&#10;            CondorVM vm = new CondorVM(i, userId, mips * ratio, pesNumber, ram, bw, size, vmm, new CloudletSchedulerSpaceShared());&#10;            list.add(vm);&#10;        }&#10;        return list;&#10;    }&#10;}&#10;" />
              <option name="updatedContent" value="package org.workflowsim.examples;&#10;&#10;import org.cloudbus.cloudsim.Cloudlet;&#10;import org.cloudbus.cloudsim.CloudletSchedulerSpaceShared;&#10;import org.cloudbus.cloudsim.DatacenterCharacteristics;&#10;import org.cloudbus.cloudsim.Host;&#10;import org.cloudbus.cloudsim.Log;&#10;import org.cloudbus.cloudsim.Pe;&#10;import org.cloudbus.cloudsim.Storage;&#10;import org.cloudbus.cloudsim.Vm;&#10;import org.cloudbus.cloudsim.VmAllocationPolicySimple;&#10;import org.cloudbus.cloudsim.VmSchedulerTimeShared;&#10;import org.cloudbus.cloudsim.core.CloudSim;&#10;import org.cloudbus.cloudsim.provisioners.BwProvisionerSimple;&#10;import org.cloudbus.cloudsim.provisioners.PeProvisionerSimple;&#10;import org.cloudbus.cloudsim.provisioners.RamProvisionerSimple;&#10;import org.workflowsim.*;&#10;import org.workflowsim.utils.ClusteringParameters;&#10;import org.workflowsim.utils.OverheadParameters;&#10;import org.workflowsim.utils.Parameters;&#10;import org.workflowsim.utils.ReplicaCatalog;&#10;&#10;import java.io.File;&#10;import java.io.FileWriter;&#10;import java.text.DecimalFormat;&#10;import java.util.ArrayList;&#10;import java.util.Calendar;&#10;import java.util.LinkedList;&#10;import java.util.List;&#10;&#10;/**&#10; * This class is the definitive data generator for the Predictive HEFT project.&#10; * It correctly initializes the WorkflowSim environment to avoid common pitfalls&#10; * like the &quot;super-job&quot; clustering anomaly and the startup race condition,&#10; * ensuring that a complete and accurate ARFF file is generated from the simulation.&#10; */&#10;public class ArffDataGenerator {&#10;&#10;    /** The list of virtual machines. */&#10;    private static List&lt;CondorVM&gt; vmList;&#10;    /** The list of finished tasks. */&#10;    private static List&lt;Cloudlet&gt; finishedList;&#10;&#10;    /**&#10;     * The main method that starts the simulation and data generation process.&#10;     */&#10;    public static void main(String[] args) {&#10;        try {&#10;            Log.printLine(&quot;Initialising...&quot;);&#10;            runSimulation();&#10;            if (finishedList != null &amp;&amp; !finishedList.isEmpty()) {&#10;                generateArffFile(finishedList);&#10;            } else {&#10;                Log.printLine(&quot;!ERROR!: Simulation did not produce any finished cloudlets. ARFF file not generated.&quot;);&#10;            }&#10;        } catch (Exception e) {&#10;            Log.printLine(&quot;The simulation has been terminated due to an unexpected error&quot;);&#10;            e.printStackTrace();&#10;        }&#10;    }&#10;&#10;    /**&#10;     * Configures and runs the WorkflowSim simulation.&#10;     */&#10;    private static void runSimulation() {&#10;        try {&#10;            int num_user = 1;&#10;            Calendar calendar = Calendar.getInstance();&#10;            boolean trace_flag = false;&#10;&#10;            CloudSim.init(num_user, calendar, trace_flag);&#10;&#10;            // Use a DAX file that exists in your system&#10;            // String daxPath = &quot;WorkflowSim-1.0/config/dax/Montage_1000.xml&quot;;&#10;            String daxPath = &quot;WorkflowSim-1.0/config/dax/Montage_100.xml&quot;; // Try with a smaller workflow first&#10;            File daxFile = new File(daxPath);&#10;            if (!daxFile.exists()) {&#10;                Log.printLine(&quot;!ERROR!: Dax file not found at &quot; + daxFile.getAbsolutePath());&#10;                Log.printLine(&quot;Current working directory: &quot; + new File(&quot;.&quot;).getAbsolutePath());&#10;                return;&#10;            } else {&#10;                Log.printLine(&quot;Found DAX file at: &quot; + daxFile.getAbsolutePath());&#10;            }&#10;&#10;            Parameters.SchedulingAlgorithm sch_method = Parameters.SchedulingAlgorithm.STATIC;&#10;            Parameters.PlanningAlgorithm pln_method = Parameters.PlanningAlgorithm.HEFT;&#10;            ReplicaCatalog.FileSystem file_system = ReplicaCatalog.FileSystem.SHARED;&#10;&#10;            // Set all overheads to 0 for simplicity&#10;            OverheadParameters op = new OverheadParameters(0, null, null, null, null, 0);&#10;&#10;            // No clustering to avoid complications&#10;            ClusteringParameters.ClusteringMethod method = ClusteringParameters.ClusteringMethod.NONE;&#10;            ClusteringParameters cp = new ClusteringParameters(0, 0, method, null);&#10;&#10;            // Use fewer VMs to start with&#10;            int vmNum = 5;&#10;            Parameters.init(vmNum, daxPath, null, null, op, cp, sch_method, pln_method, null, 0);&#10;            ReplicaCatalog.init(file_system);&#10;&#10;            // Create VMs&#10;            vmList = createVM(0, Parameters.getVmNum());&#10;&#10;            Log.printLine(&quot;VM Configuration:&quot;);&#10;            for (CondorVM vm : vmList) {&#10;                Log.printLine(&quot;VM #&quot; + vm.getId() + &quot; MIPS: &quot; + vm.getMips() + &quot; PEs: &quot; + vm.getNumberOfPes());&#10;            }&#10;&#10;            // Create Datacenter&#10;            WorkflowDatacenter datacenter0 = createDatacenter(&quot;Datacenter_0&quot;);&#10;&#10;            Log.printLine(&quot;Host Configuration:&quot;);&#10;            for (Host host : datacenter0.getHostList()) {&#10;                Log.printLine(&quot;Host #&quot; + host.getId() + &quot; RAM: &quot; + host.getRamProvisioner().getRam() +&#10;                        &quot; PEs: &quot; + host.getNumberOfPes());&#10;            }&#10;&#10;            // Create workflow planner, engine, etc.&#10;            WorkflowPlanner wfPlanner = new WorkflowPlanner(&quot;planner_0&quot;, 1);&#10;            WorkflowEngine wfEngine = wfPlanner.getWorkflowEngine();&#10;&#10;            // Submit VMs to the engine&#10;            wfEngine.submitVmList(vmList);&#10;            wfEngine.bindSchedulerDatacenter(datacenter0.getId(), 0);&#10;&#10;            Log.printLine(&quot;Submitting VMs and starting simulation...&quot;);&#10;            Log.printLine(&quot;Number of VMs: &quot; + vmList.size());&#10;&#10;            // Start the simulation&#10;            CloudSim.startSimulation();&#10;&#10;            // Get the finished jobs from the workflow engine&#10;            finishedList = wfEngine.getJobsReceivedList();&#10;&#10;            // If no jobs received from engine, try from clustering engine&#10;            if (finishedList == null || finishedList.isEmpty()) {&#10;                // Need explicit cast to resolve type incompatibility&#10;                finishedList = (List&lt;Cloudlet&gt;) (List&lt;?&gt;) wfPlanner.getClusteringEngine().getJobList();&#10;                Log.printLine(&quot;Trying to get jobs from clustering engine instead.&quot;);&#10;            }&#10;&#10;            CloudSim.stopSimulation();&#10;&#10;            Log.printLine(&quot;Simulation finished!&quot;);&#10;            if (finishedList != null) {&#10;                Log.printLine(&quot;Number of jobs received: &quot; + finishedList.size());&#10;                int successCount = 0;&#10;                for (Cloudlet cl : finishedList) {&#10;                    if (cl.getCloudletStatus() == Cloudlet.SUCCESS) {&#10;                        successCount++;&#10;                    }&#10;                }&#10;                Log.printLine(&quot;Number of finished (SUCCESS) cloudlets: &quot; + successCount);&#10;            } else {&#10;                Log.printLine(&quot;No jobs received. Check DAX parsing and job submission.&quot;);&#10;            }&#10;&#10;        } catch (Exception e) {&#10;            Log.printLine(&quot;An error occurred during simulation: &quot; + e.getMessage());&#10;            e.printStackTrace();&#10;        }&#10;    }&#10;&#10;    /**&#10;     * Generates the training_data.arff file from the simulation results.&#10;     * @param list The list of finished cloudlets.&#10;     */&#10;    private static void generateArffFile(List&lt;Cloudlet&gt; list) {&#10;        try (FileWriter writer = new FileWriter(&quot;training_data.arff&quot;)) {&#10;            writer.write(&quot;@RELATION task_execution_prediction\n\n&quot;);&#10;            writer.write(&quot;@ATTRIBUTE task_length NUMERIC\n&quot;);&#10;            writer.write(&quot;@ATTRIBUTE num_parents NUMERIC\n&quot;);&#10;            writer.write(&quot;@ATTRIBUTE vm_mips NUMERIC\n&quot;);&#10;            writer.write(&quot;@ATTRIBUTE vm_pes NUMERIC\n&quot;);&#10;            writer.write(&quot;@ATTRIBUTE actual_execution_time NUMERIC\n\n&quot;);&#10;            writer.write(&quot;@DATA\n&quot;);&#10;&#10;            DecimalFormat dft = new DecimalFormat(&quot;###.##&quot;);&#10;            int validInstances = 0;&#10;            int invalidInstances = 0;&#10;&#10;            Log.printLine(&quot;Processing &quot; + list.size() + &quot; jobs for ARFF generation...&quot;);&#10;            &#10;            for (Cloudlet cloudlet : list) {&#10;                // Debug: Log each job's status&#10;                Log.printLine(&quot;Job #&quot; + cloudlet.getCloudletId() + &quot; status: &quot; + cloudlet.getCloudletStatusString());&#10;                &#10;                // TEMPORARY FIX: Include all jobs, not just SUCCESS status&#10;                // if (cloudlet.getCloudletStatus() != Cloudlet.SUCCESS) {&#10;                //     continue;&#10;                // }&#10;                &#10;                Job job = (Job) cloudlet;&#10;                &#10;                // Get job properties&#10;                double taskLength = job.getCloudletLength();&#10;                int numParents = job.getParentList().size();&#10;                int vmId = cloudlet.getVmId();&#10;                &#10;                // Find the VM this job was assigned to&#10;                Vm vm = null;&#10;                for (Vm v : vmList) {&#10;                    if (v.getId() == vmId) {&#10;                        vm = v;&#10;                        break;&#10;                    }&#10;                }&#10;                &#10;                // Skip if we can't find the VM&#10;                if (vm == null) {&#10;                    Log.printLine(&quot;Warning: VM #&quot; + vmId + &quot; not found for job #&quot; + job.getCloudletId());&#10;                    invalidInstances++;&#10;                    continue;&#10;                }&#10;                &#10;                // Calculate execution time - use a fallback if needed&#10;                double execTime = 0.0;&#10;                if (job.getFinishTime() &gt; 0 &amp;&amp; job.getExecStartTime() &gt; 0) {&#10;                    execTime = job.getFinishTime() - job.getExecStartTime();&#10;                } else {&#10;                    // Fallback: Estimate execution time based on task length and VM MIPS&#10;                    execTime = taskLength / (vm.getMips() * vm.getNumberOfPes());&#10;                    Log.printLine(&quot;Using estimated execution time for job #&quot; + job.getCloudletId());&#10;                }&#10;                &#10;                if (execTime &lt;= 0) {&#10;                    // Another fallback: use cloudlet length directly (scaled)&#10;                    execTime = taskLength / 1000.0;&#10;                    Log.printLine(&quot;Using scaled task length as execution time for job #&quot; + job.getCloudletId());&#10;                }&#10;                &#10;                // Get VM properties&#10;                double vmMips = vm.getMips();&#10;                int vmPes = vm.getNumberOfPes();&#10;                &#10;                // Write to ARFF&#10;                writer.write(dft.format(taskLength) + &quot;,&quot; + &#10;                             numParents + &quot;,&quot; + &#10;                             dft.format(vmMips) + &quot;,&quot; + &#10;                             vmPes + &quot;,&quot; + &#10;                             dft.format(execTime) + &quot;\n&quot;);&#10;                validInstances++;&#10;            }&#10;            &#10;            Log.printLine(&quot;ARFF generation results:&quot;);&#10;            Log.printLine(&quot; - Valid instances written: &quot; + validInstances);&#10;            Log.printLine(&quot; - Invalid instances skipped: &quot; + invalidInstances);&#10;            Log.printLine(&quot;Successfully created training_data.arff file.&quot;);&#10;        } catch (Exception e) {&#10;            Log.printLine(&quot;Error creating ARFF file: &quot; + e.getMessage());&#10;            e.printStackTrace();&#10;        }&#10;    }&#10;&#10;    /**&#10;     * Creates a Datacenter.&#10;     * @param name The name of the datacenter.&#10;     * @return The created WorkflowDatacenter.&#10;     */&#10;    protected static WorkflowDatacenter createDatacenter(String name) {&#10;        List&lt;Host&gt; hostList = new ArrayList&lt;&gt;();&#10;        int mips = 1000;&#10;        int ram = 16384; // Increased RAM per host&#10;        long storage = 1000000;&#10;        int bw = 10000;&#10;        int hostNum = Math.max(Parameters.getVmNum() * 2, 10); // More hosts than VMs&#10;        int pesPerHost = 4; // Multiple cores per host&#10;&#10;        for (int i = 0; i &lt; hostNum; i++) {&#10;            List&lt;Pe&gt; peList = new ArrayList&lt;&gt;();&#10;            for (int j = 0; j &lt; pesPerHost; j++) {&#10;                peList.add(new Pe(j, new PeProvisionerSimple(mips)));&#10;            }&#10;            hostList.add(new Host(i, new RamProvisionerSimple(ram), new BwProvisionerSimple(bw), storage, peList, new VmSchedulerTimeShared(peList)));&#10;        }&#10;        String arch = &quot;x86&quot;;&#10;        String os = &quot;Linux&quot;;&#10;        String vmm = &quot;Xen&quot;;&#10;        double time_zone = 10.0;&#10;        double cost = 3.0;&#10;        double costPerMem = 0.05;&#10;        double costPerStorage = 0.001;&#10;        double costPerBw = 0.0;&#10;        DatacenterCharacteristics characteristics = new DatacenterCharacteristics(arch, os, vmm, hostList, time_zone, cost, costPerMem, costPerStorage, costPerBw);&#10;        WorkflowDatacenter datacenter = null;&#10;        try {&#10;            datacenter = new WorkflowDatacenter(name, characteristics, new VmAllocationPolicySimple(hostList), new LinkedList&lt;&gt;(), 0);&#10;        } catch (Exception e) {&#10;            e.printStackTrace();&#10;        }&#10;        return datacenter;&#10;    }&#10;&#10;    /**&#10;     * Creates a list of virtual machines.&#10;     * @param userId The user ID.&#10;     * @param vms The number of VMs.&#10;     * @return A list of CondorVMs.&#10;     */&#10;    protected static List&lt;CondorVM&gt; createVM(int userId, int vms) {&#10;        LinkedList&lt;CondorVM&gt; list = new LinkedList&lt;&gt;();&#10;        long size = 10000;&#10;        int ram = 2048; // Increased RAM per VM&#10;        int mips = 1000;&#10;        long bw = 1000;&#10;        int pesNumber = 2; // Give each VM 2 processing elements&#10;        String vmm = &quot;Xen&quot;;&#10;        for (int i = 0; i &lt; vms; i++) {&#10;            double ratio = 1.0;&#10;            CondorVM vm = new CondorVM(i, userId, mips * ratio, pesNumber, ram, bw, size, vmm, new CloudletSchedulerSpaceShared());&#10;            list.add(vm);&#10;        }&#10;        return list;&#10;    }&#10;}&#10;" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/WorkflowSim-1.0/sources/org/workflowsim/WorkflowPlanner.java">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/WorkflowSim-1.0/sources/org/workflowsim/WorkflowPlanner.java" />
              <option name="originalContent" value="/**&#10; * Copyright 2012-2013 University Of Southern California&#10; *&#10; * Licensed under the Apache License, Version 2.0 (the &quot;License&quot;); you may not&#10; * use this file except in compliance with the License. You may obtain a copy of&#10; * the License at&#10; *&#10; * http://www.apache.org/licenses/LICENSE-2.0&#10; *&#10; * Unless required by applicable law or agreed to in writing, software&#10; * distributed under the License is distributed on an &quot;AS IS&quot; BASIS, WITHOUT&#10; * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the&#10; * License for the specific language governing permissions and limitations under&#10; * the License.&#10; */&#10;package org.workflowsim;&#10;&#10;import java.util.ArrayList;&#10;import java.util.List;&#10;import org.cloudbus.cloudsim.Log;&#10;import org.cloudbus.cloudsim.core.CloudSimTags;&#10;import org.cloudbus.cloudsim.core.SimEntity;&#10;import org.cloudbus.cloudsim.core.SimEvent;&#10;import org.workflowsim.planning.BasePlanningAlgorithm;&#10;import org.workflowsim.planning.DHEFTPlanningAlgorithm;&#10;import org.workflowsim.planning.HEFTPlanningAlgorithm;&#10;import org.workflowsim.planning.PredictiveHEFTPlanner;&#10;import org.workflowsim.planning.RandomPlanningAlgorithm;&#10;import org.workflowsim.utils.Parameters;&#10;import org.workflowsim.utils.Parameters.PlanningAlgorithm;&#10;&#10;/**&#10; * WorkflowPlanner supports dynamic planning. In the future we will have global&#10; * and static algorithm here. The WorkflowSim starts from WorkflowPlanner. It&#10; * picks up a planning algorithm based on the configuration&#10; *&#10; * @author Weiwei Chen&#10; * @since WorkflowSim Toolkit 1.0&#10; * @date Apr 9, 2013&#10; *&#10; */&#10;public final class WorkflowPlanner extends SimEntity {&#10;&#10;    /**&#10;     * The task list.&#10;     */&#10;    protected List&lt; Task&gt; taskList;&#10;    /**&#10;     * The workflow parser.&#10;     */&#10;    protected WorkflowParser parser;&#10;    /**&#10;     * The associated clustering engine.&#10;     */&#10;    private int clusteringEngineId;&#10;    private ClusteringEngine clusteringEngine;&#10;&#10;    /**&#10;     * Created a new WorkflowPlanner object.&#10;     *&#10;     * @param name name to be associated with this entity (as required by&#10;&#10;     * @throws Exception the exception&#10;     * @pre name != null&#10;     * @post $none&#10;     */&#10;    public WorkflowPlanner(String name) throws Exception {&#10;        this(name, 1);&#10;    }&#10;&#10;    public WorkflowPlanner(String name, int schedulers) throws Exception {&#10;        super(name);&#10;&#10;        setTaskList(new ArrayList&lt;&gt;());&#10;        this.clusteringEngine = new ClusteringEngine(name + &quot;_Merger_&quot;, schedulers);&#10;        this.clusteringEngineId = this.clusteringEngine.getId();&#10;        this.parser = new WorkflowParser(getClusteringEngine().getWorkflowEngine().getSchedulerId(0));&#10;&#10;    }&#10;&#10;    /**&#10;     * Gets the clustering engine id&#10;     *&#10;     * @return clustering engine id&#10;     */&#10;    public int getClusteringEngineId() {&#10;        return this.clusteringEngineId;&#10;    }&#10;&#10;    /**&#10;     * Gets the clustering engine&#10;     *&#10;     * @return the clustering engine&#10;     */&#10;    public ClusteringEngine getClusteringEngine() {&#10;        return this.clusteringEngine;&#10;    }&#10;&#10;    /**&#10;     * Gets the workflow parser&#10;     *&#10;     * @return the workflow parser&#10;     */&#10;    public WorkflowParser getWorkflowParser() {&#10;        return this.parser;&#10;    }&#10;&#10;    /**&#10;     * Gets the workflow engine id&#10;     *&#10;     * @return the workflow engine id&#10;     */&#10;    public int getWorkflowEngineId() {&#10;        return getClusteringEngine().getWorkflowEngineId();&#10;    }&#10;&#10;    /**&#10;     * Gets the workflow engine&#10;     *&#10;     * @return the workflow engine&#10;     */&#10;    public WorkflowEngine getWorkflowEngine() {&#10;        return getClusteringEngine().getWorkflowEngine();&#10;    }&#10;&#10;    /**&#10;     * Processes events available for this Broker.&#10;     *&#10;     * @param ev a SimEvent object&#10;     * @pre ev != null&#10;     * @post $none&#10;     */&#10;    @Override&#10;    public void processEvent(SimEvent ev) {&#10;        switch (ev.getTag()) {&#10;            case WorkflowSimTags.START_SIMULATION:&#10;                getWorkflowParser().parse();&#10;                setTaskList(getWorkflowParser().getTaskList());&#10;                processPlanning();&#10;                processImpactFactors(getTaskList());&#10;                sendNow(getClusteringEngineId(), WorkflowSimTags.JOB_SUBMIT, getTaskList());&#10;                break;&#10;            case CloudSimTags.END_OF_SIMULATION:&#10;                shutdownEntity();&#10;                break;&#10;            // other unknown tags are processed by this method&#10;            default:&#10;                processOtherEvent(ev);&#10;                break;&#10;        }&#10;    }&#10;&#10;    private void processPlanning() {&#10;        if (Parameters.getPlanningAlgorithm().equals(PlanningAlgorithm.INVALID)) {&#10;            return;&#10;        }&#10;        BasePlanningAlgorithm planner = getPlanningAlgorithm(Parameters.getPlanningAlgorithm());&#10;        &#10;        planner.setTaskList(getTaskList());&#10;        planner.setVmList(getWorkflowEngine().getAllVmList());&#10;        try {&#10;            Log.printLine(&quot;HEFT planner running with &quot; + getTaskList().size() + &quot; tasks.&quot;);&#10;            planner.run();&#10;            Log.printLine(&quot;Planning completed. Tasks scheduled.&quot;);&#10;        } catch (Exception e) {&#10;            Log.printLine(&quot;Error in configuring scheduler_method&quot;);&#10;            e.printStackTrace();&#10;        }&#10;    }&#10;&#10;    /**&#10;&#10;     *&#10;     * @param name the SCHMethod name&#10;     * @return the scheduler that extends BaseScheduler&#10;     */&#10;    public BasePlanningAlgorithm getPlanningAlgorithm(PlanningAlgorithm name) {&#10;        BasePlanningAlgorithm planner;&#10;&#10;&#10;        //Parameters.java&#10;        switch (name) {&#10;&#10;            case INVALID:&#10;                planner = null;&#10;                break;&#10;            case RANDOM:&#10;                planner = new RandomPlanningAlgorithm();&#10;                break;&#10;            case HEFT:&#10;                planner = new HEFTPlanningAlgorithm();&#10;                break;&#10;            case PREDICTIVE_HEFT: // ADD THIS CASE BLOCK&#10;                planner = new PredictiveHEFTPlanner();&#10;                break;&#10;            case DHEFT:&#10;                planner = new DHEFTPlanningAlgorithm();&#10;                break;&#10;            default:&#10;                planner = null;&#10;                break;&#10;        }&#10;        return planner;&#10;    }&#10;&#10;    /**&#10;     * Add impact factor for each task. This is useful in task balanced&#10;&#10;     *&#10;     * @param taskList all the tasks&#10;     */&#10;    private void processImpactFactors(List&lt;Task&gt; taskList) {&#10;        List&lt;Task&gt; exits = new ArrayList&lt;&gt;();&#10;        for (Task task : taskList) {&#10;            if (task.getChildList().isEmpty()) {&#10;                exits.add(task);&#10;            }&#10;        }&#10;        double avg = 1.0 / exits.size();&#10;        for (Task task : exits) {&#10;            addImpact(task, avg);&#10;        }&#10;    }&#10;&#10;    /**&#10;     * Add impact factor for one particular task&#10;     *&#10;     * @param task, the task&#10;     * @param impact , the impact factor&#10;     */&#10;    private void addImpact(Task task, double impact) {&#10;&#10;        task.setImpact(task.getImpact() + impact);&#10;        int size = task.getParentList().size();&#10;        if (size &gt; 0) {&#10;            double avg = impact / size;&#10;            for (Task parent : task.getParentList()) {&#10;                addImpact(parent, avg);&#10;            }&#10;        }&#10;    }&#10;&#10;    /**&#10;     * Overrides this method when making a new and different type of Broker.&#10;&#10;     *&#10;     * @param ev a SimEvent object&#10;     * @pre ev != null&#10;     * @post $none&#10;     */&#10;    protected void processOtherEvent(SimEvent ev) {&#10;        if (ev == null) {&#10;            Log.printLine(getName() + &quot;.processOtherEvent(): &quot; + &quot;Error - an event is null.&quot;);&#10;            return;&#10;        }&#10;&#10;        Log.printLine(getName() + &quot;.processOtherEvent(): &quot;&#10;                + &quot;Error - event unknown by this DatacenterBroker.&quot;);&#10;    }&#10;&#10;    /**&#10;     * Send an internal event communicating the end of the simulation.&#10;     *&#10;     * @pre $none&#10;     * @post $none&#10;     */&#10;    protected void finishExecution() {&#10;        //sendNow(getId(), CloudSimTags.END_OF_SIMULATION);&#10;    }&#10;&#10;    /*&#10;     * (non-Javadoc)&#10;     * @see cloudsim.core.SimEntity#shutdownEntity()&#10;     */&#10;    @Override&#10;    public void shutdownEntity() {&#10;        Log.printLine(getName() + &quot; is shutting down...&quot;);&#10;    }&#10;&#10;    /*&#10;     * (non-Javadoc)&#10;     * @see cloudsim.core.SimEntity#startEntity()&#10;     */&#10;    @Override&#10;    public void startEntity() {&#10;        Log.printLine(&quot;Starting WorkflowSim &quot; + Parameters.getVersion());&#10;        Log.printLine(getName() + &quot; is starting...&quot;);&#10;        schedule(getId(), 0, WorkflowSimTags.START_SIMULATION);&#10;    }&#10;&#10;    /**&#10;     * Gets the task list.&#10;     *&#10;     * @return the task list&#10;     */&#10;    @SuppressWarnings(&quot;unchecked&quot;)&#10;    public List&lt;Task&gt; getTaskList() {&#10;        return (List&lt;Task&gt;) taskList;&#10;    }&#10;&#10;    /**&#10;     * Sets the task list.&#10;     *&#10;     * @param taskList&#10;     */&#10;    protected void setTaskList(List&lt;Task&gt; taskList) {&#10;        this.taskList = taskList;&#10;    }&#10;}&#10;" />
              <option name="updatedContent" value="/**&#10; * Copyright 2012-2013 University Of Southern California&#10; *&#10; * Licensed under the Apache License, Version 2.0 (the &quot;License&quot;); you may not&#10; * use this file except in compliance with the License. You may obtain a copy of&#10; * the License at&#10; *&#10; * http://www.apache.org/licenses/LICENSE-2.0&#10; *&#10; * Unless required by applicable law or agreed to in writing, software&#10; * distributed under the License is distributed on an &quot;AS IS&quot; BASIS, WITHOUT&#10; * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the&#10; * License for the specific language governing permissions and limitations under&#10; * the License.&#10; */&#10;package org.workflowsim;&#10;&#10;import java.util.ArrayList;&#10;import java.util.List;&#10;import org.cloudbus.cloudsim.Log;&#10;import org.cloudbus.cloudsim.core.CloudSimTags;&#10;import org.cloudbus.cloudsim.core.SimEntity;&#10;import org.cloudbus.cloudsim.core.SimEvent;&#10;import org.workflowsim.planning.BasePlanningAlgorithm;&#10;import org.workflowsim.planning.DHEFTPlanningAlgorithm;&#10;import org.workflowsim.planning.HEFTPlanningAlgorithm;&#10;import org.workflowsim.planning.PredictiveHEFTPlanner;&#10;import org.workflowsim.planning.RandomPlanningAlgorithm;&#10;import org.workflowsim.utils.Parameters;&#10;import org.workflowsim.utils.Parameters.PlanningAlgorithm;&#10;&#10;/**&#10; * WorkflowPlanner supports dynamic planning. In the future we will have global&#10; * and static algorithm here. The WorkflowSim starts from WorkflowPlanner. It&#10; * picks up a planning algorithm based on the configuration&#10; *&#10; * @author Weiwei Chen&#10; * @since WorkflowSim Toolkit 1.0&#10; * @date Apr 9, 2013&#10; *&#10; */&#10;public final class WorkflowPlanner extends SimEntity {&#10;&#10;    /**&#10;     * The task list.&#10;     */&#10;    protected List&lt; Task&gt; taskList;&#10;    /**&#10;     * The workflow parser.&#10;     */&#10;    protected WorkflowParser parser;&#10;    /**&#10;     * The associated clustering engine.&#10;     */&#10;    private int clusteringEngineId;&#10;    private ClusteringEngine clusteringEngine;&#10;&#10;    /**&#10;     * Created a new WorkflowPlanner object.&#10;     *&#10;     * @param name name to be associated with this entity (as required by&#10;&#10;     * @throws Exception the exception&#10;     * @pre name != null&#10;     * @post $none&#10;     */&#10;    public WorkflowPlanner(String name) throws Exception {&#10;        this(name, 1);&#10;    }&#10;&#10;    public WorkflowPlanner(String name, int schedulers) throws Exception {&#10;        super(name);&#10;&#10;        setTaskList(new ArrayList&lt;&gt;());&#10;        this.clusteringEngine = new ClusteringEngine(name + &quot;_Merger_&quot;, schedulers);&#10;        this.clusteringEngineId = this.clusteringEngine.getId();&#10;        this.parser = new WorkflowParser(getClusteringEngine().getWorkflowEngine().getSchedulerId(0));&#10;&#10;    }&#10;&#10;    /**&#10;     * Gets the clustering engine id&#10;     *&#10;     * @return clustering engine id&#10;     */&#10;    public int getClusteringEngineId() {&#10;        return this.clusteringEngineId;&#10;    }&#10;&#10;    /**&#10;     * Gets the clustering engine&#10;     *&#10;     * @return the clustering engine&#10;     */&#10;    public ClusteringEngine getClusteringEngine() {&#10;        return this.clusteringEngine;&#10;    }&#10;&#10;    /**&#10;     * Gets the workflow parser&#10;     *&#10;     * @return the workflow parser&#10;     */&#10;    public WorkflowParser getWorkflowParser() {&#10;        return this.parser;&#10;    }&#10;&#10;    /**&#10;     * Gets the workflow engine id&#10;     *&#10;     * @return the workflow engine id&#10;     */&#10;    public int getWorkflowEngineId() {&#10;        return getClusteringEngine().getWorkflowEngineId();&#10;    }&#10;&#10;    /**&#10;     * Gets the workflow engine&#10;     *&#10;     * @return the workflow engine&#10;     */&#10;    public WorkflowEngine getWorkflowEngine() {&#10;        return getClusteringEngine().getWorkflowEngine();&#10;    }&#10;&#10;    /**&#10;     * Processes events available for this Broker.&#10;     *&#10;     * @param ev a SimEvent object&#10;     * @pre ev != null&#10;     * @post $none&#10;     */&#10;    @Override&#10;    public void processEvent(SimEvent ev) {&#10;        switch (ev.getTag()) {&#10;            case WorkflowSimTags.START_SIMULATION:&#10;                getWorkflowParser().parse();&#10;                setTaskList(getWorkflowParser().getTaskList());&#10;                Log.printLine(&quot;WorkflowPlanner: Parsed &quot; + getTaskList().size() + &quot; tasks from DAX.&quot;);&#10;                processPlanning();&#10;                processImpactFactors(getTaskList());&#10;                sendNow(getClusteringEngineId(), WorkflowSimTags.JOB_SUBMIT, getTaskList());&#10;                Log.printLine(&quot;WorkflowPlanner: Submitted &quot; + getTaskList().size() + &quot; tasks to clustering engine.&quot;);&#10;                break;&#10;            case CloudSimTags.END_OF_SIMULATION:&#10;                shutdownEntity();&#10;                break;&#10;            // other unknown tags are processed by this method&#10;            default:&#10;                processOtherEvent(ev);&#10;                break;&#10;        }&#10;    }&#10;&#10;    private void processPlanning() {&#10;        if (Parameters.getPlanningAlgorithm().equals(PlanningAlgorithm.INVALID)) {&#10;            return;&#10;        }&#10;        BasePlanningAlgorithm planner = getPlanningAlgorithm(Parameters.getPlanningAlgorithm());&#10;        &#10;        planner.setTaskList(getTaskList());&#10;        planner.setVmList(getWorkflowEngine().getAllVmList());&#10;        try {&#10;            Log.printLine(&quot;HEFT planner running with &quot; + getTaskList().size() + &quot; tasks.&quot;);&#10;            planner.run();&#10;            Log.printLine(&quot;Planning completed. Tasks scheduled.&quot;);&#10;        } catch (Exception e) {&#10;            Log.printLine(&quot;Error in configuring scheduler_method&quot;);&#10;            e.printStackTrace();&#10;        }&#10;    }&#10;&#10;    /**&#10;&#10;     *&#10;     * @param name the SCHMethod name&#10;     * @return the scheduler that extends BaseScheduler&#10;     */&#10;    public BasePlanningAlgorithm getPlanningAlgorithm(PlanningAlgorithm name) {&#10;        BasePlanningAlgorithm planner;&#10;&#10;&#10;        //Parameters.java&#10;        switch (name) {&#10;&#10;            case INVALID:&#10;                planner = null;&#10;                break;&#10;            case RANDOM:&#10;                planner = new RandomPlanningAlgorithm();&#10;                break;&#10;            case HEFT:&#10;                planner = new HEFTPlanningAlgorithm();&#10;                break;&#10;            case PREDICTIVE_HEFT: // ADD THIS CASE BLOCK&#10;                planner = new PredictiveHEFTPlanner();&#10;                break;&#10;            case DHEFT:&#10;                planner = new DHEFTPlanningAlgorithm();&#10;                break;&#10;            default:&#10;                planner = null;&#10;                break;&#10;        }&#10;        return planner;&#10;    }&#10;&#10;    /**&#10;     * Add impact factor for each task. This is useful in task balanced&#10;&#10;     *&#10;     * @param taskList all the tasks&#10;     */&#10;    private void processImpactFactors(List&lt;Task&gt; taskList) {&#10;        List&lt;Task&gt; exits = new ArrayList&lt;&gt;();&#10;        for (Task task : taskList) {&#10;            if (task.getChildList().isEmpty()) {&#10;                exits.add(task);&#10;            }&#10;        }&#10;        double avg = 1.0 / exits.size();&#10;        for (Task task : exits) {&#10;            addImpact(task, avg);&#10;        }&#10;    }&#10;&#10;    /**&#10;     * Add impact factor for one particular task&#10;     *&#10;     * @param task, the task&#10;     * @param impact , the impact factor&#10;     */&#10;    private void addImpact(Task task, double impact) {&#10;&#10;        task.setImpact(task.getImpact() + impact);&#10;        int size = task.getParentList().size();&#10;        if (size &gt; 0) {&#10;            double avg = impact / size;&#10;            for (Task parent : task.getParentList()) {&#10;                addImpact(parent, avg);&#10;            }&#10;        }&#10;    }&#10;&#10;    /**&#10;     * Overrides this method when making a new and different type of Broker.&#10;&#10;     *&#10;     * @param ev a SimEvent object&#10;     * @pre ev != null&#10;     * @post $none&#10;     */&#10;    protected void processOtherEvent(SimEvent ev) {&#10;        if (ev == null) {&#10;            Log.printLine(getName() + &quot;.processOtherEvent(): &quot; + &quot;Error - an event is null.&quot;);&#10;            return;&#10;        }&#10;&#10;        Log.printLine(getName() + &quot;.processOtherEvent(): &quot;&#10;                + &quot;Error - event unknown by this DatacenterBroker.&quot;);&#10;    }&#10;&#10;    /**&#10;     * Send an internal event communicating the end of the simulation.&#10;     *&#10;     * @pre $none&#10;     * @post $none&#10;     */&#10;    protected void finishExecution() {&#10;        //sendNow(getId(), CloudSimTags.END_OF_SIMULATION);&#10;    }&#10;&#10;    /*&#10;     * (non-Javadoc)&#10;     * @see cloudsim.core.SimEntity#shutdownEntity()&#10;     */&#10;    @Override&#10;    public void shutdownEntity() {&#10;        Log.printLine(getName() + &quot; is shutting down...&quot;);&#10;    }&#10;&#10;    /*&#10;     * (non-Javadoc)&#10;     * @see cloudsim.core.SimEntity#startEntity()&#10;     */&#10;    @Override&#10;    public void startEntity() {&#10;        Log.printLine(&quot;Starting WorkflowSim &quot; + Parameters.getVersion());&#10;        Log.printLine(getName() + &quot; is starting...&quot;);&#10;        schedule(getId(), 0, WorkflowSimTags.START_SIMULATION);&#10;    }&#10;&#10;    /**&#10;     * Gets the task list.&#10;     *&#10;     * @return the task list&#10;     */&#10;    @SuppressWarnings(&quot;unchecked&quot;)&#10;    public List&lt;Task&gt; getTaskList() {&#10;        return (List&lt;Task&gt;) taskList;&#10;    }&#10;&#10;    /**&#10;     * Sets the task list.&#10;     *&#10;     * @param taskList&#10;     */&#10;    protected void setTaskList(List&lt;Task&gt; taskList) {&#10;        this.taskList = taskList;&#10;    }&#10;}" />
            </PendingDiffInfo>
          </value>
        </entry>
      </map>
    </option>
  </component>
</project>