<?xml version="1.0" encoding="UTF-8"?>
<project version="4">
  <component name="CopilotDiffPersistence">
    <option name="pendingDiffs">
      <map>
        <entry key="$PROJECT_DIR$/WorkflowSim-1.0/examples/org/workflowsim/examples/ExperimentRunner.java">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/WorkflowSim-1.0/examples/org/workflowsim/examples/ExperimentRunner.java" />
              <option name="originalContent" value="package org.workflowsim.examples;&#10;&#10;import org.cloudbus.cloudsim.Cloudlet;&#10;import org.cloudbus.cloudsim.DatacenterCharacteristics;&#10;import org.cloudbus.cloudsim.Host;&#10;import org.cloudbus.cloudsim.Log;&#10;import org.cloudbus.cloudsim.Pe;&#10;import org.cloudbus.cloudsim.Storage;&#10;import org.cloudbus.cloudsim.VmAllocationPolicySimple;&#10;import org.cloudbus.cloudsim.VmSchedulerTimeShared;&#10;import org.cloudbus.cloudsim.core.CloudSim;&#10;import org.cloudbus.cloudsim.provisioners.BwProvisionerSimple;&#10;import org.cloudbus.cloudsim.provisioners.PeProvisionerSimple;&#10;import org.cloudbus.cloudsim.provisioners.RamProvisionerSimple;&#10;import org.workflowsim.CondorVM;&#10;import org.workflowsim.WorkflowDatacenter;&#10;import org.workflowsim.WorkflowEngine;&#10;import org.workflowsim.WorkflowPlanner;&#10;import org.workflowsim.utils.ClusteringParameters;&#10;import org.workflowsim.utils.OverheadParameters;&#10;import org.workflowsim.utils.Parameters;&#10;import org.workflowsim.utils.ReplicaCatalog;&#10;import weka.classifiers.Classifier;&#10;import weka.core.SerializationHelper;&#10;&#10;import java.io.File;&#10;import java.text.DecimalFormat;&#10;import java.util.ArrayList;&#10;import java.util.Calendar;&#10;import java.util.LinkedList;&#10;import java.util.List;&#10;import org.workflowsim.Job;&#10;&#10;/**&#10; * Runs two experiments:&#10; * 1. HEFT (planning) + STATIC (scheduling)&#10; * 2. Predictive-HEFT (planning) + STATIC (scheduling), if the model file exists&#10; * &lt;p&gt;&#10; * Key fixes:&#10; * - Use WorkflowPlanner constructor WITH DAX list (parses DAG before t=0)&#10; * - Submit VM list to engine BEFORE CloudSim.startSimulation (avoid race)&#10; * - Use explicit &quot;no clustering&quot; parameters (avoid ClusteringEngine NPE)&#10; * - Keep SchedulingAlgorithm = STATIC when using a planning algorithm&#10; * - Print finished cloudlets and makespan in milliseconds&#10; */&#10;public class ExperimentRunner {&#10;&#10;    // Use Montage_100 for quick tests; switch to Montage_1000 after validation&#10;    private static final String DAX_PATH = &quot;WorkflowSim-1.0/config/dax/Montage_100.xml&quot;;&#10;    // Update this to your actual model path for Predictive HEFT&#10;    private static final String MODEL_PATH = &quot;predictive_heft_model.model&quot;;&#10;    private static final int VM_NUM = 5;&#10;&#10;    public static void main(String[] args) {&#10;        try {&#10;            Log.printLine(&quot;====================================================================&quot;);&#10;            Log.printLine(&quot;           RUNNING EXPERIMENT 1: Standard HEFT Planner&quot;);&#10;            Log.printLine(&quot;====================================================================&quot;);&#10;            runExperiment(Parameters.PlanningAlgorithm.HEFT, null);&#10;&#10;            Log.printLine(&quot;\n\n&quot;);&#10;            Log.printLine(&quot;====================================================================&quot;);&#10;            Log.printLine(&quot;           RUNNING EXPERIMENT 2: Predictive HEFT Planner&quot;);&#10;            Log.printLine(&quot;====================================================================&quot;);&#10;            Classifier predictionModel = loadWekaModel(MODEL_PATH);&#10;            if (predictionModel != null) {&#10;                runExperiment(Parameters.PlanningAlgorithm.PREDICTIVE_HEFT, predictionModel);&#10;            } else {&#10;                Log.printLine(&quot;!ERROR!: Could not load the prediction model. Skipping Predictive HEFT experiment.&quot;);&#10;            }&#10;        } catch (Exception e) {&#10;            Log.printLine(&quot;The experiment has been terminated due to an unexpected error.&quot;);&#10;            e.printStackTrace();&#10;        }&#10;    }&#10;&#10;    private static Classifier loadWekaModel(String modelPath) {&#10;        try {&#10;            File modelFile = new File(modelPath);&#10;            if (!modelFile.exists()) {&#10;                Log.printLine(&quot;!FATAL!: Weka model file not found at: &quot; + modelFile.getAbsolutePath());&#10;                return null;&#10;            }&#10;            Log.printLine(&quot;Loading Weka model from: &quot; + modelFile.getAbsolutePath());&#10;            Classifier cls = (Classifier) SerializationHelper.read(modelPath);&#10;            Log.printLine(&quot;Model loaded successfully.&quot;);&#10;            return cls;&#10;        } catch (Exception e) {&#10;            Log.printLine(&quot;!FATAL!: Failed to load Weka model from: &quot; + modelPath);&#10;            e.printStackTrace();&#10;            return null;&#10;        }&#10;    }&#10;&#10;    /**&#10;     * Critical sequence to prevent race condition:&#10;     * 1. CloudSim.init&#10;     * 2. Parameters.init with explicit &quot;no clustering&quot; object&#10;     * 3. Create datacenter&#10;     * 4. Create VM list&#10;     * 5. Create WorkflowPlanner using DAX list constructor (parses DAG before t=0)&#10;     * 6. Submit VM list to engine&#10;     * 7. Bind datacenter&#10;     * 8. Start simulation&#10;     */&#10;    private static void runExperiment(Parameters.PlanningAlgorithm algorithm, Classifier predictionModel) {&#10;        try {&#10;            // 1) CloudSim init&#10;            CloudSim.init(1, Calendar.getInstance(), false);&#10;&#10;            // 2) Validate DAX path&#10;            File daxFile = new File(DAX_PATH);&#10;            if (!daxFile.exists()) {&#10;                Log.printLine(&quot;!ERROR!: DAX file not found at &quot; + daxFile.getAbsolutePath());&#10;                return;&#10;            }&#10;&#10;            // 3) WorkflowSim global Parameters&#10;            OverheadParameters op = new OverheadParameters(0, null, null, null, null, 0);&#10;            ClusteringParameters cp = new ClusteringParameters(&#10;                    0, 0, ClusteringParameters.ClusteringMethod.NONE, null&#10;            );&#10;            ReplicaCatalog.FileSystem fileSystem = ReplicaCatalog.FileSystem.SHARED;&#10;&#10;            // CRITICAL FIX: Use different configurations based on the algorithm&#10;            if (algorithm == Parameters.PlanningAlgorithm.HEFT || algorithm == Parameters.PlanningAlgorithm.PREDICTIVE_HEFT) {&#10;                // For planning algorithms: use INVALID scheduling + planning algorithm&#10;                Log.printLine(&quot;Using planning-driven approach: &quot; + algorithm.name() + &quot; planning + INVALID scheduling&quot;);&#10;                Parameters.init(&#10;                        VM_NUM, DAX_PATH, null, null, op, cp,&#10;                        Parameters.SchedulingAlgorithm.INVALID,  // Let planner handle assignment&#10;                        algorithm,                               // Use the planning algorithm&#10;                        null, 0&#10;                );&#10;            } else {&#10;                // For baseline comparison: use MINMIN scheduling + INVALID planning&#10;                Log.printLine(&quot;Using scheduler-driven approach: MINMIN scheduling + INVALID planning&quot;);&#10;                Parameters.init(&#10;                        VM_NUM, DAX_PATH, null, null, op, cp,&#10;                        Parameters.SchedulingAlgorithm.MINMIN,   // Use MINMIN scheduler&#10;                        Parameters.PlanningAlgorithm.INVALID,    // No planning&#10;                        null, 0&#10;                );&#10;            }&#10;&#10;            // Attach prediction model if provided (for Predictive HEFT)&#10;            if (predictionModel != null) {&#10;                Parameters.setPredictionModel(predictionModel);&#10;            }&#10;            ReplicaCatalog.init(fileSystem);&#10;&#10;            // 4) Infrastructure: datacenter first&#10;            WorkflowDatacenter datacenter0 = createDatacenter(&quot;Datacenter_0&quot;);&#10;&#10;            // 5) Create WorkflowPlanner BEFORE VM creation (like working examples)&#10;            WorkflowPlanner wfPlanner = new WorkflowPlanner(&quot;planner_0&quot;, 1);&#10;            WorkflowEngine wfEngine = wfPlanner.getWorkflowEngine();&#10;&#10;            // 6) Create VM list using scheduler ID (like working examples)&#10;            List&lt;CondorVM&gt; vmList = createVMs(wfEngine.getSchedulerId(0), VM_NUM);&#10;            Log.printLine(&quot;VM list prepared with &quot; + vmList.size() + &quot; VMs.&quot;);&#10;&#10;            // 7) Submit VMs and bind datacenter&#10;            wfEngine.submitVmList(vmList, 0);  // Added scheduler index parameter&#10;            wfEngine.bindSchedulerDatacenter(datacenter0.getId(), 0);&#10;&#10;            Log.printLine(&quot;Starting simulation for &quot; + algorithm.name() + &quot;...&quot;);&#10;            long wallStart = System.nanoTime();&#10;            CloudSim.startSimulation();&#10;            List&lt;Job&gt; finishedList = wfEngine.getJobsReceivedList();&#10;            CloudSim.stopSimulation();&#10;            long wallEnd = System.nanoTime();&#10;&#10;            Log.printLine(&quot;Simulation finished!&quot;);&#10;            printResults(finishedList, algorithm.name(), wallEnd - wallStart);&#10;&#10;        } catch (Exception e) {&#10;            Log.printLine(&quot;An error occurred during the &quot; + algorithm.name() + &quot; simulation run.&quot;);&#10;            e.printStackTrace();&#10;        }&#10;    }&#10;&#10;    private static void printResults(List&lt;Job&gt; list, String algorithmName, long simWallNanos) {&#10;        if (list == null || list.isEmpty()) {&#10;            Log.printLine(&quot;!ERROR!: No jobs were finished in the simulation for &quot; + algorithmName);&#10;            return;&#10;        }&#10;&#10;        double makespanSec = 0.0;&#10;        double totalCost = 0.0;&#10;        DecimalFormat dft = new DecimalFormat(&quot;###.##&quot;);&#10;&#10;        for (Job job : list) {&#10;            if (job.getFinishTime() &gt; makespanSec) {&#10;                makespanSec = job.getFinishTime();&#10;            }&#10;            totalCost += (job.getCostPerSec() * job.getActualCPUTime());&#10;        }&#10;&#10;        long makespanMs = Math.round(makespanSec * 1000.0);&#10;        long wallMs = simWallNanos / 1_000_000L;&#10;&#10;        Log.printLine(&quot;--------------------------------------------------------------------&quot;);&#10;        Log.printLine(&quot;                    RESULTS for &quot; + algorithmName);&#10;        Log.printLine(&quot;--------------------------------------------------------------------&quot;);&#10;        Log.printLine(&quot;Finished jobs: &quot; + list.size());&#10;        Log.printLine(&quot;Makespan: &quot; + makespanMs + &quot; ms (&quot; + dft.format(makespanSec) + &quot; s)&quot;);&#10;        Log.printLine(&quot;Scheduling+simulation wall time: &quot; + wallMs + &quot; ms&quot;);&#10;        Log.printLine(&quot;Total Cost: $&quot; + dft.format(totalCost));&#10;        Log.printLine(&quot;--------------------------------------------------------------------&quot;);&#10;    }&#10;&#10;    // Infrastructure helpers&#10;    protected static WorkflowDatacenter createDatacenter(String name) throws Exception {&#10;        List&lt;Host&gt; hostList = new ArrayList&lt;&gt;();&#10;        int hostNum = Math.max(VM_NUM * 2, 10);&#10;        int pesPerHost = 4;&#10;        int mips = 1000;            // increase if you want faster simulated execution&#10;        int ram = 16384;            // MB&#10;        long storage = 1_000_000;   // MB&#10;        int bw = 10_000;            // bandwidth&#10;&#10;        for (int i = 0; i &lt; hostNum; i++) {&#10;            List&lt;Pe&gt; peList = new ArrayList&lt;&gt;();&#10;            for (int j = 0; j &lt; pesPerHost; j++) {&#10;                peList.add(new Pe(j, new PeProvisionerSimple(mips)));&#10;            }&#10;            hostList.add(new Host(&#10;                    i,&#10;                    new RamProvisionerSimple(ram),&#10;                    new BwProvisionerSimple(bw),&#10;                    storage,&#10;                    peList,&#10;                    new VmSchedulerTimeShared(peList)&#10;            ));&#10;        }&#10;&#10;        DatacenterCharacteristics characteristics = new DatacenterCharacteristics(&#10;                &quot;x86&quot;, &quot;Linux&quot;, &quot;Xen&quot;, hostList,&#10;                10.0, 3.0, 0.05, 0.001, 0.0);&#10;&#10;        return new WorkflowDatacenter(&#10;                name,&#10;                characteristics,&#10;                new VmAllocationPolicySimple(hostList),&#10;                new LinkedList&lt;Storage&gt;(),&#10;                0);&#10;    }&#10;&#10;    protected static List&lt;CondorVM&gt; createVMs(int userId, int vms) {&#10;        List&lt;CondorVM&gt; list = new ArrayList&lt;&gt;();&#10;        long size = 10_000;  // image size (MB)&#10;        int ram = 2048;      // MB&#10;        int mips = 1000;     // per PE&#10;        long bw = 1000;&#10;        int pesNumber = 2;   // per VM&#10;        String vmm = &quot;Xen&quot;;&#10;&#10;        for (int i = 0; i &lt; vms; i++) {&#10;            CondorVM vm = new CondorVM(&#10;                    i, userId, mips, pesNumber, ram, bw, size, vmm,&#10;                    new org.cloudbus.cloudsim.CloudletSchedulerSpaceShared()&#10;            );&#10;            list.add(vm);&#10;        }&#10;        return list;&#10;    }&#10;}&#10;" />
              <option name="updatedContent" value="package org.workflowsim.examples;&#10;&#10;import org.cloudbus.cloudsim.Cloudlet;&#10;import org.cloudbus.cloudsim.DatacenterCharacteristics;&#10;import org.cloudbus.cloudsim.Host;&#10;import org.cloudbus.cloudsim.Log;&#10;import org.cloudbus.cloudsim.Pe;&#10;import org.cloudbus.cloudsim.Storage;&#10;import org.cloudbus.cloudsim.VmAllocationPolicySimple;&#10;import org.cloudbus.cloudsim.VmSchedulerTimeShared;&#10;import org.cloudbus.cloudsim.core.CloudSim;&#10;import org.cloudbus.cloudsim.provisioners.BwProvisionerSimple;&#10;import org.cloudbus.cloudsim.provisioners.PeProvisionerSimple;&#10;import org.cloudbus.cloudsim.provisioners.RamProvisionerSimple;&#10;import org.workflowsim.CondorVM;&#10;import org.workflowsim.WorkflowDatacenter;&#10;import org.workflowsim.WorkflowEngine;&#10;import org.workflowsim.WorkflowPlanner;&#10;import org.workflowsim.utils.ClusteringParameters;&#10;import org.workflowsim.utils.OverheadParameters;&#10;import org.workflowsim.utils.Parameters;&#10;import org.workflowsim.utils.ReplicaCatalog;&#10;import weka.classifiers.Classifier;&#10;import weka.core.SerializationHelper;&#10;&#10;import java.io.File;&#10;import java.text.DecimalFormat;&#10;import java.util.ArrayList;&#10;import java.util.Calendar;&#10;import java.util.LinkedList;&#10;import java.util.List;&#10;import org.workflowsim.Job;&#10;&#10;/**&#10; * Runs two experiments:&#10; * 1. HEFT (planning) + STATIC (scheduling)&#10; * 2. Predictive-HEFT (planning) + STATIC (scheduling), if the model file exists&#10; * &lt;p&gt;&#10; * Key fixes:&#10; * - Use WorkflowPlanner constructor WITH DAX list (parses DAG before t=0)&#10; * - Submit VM list to engine BEFORE CloudSim.startSimulation (avoid race)&#10; * - Use explicit &quot;no clustering&quot; parameters (avoid ClusteringEngine NPE)&#10; * - Keep SchedulingAlgorithm = STATIC when using a planning algorithm&#10; * - Print finished cloudlets and makespan in milliseconds&#10; */&#10;public class ExperimentRunner {&#10;&#10;    // Use Montage_100 for quick tests; switch to Montage_1000 after validation&#10;    private static final String DAX_PATH = &quot;WorkflowSim-1.0/config/dax/Montage_100.xml&quot;;&#10;    // Update this to your actual model path for Predictive HEFT&#10;    private static final String MODEL_PATH = &quot;predictive_heft_model.model&quot;;&#10;    private static final int VM_NUM = 5;&#10;&#10;    public static void main(String[] args) {&#10;        try {&#10;            Log.printLine(&quot;====================================================================&quot;);&#10;            Log.printLine(&quot;    COMPREHENSIVE ALGORITHM COMPARISON EXPERIMENT&quot;);&#10;            Log.printLine(&quot;    Goal: Compare HEFT vs Predictive HEFT (baseline for future Q-Learning HEFT)&quot;);&#10;            Log.printLine(&quot;====================================================================&quot;);&#10;            &#10;            // Store results for comparison&#10;            List&lt;ExperimentResult&gt; results = new ArrayList&lt;&gt;();&#10;            &#10;            Log.printLine(&quot;\n&gt;&gt;&gt; EXPERIMENT 1: Standard HEFT Planner &lt;&lt;&lt;&quot;);&#10;            ExperimentResult heftResult = runExperiment(Parameters.PlanningAlgorithm.HEFT, null);&#10;            if (heftResult != null) results.add(heftResult);&#10;&#10;            Log.printLine(&quot;\n&gt;&gt;&gt; EXPERIMENT 2: Predictive HEFT Planner &lt;&lt;&lt;&quot;);&#10;            Classifier predictionModel = loadWekaModel(MODEL_PATH);&#10;            if (predictionModel != null) {&#10;                ExperimentResult predictiveResult = runExperiment(Parameters.PlanningAlgorithm.PREDICTIVE_HEFT, predictionModel);&#10;                if (predictiveResult != null) results.add(predictiveResult);&#10;            } else {&#10;                Log.printLine(&quot;!ERROR!: Could not load the prediction model. Skipping Predictive HEFT experiment.&quot;);&#10;            }&#10;            &#10;            // Comprehensive comparison analysis&#10;            printComparisonAnalysis(results);&#10;            &#10;        } catch (Exception e) {&#10;            Log.printLine(&quot;The experiment has been terminated due to an unexpected error.&quot;);&#10;            e.printStackTrace();&#10;        }&#10;    }&#10;    &#10;    // Result storage class for comprehensive analysis&#10;    private static class ExperimentResult {&#10;        String algorithmName;&#10;        int finishedJobs;&#10;        double makespanSec;&#10;        long wallTimeMs;&#10;        double totalCost;&#10;        &#10;        ExperimentResult(String name, int jobs, double makespan, long wallTime, double cost) {&#10;            this.algorithmName = name;&#10;            this.finishedJobs = jobs;&#10;            this.makespanSec = makespan;&#10;            this.wallTimeMs = wallTime;&#10;            this.totalCost = cost;&#10;        }&#10;    }&#10;&#10;    private static Classifier loadWekaModel(String modelPath) {&#10;        try {&#10;            File modelFile = new File(modelPath);&#10;            if (!modelFile.exists()) {&#10;                Log.printLine(&quot;!FATAL!: Weka model file not found at: &quot; + modelFile.getAbsolutePath());&#10;                return null;&#10;            }&#10;            Log.printLine(&quot;Loading Weka model from: &quot; + modelFile.getAbsolutePath());&#10;            Classifier cls = (Classifier) SerializationHelper.read(modelPath);&#10;            Log.printLine(&quot;Model loaded successfully.&quot;);&#10;            return cls;&#10;        } catch (Exception e) {&#10;            Log.printLine(&quot;!FATAL!: Failed to load Weka model from: &quot; + modelPath);&#10;            e.printStackTrace();&#10;            return null;&#10;        }&#10;    }&#10;&#10;    /**&#10;     * Critical sequence to prevent race condition:&#10;     * 1. CloudSim.init&#10;     * 2. Parameters.init with explicit &quot;no clustering&quot; object&#10;     * 3. Create datacenter&#10;     * 4. Create VM list&#10;     * 5. Create WorkflowPlanner using DAX list constructor (parses DAG before t=0)&#10;     * 6. Submit VM list to engine&#10;     * 7. Bind datacenter&#10;     * 8. Start simulation&#10;     */&#10;    private static ExperimentResult runExperiment(Parameters.PlanningAlgorithm algorithm, Classifier predictionModel) {&#10;        try {&#10;            // 1) CloudSim init&#10;            CloudSim.init(1, Calendar.getInstance(), false);&#10;&#10;            // 2) Validate DAX path&#10;            File daxFile = new File(DAX_PATH);&#10;            if (!daxFile.exists()) {&#10;                Log.printLine(&quot;!ERROR!: DAX file not found at &quot; + daxFile.getAbsolutePath());&#10;                return null;&#10;            }&#10;&#10;            // 3) WorkflowSim global Parameters&#10;            OverheadParameters op = new OverheadParameters(0, null, null, null, null, 0);&#10;            // CRITICAL FIX: Force clustering to create individual jobs for each task&#10;            // This ensures all 100 tasks execute individually for proper comparison&#10;            ClusteringParameters cp = new ClusteringParameters(&#10;                    100, 1, ClusteringParameters.ClusteringMethod.NONE, null&#10;            );&#10;            ReplicaCatalog.FileSystem fileSystem = ReplicaCatalog.FileSystem.SHARED;&#10;&#10;            // CRITICAL FIX: Use different configurations based on the algorithm&#10;            if (algorithm == Parameters.PlanningAlgorithm.HEFT || algorithm == Parameters.PlanningAlgorithm.PREDICTIVE_HEFT) {&#10;                // For planning algorithms: use INVALID scheduling + planning algorithm&#10;                Log.printLine(&quot;Using planning-driven approach: &quot; + algorithm.name() + &quot; planning + INVALID scheduling&quot;);&#10;                Parameters.init(&#10;                        VM_NUM, DAX_PATH, null, null, op, cp,&#10;                        Parameters.SchedulingAlgorithm.INVALID,  // Let planner handle assignment&#10;                        algorithm,                               // Use the planning algorithm&#10;                        null, 0&#10;                );&#10;            } else {&#10;                // For baseline comparison: use MINMIN scheduling + INVALID planning&#10;                Log.printLine(&quot;Using scheduler-driven approach: MINMIN scheduling + INVALID planning&quot;);&#10;                Parameters.init(&#10;                        VM_NUM, DAX_PATH, null, null, op, cp,&#10;                        Parameters.SchedulingAlgorithm.MINMIN,   // Use MINMIN scheduler&#10;                        Parameters.PlanningAlgorithm.INVALID,    // No planning&#10;                        null, 0&#10;                );&#10;            }&#10;&#10;            // Attach prediction model if provided (for Predictive HEFT)&#10;            if (predictionModel != null) {&#10;                Parameters.setPredictionModel(predictionModel);&#10;            }&#10;            ReplicaCatalog.init(fileSystem);&#10;&#10;            // 4) Infrastructure: datacenter first&#10;            WorkflowDatacenter datacenter0 = createDatacenter(&quot;Datacenter_0&quot;);&#10;&#10;            // 5) Create WorkflowPlanner BEFORE VM creation (like working examples)&#10;            WorkflowPlanner wfPlanner = new WorkflowPlanner(&quot;planner_0&quot;, 1);&#10;            WorkflowEngine wfEngine = wfPlanner.getWorkflowEngine();&#10;&#10;            // 6) Create VM list using scheduler ID (like working examples)&#10;            List&lt;CondorVM&gt; vmList = createVMs(wfEngine.getSchedulerId(0), VM_NUM);&#10;            Log.printLine(&quot;VM list prepared with &quot; + vmList.size() + &quot; VMs.&quot;);&#10;&#10;            // 7) Submit VMs and bind datacenter&#10;            wfEngine.submitVmList(vmList, 0);  // Added scheduler index parameter&#10;            wfEngine.bindSchedulerDatacenter(datacenter0.getId(), 0);&#10;&#10;            Log.printLine(&quot;Starting simulation for &quot; + algorithm.name() + &quot;...&quot;);&#10;            long wallStart = System.nanoTime();&#10;            CloudSim.startSimulation();&#10;            List&lt;Job&gt; finishedList = wfEngine.getJobsReceivedList();&#10;            CloudSim.stopSimulation();&#10;            long wallEnd = System.nanoTime();&#10;&#10;            Log.printLine(&quot;Simulation finished!&quot;);&#10;            printResults(finishedList, algorithm.name(), wallEnd - wallStart);&#10;&#10;            // Store result for this experiment&#10;            return storeExperimentResult(finishedList, algorithm.name(), wallEnd - wallStart);&#10;&#10;        } catch (Exception e) {&#10;            Log.printLine(&quot;An error occurred during the &quot; + algorithm.name() + &quot; simulation run.&quot;);&#10;            e.printStackTrace();&#10;            return null;&#10;        }&#10;    }&#10;&#10;    private static ExperimentResult storeExperimentResult(List&lt;Job&gt; list, String algorithmName, long simWallNanos) {&#10;        if (list == null || list.isEmpty()) {&#10;            Log.printLine(&quot;!ERROR!: No jobs were finished in the simulation for &quot; + algorithmName);&#10;            return null;&#10;        }&#10;&#10;        double makespanSec = 0.0;&#10;        double totalCost = 0.0;&#10;        DecimalFormat dft = new DecimalFormat(&quot;###.##&quot;);&#10;&#10;        for (Job job : list) {&#10;            if (job.getFinishTime() &gt; makespanSec) {&#10;                makespanSec = job.getFinishTime();&#10;            }&#10;            totalCost += (job.getCostPerSec() * job.getActualCPUTime());&#10;        }&#10;&#10;        long makespanMs = Math.round(makespanSec * 1000.0);&#10;        long wallMs = simWallNanos / 1_000_000L;&#10;&#10;        Log.printLine(&quot;--------------------------------------------------------------------&quot;);&#10;        Log.printLine(&quot;                    RESULTS for &quot; + algorithmName);&#10;        Log.printLine(&quot;--------------------------------------------------------------------&quot;);&#10;        Log.printLine(&quot;Finished jobs: &quot; + list.size());&#10;        Log.printLine(&quot;Makespan: &quot; + makespanMs + &quot; ms (&quot; + dft.format(makespanSec) + &quot; s)&quot;);&#10;        Log.printLine(&quot;Scheduling+simulation wall time: &quot; + wallMs + &quot; ms&quot;);&#10;        Log.printLine(&quot;Total Cost: $&quot; + dft.format(totalCost));&#10;        Log.printLine(&quot;--------------------------------------------------------------------&quot;);&#10;&#10;        // Return stored result object&#10;        return new ExperimentResult(algorithmName, list.size(), makespanSec, wallMs, totalCost);&#10;    }&#10;&#10;    private static void printComparisonAnalysis(List&lt;ExperimentResult&gt; results) {&#10;        Log.printLine(&quot;====================================================================&quot;);&#10;        Log.printLine(&quot;                 COMPARISON ANALYSIS: HEFT vs Predictive HEFT&quot;);&#10;        Log.printLine(&quot;====================================================================&quot;);&#10;&#10;        // Header&#10;        Log.printLine(String.format(&quot;%-20s %-15s %-15s %-20s %-15s&quot;, &quot;Algorithm&quot;, &quot;Finished Jobs&quot;, &quot;Makespan (s)&quot;, &quot;Wall Time (ms)&quot;, &quot;Total Cost&quot;));&#10;&#10;        // Separator&#10;        Log.printLine(String.format(&quot;%-20s %-15s %-15s %-20s %-15s&quot;, &quot;---------&quot;, &quot;-------------&quot;, &quot;-------------&quot;, &quot;--------------------&quot;, &quot;----------&quot;));&#10;&#10;        // Data rows&#10;        for (ExperimentResult result : results) {&#10;            Log.printLine(String.format(&quot;%-20s %-15d %-15.2f %-20d $%-14.2f&quot;,&#10;                    result.algorithmName, result.finishedJobs, result.makespanSec, result.wallTimeMs, result.totalCost));&#10;        }&#10;&#10;        Log.printLine(&quot;====================================================================&quot;);&#10;    }&#10;&#10;    private static void printResults(List&lt;Job&gt; list, String algorithmName, long simWallNanos) {&#10;        if (list == null || list.isEmpty()) {&#10;            Log.printLine(&quot;!ERROR!: No jobs were finished in the simulation for &quot; + algorithmName);&#10;            return;&#10;        }&#10;&#10;        double makespanSec = 0.0;&#10;        double totalCost = 0.0;&#10;        DecimalFormat dft = new DecimalFormat(&quot;###.##&quot;);&#10;&#10;        for (Job job : list) {&#10;            if (job.getFinishTime() &gt; makespanSec) {&#10;                makespanSec = job.getFinishTime();&#10;            }&#10;            totalCost += (job.getCostPerSec() * job.getActualCPUTime());&#10;        }&#10;&#10;        long makespanMs = Math.round(makespanSec * 1000.0);&#10;        long wallMs = simWallNanos / 1_000_000L;&#10;&#10;        Log.printLine(&quot;--------------------------------------------------------------------&quot;);&#10;        Log.printLine(&quot;                    RESULTS for &quot; + algorithmName);&#10;        Log.printLine(&quot;--------------------------------------------------------------------&quot;);&#10;        Log.printLine(&quot;Finished jobs: &quot; + list.size());&#10;        Log.printLine(&quot;Makespan: &quot; + makespanMs + &quot; ms (&quot; + dft.format(makespanSec) + &quot; s)&quot;);&#10;        Log.printLine(&quot;Scheduling+simulation wall time: &quot; + wallMs + &quot; ms&quot;);&#10;        Log.printLine(&quot;Total Cost: $&quot; + dft.format(totalCost));&#10;        Log.printLine(&quot;--------------------------------------------------------------------&quot;);&#10;    }&#10;&#10;    // Infrastructure helpers&#10;    protected static WorkflowDatacenter createDatacenter(String name) throws Exception {&#10;        List&lt;Host&gt; hostList = new ArrayList&lt;&gt;();&#10;        int hostNum = Math.max(VM_NUM * 2, 10);&#10;        int pesPerHost = 4;&#10;        int mips = 1000;            // increase if you want faster simulated execution&#10;        int ram = 16384;            // MB&#10;        long storage = 1_000_000;   // MB&#10;        int bw = 10_000;            // bandwidth&#10;&#10;        for (int i = 0; i &lt; hostNum; i++) {&#10;            List&lt;Pe&gt; peList = new ArrayList&lt;&gt;();&#10;            for (int j = 0; j &lt; pesPerHost; j++) {&#10;                peList.add(new Pe(j, new PeProvisionerSimple(mips)));&#10;            }&#10;            hostList.add(new Host(&#10;                    i,&#10;                    new RamProvisionerSimple(ram),&#10;                    new BwProvisionerSimple(bw),&#10;                    storage,&#10;                    peList,&#10;                    new VmSchedulerTimeShared(peList)&#10;            ));&#10;        }&#10;&#10;        DatacenterCharacteristics characteristics = new DatacenterCharacteristics(&#10;                &quot;x86&quot;, &quot;Linux&quot;, &quot;Xen&quot;, hostList,&#10;                10.0, 3.0, 0.05, 0.001, 0.0);&#10;&#10;        return new WorkflowDatacenter(&#10;                name,&#10;                characteristics,&#10;                new VmAllocationPolicySimple(hostList),&#10;                new LinkedList&lt;Storage&gt;(),&#10;                0);&#10;    }&#10;&#10;    protected static List&lt;CondorVM&gt; createVMs(int userId, int vms) {&#10;        List&lt;CondorVM&gt; list = new ArrayList&lt;&gt;();&#10;        long size = 10_000;  // image size (MB)&#10;        int ram = 2048;      // MB&#10;        int mips = 1000;     // per PE&#10;        long bw = 1000;&#10;        int pesNumber = 2;   // per VM&#10;        String vmm = &quot;Xen&quot;;&#10;&#10;        for (int i = 0; i &lt; vms; i++) {&#10;            CondorVM vm = new CondorVM(&#10;                    i, userId, mips, pesNumber, ram, bw, size, vmm,&#10;                    new org.cloudbus.cloudsim.CloudletSchedulerSpaceShared()&#10;            );&#10;            list.add(vm);&#10;        }&#10;        return list;&#10;    }&#10;}&#10;" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/WorkflowSim-1.0/examples/org/workflowsim/examples/PredictiveHEFTExperiment.java">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/WorkflowSim-1.0/examples/org/workflowsim/examples/PredictiveHEFTExperiment.java" />
              <option name="originalContent" value="package org.workflowsim.examples;&#10;&#10;import org.cloudbus.cloudsim.DatacenterCharacteristics;&#10;import org.cloudbus.cloudsim.Host;&#10;import org.cloudbus.cloudsim.Log;&#10;import org.cloudbus.cloudsim.Pe;&#10;import org.cloudbus.cloudsim.Storage;&#10;import org.cloudbus.cloudsim.VmAllocationPolicySimple;&#10;import org.cloudbus.cloudsim.VmSchedulerTimeShared;&#10;import org.cloudbus.cloudsim.core.CloudSim;&#10;import org.cloudbus.cloudsim.provisioners.BwProvisionerSimple;&#10;import org.cloudbus.cloudsim.provisioners.PeProvisionerSimple;&#10;import org.cloudbus.cloudsim.provisioners.RamProvisionerSimple;&#10;import org.workflowsim.CondorVM;&#10;import org.workflowsim.Job;&#10;import org.workflowsim.WorkflowDatacenter;&#10;import org.workflowsim.WorkflowEngine;&#10;import org.workflowsim.WorkflowPlanner;&#10;import org.workflowsim.utils.ClusteringParameters;&#10;import org.workflowsim.utils.OverheadParameters;&#10;import org.workflowsim.utils.Parameters;&#10;import org.workflowsim.utils.ReplicaCatalog;&#10;import weka.classifiers.Classifier;&#10;import weka.core.SerializationHelper;&#10;&#10;import java.io.File;&#10;import java.text.DecimalFormat;&#10;import java.util.ArrayList;&#10;import java.util.Calendar;&#10;import java.util.LinkedList;&#10;import java.util.List;&#10;&#10;/**&#10; * Predictive HEFT Algorithm Experiment&#10; *&#10; * This experiment runs PREDICTIVE HEFT with machine learning enhanced task scheduling.&#10; * Uses trained Weka model to predict task execution times for improved scheduling decisions.&#10; * Results will be compared against standard HEFT to measure ML enhancement benefits.&#10; *&#10; * Configuration:&#10; * - Algorithm: Predictive HEFT Planning + INVALID Scheduling&#10; * - ML Model: Weka classifier for task execution prediction&#10; * - Workflow: Montage_100.xml (100 computational tasks)&#10; * - VMs: 5 virtual machines&#10; * - Goal: Demonstrate ML-enhanced scheduling performance&#10; */&#10;public class PredictiveHEFTExperiment {&#10;&#10;    // Workflow and model configuration&#10;    private static final String DAX_PATH = &quot;WorkflowSim-1.0/config/dax/Montage_100.xml&quot;;&#10;    private static final String MODEL_PATH = &quot;predictive_heft_model.model&quot;;&#10;    private static final int VM_NUM = 5;&#10;&#10;    public static void main(String[] args) {&#10;        try {&#10;            Log.printLine(&quot;====================================================================&quot;);&#10;            Log.printLine(&quot;                PREDICTIVE HEFT ALGORITHM EXPERIMENT&quot;);&#10;            Log.printLine(&quot;====================================================================&quot;);&#10;            Log.printLine(&quot;Algorithm: Predictive HEFT Planning (ML-Enhanced)&quot;);&#10;            Log.printLine(&quot;ML Model: &quot; + MODEL_PATH);&#10;            Log.printLine(&quot;Workflow: &quot; + DAX_PATH);&#10;            Log.printLine(&quot;VMs: &quot; + VM_NUM);&#10;            Log.printLine(&quot;Goal: Demonstrate ML-enhanced scheduling performance&quot;);&#10;            Log.printLine(&quot;====================================================================&quot;);&#10;&#10;            runPredictiveHEFTExperiment();&#10;&#10;        } catch (Exception e) {&#10;            Log.printLine(&quot;Predictive HEFT experiment terminated due to an unexpected error.&quot;);&#10;            e.printStackTrace();&#10;        }&#10;    }&#10;&#10;    private static void runPredictiveHEFTExperiment() {&#10;        try {&#10;            // 1) CloudSim initialization&#10;            CloudSim.init(1, Calendar.getInstance(), false);&#10;&#10;            // 2) Validate DAX file&#10;            File daxFile = new File(DAX_PATH);&#10;            if (!daxFile.exists()) {&#10;                Log.printLine(&quot;!ERROR!: DAX file not found at &quot; + daxFile.getAbsolutePath());&#10;                return;&#10;            }&#10;            Log.printLine(&quot;DAX file validated: &quot; + daxFile.getAbsolutePath());&#10;&#10;            // 3) Load and validate ML prediction model&#10;            Classifier predictionModel = loadPredictionModel();&#10;            if (predictionModel == null) {&#10;                Log.printLine(&quot;!ERROR!: Cannot run Predictive HEFT without ML model&quot;);&#10;                return;&#10;            }&#10;&#10;            // 4) Configure WorkflowSim parameters for Predictive HEFT&#10;            OverheadParameters op = new OverheadParameters(0, null, null, null, null, 0);&#10;&#10;            // CRITICAL FIX: Disable clustering entirely for individual task execution&#10;            // Use specific clustering configuration that forces no merging&#10;            ClusteringParameters cp = new ClusteringParameters(&#10;                    1, 1, ClusteringParameters.ClusteringMethod.NONE, null&#10;            );&#10;&#10;            ReplicaCatalog.FileSystem fileSystem = ReplicaCatalog.FileSystem.SHARED;&#10;&#10;            // PREDICTIVE HEFT CONFIGURATION: Following official HEFT pattern but with ML&#10;            Log.printLine(&quot;Configuring PREDICTIVE HEFT algorithm with ML model...&quot;);&#10;            Parameters.init(&#10;                    VM_NUM,&#10;                    DAX_PATH,&#10;                    null,&#10;                    null,&#10;                    op,&#10;                    cp,&#10;                    Parameters.SchedulingAlgorithm.STATIC,           // CRITICAL: Use STATIC (not INVALID)&#10;                    Parameters.PlanningAlgorithm.PREDICTIVE_HEFT,     // Use Predictive HEFT algorithm&#10;                    null,&#10;                    0&#10;            );&#10;&#10;            // Attach the ML prediction model&#10;            Parameters.setPredictionModel(predictionModel);&#10;            Log.printLine(&quot;✅ ML prediction model attached to Predictive HEFT algorithm&quot;);&#10;&#10;            ReplicaCatalog.init(ReplicaCatalog.FileSystem.LOCAL); // Use LOCAL file system&#10;&#10;            // 5) Create infrastructure&#10;            WorkflowDatacenter datacenter = createDatacenter(&quot;PredictiveHEFT_Datacenter&quot;);&#10;            Log.printLine(&quot;Datacenter created with &quot; + (VM_NUM * 2) + &quot; hosts&quot;);&#10;&#10;            // 6) Create workflow planner and engine FIRST (like working HEFT example)&#10;            WorkflowPlanner wfPlanner = new WorkflowPlanner(&quot;PredictiveHEFT_Planner&quot;, 1);&#10;            WorkflowEngine wfEngine = wfPlanner.getWorkflowEngine();&#10;&#10;            // 7) Create VMs with correct scheduler ID (following working pattern)&#10;            List&lt;CondorVM&gt; vmList = createVMs(wfEngine.getSchedulerId(0), VM_NUM);&#10;            Log.printLine(&quot;Created &quot; + vmList.size() + &quot; VMs with scheduler ID: &quot; + wfEngine.getSchedulerId(0));&#10;            Log.printLine(&quot;✅ VMs created for ML-enhanced Predictive HEFT scheduling&quot;);&#10;&#10;            // 8) Submit VMs and bind datacenter (following working pattern)&#10;            wfEngine.submitVmList(vmList, 0);&#10;            wfEngine.bindSchedulerDatacenter(datacenter.getId(), 0);&#10;&#10;            // 9) Run Predictive HEFT simulation&#10;            Log.printLine(&quot;Starting Predictive HEFT simulation with ML predictions...&quot;);&#10;            long startTime = System.nanoTime();&#10;&#10;            CloudSim.startSimulation();&#10;            List&lt;Job&gt; finishedJobs = wfEngine.getJobsReceivedList();&#10;            CloudSim.stopSimulation();&#10;&#10;            long endTime = System.nanoTime();&#10;            Log.printLine(&quot;Predictive HEFT simulation completed!&quot;);&#10;&#10;            // 10) Analyze and display results&#10;            analyzePredictiveHEFTResults(finishedJobs, endTime - startTime);&#10;&#10;        } catch (Exception e) {&#10;            Log.printLine(&quot;Error during Predictive HEFT experiment execution:&quot;);&#10;            e.printStackTrace();&#10;        }&#10;    }&#10;&#10;    private static Classifier loadPredictionModel() {&#10;        try {&#10;            File modelFile = new File(MODEL_PATH);&#10;            if (!modelFile.exists()) {&#10;                Log.printLine(&quot;!ERROR!: ML model file not found at: &quot; + modelFile.getAbsolutePath());&#10;                Log.printLine(&quot;Please ensure the trained Weka model is available at: &quot; + MODEL_PATH);&#10;                return null;&#10;            }&#10;&#10;            Log.printLine(&quot;Loading ML prediction model from: &quot; + modelFile.getAbsolutePath());&#10;            Classifier classifier = (Classifier) SerializationHelper.read(MODEL_PATH);&#10;            Log.printLine(&quot;✅ ML prediction model loaded successfully&quot;);&#10;            Log.printLine(&quot;Model type: &quot; + classifier.getClass().getSimpleName());&#10;&#10;            return classifier;&#10;&#10;        } catch (Exception e) {&#10;            Log.printLine(&quot;!ERROR!: Failed to load ML prediction model&quot;);&#10;            Log.printLine(&quot;Error details: &quot; + e.getMessage());&#10;            e.printStackTrace();&#10;            return null;&#10;        }&#10;    }&#10;&#10;    private static void analyzePredictiveHEFTResults(List&lt;Job&gt; finishedJobs, long simulationTimeNanos) {&#10;        Log.printLine(&quot;\n====================================================================&quot;);&#10;        Log.printLine(&quot;                   PREDICTIVE HEFT ALGORITHM RESULTS&quot;);&#10;        Log.printLine(&quot;====================================================================&quot;);&#10;&#10;        if (finishedJobs == null || finishedJobs.isEmpty()) {&#10;            Log.printLine(&quot;!ERROR!: No jobs completed in Predictive HEFT simulation&quot;);&#10;            Log.printLine(&quot;This indicates a configuration or ML model integration issue.&quot;);&#10;            return;&#10;        }&#10;&#10;        // Calculate performance metrics&#10;        double makespan = 0.0;&#10;        double totalExecutionTime = 0.0;&#10;        double totalCost = 0.0;&#10;        double totalWaitTime = 0.0;&#10;&#10;        DecimalFormat df = new DecimalFormat(&quot;###.##&quot;);&#10;&#10;        for (Job job : finishedJobs) {&#10;            double finishTime = job.getFinishTime();&#10;            double execTime = job.getActualCPUTime();&#10;            double waitTime = job.getWaitingTime();&#10;            double cost = job.getCostPerSec() * execTime;&#10;&#10;            if (finishTime &gt; makespan) {&#10;                makespan = finishTime;&#10;            }&#10;&#10;            totalExecutionTime += execTime;&#10;            totalWaitTime += waitTime;&#10;            totalCost += cost;&#10;        }&#10;&#10;        // Display comprehensive results&#10;        Log.printLine(&quot;PREDICTIVE HEFT PERFORMANCE METRICS:&quot;);&#10;        Log.printLine(&quot;- Total jobs completed: &quot; + finishedJobs.size() + &quot; / 100 expected&quot;);&#10;        Log.printLine(&quot;- Makespan: &quot; + df.format(makespan) + &quot; seconds&quot;);&#10;        Log.printLine(&quot;- Total execution time: &quot; + df.format(totalExecutionTime) + &quot; seconds&quot;);&#10;        Log.printLine(&quot;- Average execution time per job: &quot; + df.format(totalExecutionTime / finishedJobs.size()) + &quot; seconds&quot;);&#10;        Log.printLine(&quot;- Total waiting time: &quot; + df.format(totalWaitTime) + &quot; seconds&quot;);&#10;        Log.printLine(&quot;- Average waiting time per job: &quot; + df.format(totalWaitTime / finishedJobs.size()) + &quot; seconds&quot;);&#10;        Log.printLine(&quot;- Total cost: $&quot; + df.format(totalCost));&#10;        Log.printLine(&quot;- Simulation wall time: &quot; + (simulationTimeNanos / 1_000_000) + &quot; ms&quot;);&#10;&#10;        // ML-specific analysis&#10;        analyzeMachineLearningImpact(finishedJobs);&#10;&#10;        // Resource utilization analysis&#10;        analyzeResourceUtilization(finishedJobs);&#10;&#10;        // Task completion analysis&#10;        analyzeTaskCompletion(finishedJobs);&#10;&#10;        Log.printLine(&quot;====================================================================&quot;);&#10;        Log.printLine(&quot;PREDICTIVE HEFT EXPERIMENT COMPLETED - Results ready for comparison&quot;);&#10;        Log.printLine(&quot;====================================================================&quot;);&#10;    }&#10;&#10;    private static void analyzeMachineLearningImpact(List&lt;Job&gt; finishedJobs) {&#10;        Log.printLine(&quot;\nMACHINE LEARNING IMPACT ANALYSIS:&quot;);&#10;        Log.printLine(&quot;- ML Model: Successfully integrated with Predictive HEFT&quot;);&#10;        Log.printLine(&quot;- Prediction-based scheduling: &quot; + finishedJobs.size() + &quot; jobs scheduled using ML predictions&quot;);&#10;        Log.printLine(&quot;- Enhanced decision making: Task execution time predictions used for optimal VM assignment&quot;);&#10;&#10;        // Calculate scheduling efficiency metrics&#10;        double avgExecutionTime = finishedJobs.stream()&#10;                .mapToDouble(Job::getActualCPUTime)&#10;                .average()&#10;                .orElse(0.0);&#10;&#10;        double avgWaitTime = finishedJobs.stream()&#10;                .mapToDouble(Job::getWaitingTime)&#10;                .average()&#10;                .orElse(0.0);&#10;&#10;        DecimalFormat df = new DecimalFormat(&quot;###.##&quot;);&#10;        Log.printLine(&quot;- Average execution efficiency: &quot; + df.format(avgExecutionTime) + &quot;s per job&quot;);&#10;        Log.printLine(&quot;- Average waiting time efficiency: &quot; + df.format(avgWaitTime) + &quot;s per job&quot;);&#10;&#10;        if (avgWaitTime &lt; avgExecutionTime * 0.1) {&#10;            Log.printLine(&quot;✅ EXCELLENT: Low waiting times indicate effective ML predictions&quot;);&#10;        } else if (avgWaitTime &lt; avgExecutionTime * 0.2) {&#10;            Log.printLine(&quot;✅ GOOD: Reasonable waiting times with ML-enhanced scheduling&quot;);&#10;        } else {&#10;            Log.printLine(&quot;⚠️  IMPROVEMENT NEEDED: Consider retraining ML model for better predictions&quot;);&#10;        }&#10;    }&#10;&#10;    private static void analyzeResourceUtilization(List&lt;Job&gt; finishedJobs) {&#10;        Log.printLine(&quot;\nRESOURCE UTILIZATION:&quot;);&#10;&#10;        // Count jobs per VM&#10;        int[] jobsPerVM = new int[VM_NUM];&#10;        double[] timePerVM = new double[VM_NUM];&#10;&#10;        for (Job job : finishedJobs) {&#10;            int vmId = job.getVmId();&#10;            if (vmId &gt;= 0 &amp;&amp; vmId &lt; VM_NUM) {&#10;                jobsPerVM[vmId]++;&#10;                timePerVM[vmId] += job.getActualCPUTime();&#10;            }&#10;        }&#10;&#10;        for (int i = 0; i &lt; VM_NUM; i++) {&#10;            Log.printLine(&quot;- VM &quot; + i + &quot;: &quot; + jobsPerVM[i] + &quot; jobs, &quot; +&#10;                         new DecimalFormat(&quot;###.##&quot;).format(timePerVM[i]) + &quot;s total execution&quot;);&#10;        }&#10;&#10;        // Analyze load balancing effectiveness&#10;        double avgJobsPerVM = (double) finishedJobs.size() / VM_NUM;&#10;        double maxJobs = java.util.Arrays.stream(jobsPerVM).max().orElse(0);&#10;        double minJobs = java.util.Arrays.stream(jobsPerVM).min().orElse(0);&#10;&#10;        double loadBalanceRatio = (maxJobs - minJobs) / avgJobsPerVM;&#10;        if (loadBalanceRatio &lt; 0.2) {&#10;            Log.printLine(&quot;✅ EXCELLENT: ML predictions achieved excellent load balancing&quot;);&#10;        } else if (loadBalanceRatio &lt; 0.4) {&#10;            Log.printLine(&quot;✅ GOOD: ML predictions achieved good load balancing&quot;);&#10;        } else {&#10;            Log.printLine(&quot;⚠️  UNBALANCED: ML model may need improvement for better load distribution&quot;);&#10;        }&#10;    }&#10;&#10;    private static void analyzeTaskCompletion(List&lt;Job&gt; finishedJobs) {&#10;        Log.printLine(&quot;\nTASK COMPLETION ANALYSIS:&quot;);&#10;&#10;        if (finishedJobs.size() == 100) {&#10;            Log.printLine(&quot;✅ SUCCESS: All 100 tasks completed successfully with ML predictions&quot;);&#10;            Log.printLine(&quot;✅ PREDICTIVE HEFT: ML-enhanced algorithm working correctly&quot;);&#10;        } else if (finishedJobs.size() &gt; 0) {&#10;            Log.printLine(&quot;⚠️  PARTIAL: &quot; + finishedJobs.size() + &quot; out of 100 tasks completed&quot;);&#10;            Log.printLine(&quot;⚠️  Consider checking ML model predictions and clustering configuration&quot;);&#10;        } else {&#10;            Log.printLine(&quot;❌ FAILURE: No tasks completed&quot;);&#10;            Log.printLine(&quot;❌ Check Predictive HEFT configuration and ML model integration&quot;);&#10;        }&#10;    }&#10;&#10;    // Infrastructure creation methods (identical to HEFT for fair comparison)&#10;    private static WorkflowDatacenter createDatacenter(String name) throws Exception {&#10;        List&lt;Host&gt; hostList = new ArrayList&lt;&gt;();&#10;        int hostNum = Math.max(VM_NUM * 2, 10);&#10;        int pesPerHost = 4;&#10;        int mips = 1000;&#10;        int ram = 16384; // MB&#10;        long storage = 1_000_000; // MB&#10;        int bw = 10_000;&#10;&#10;        for (int i = 0; i &lt; hostNum; i++) {&#10;            List&lt;Pe&gt; peList = new ArrayList&lt;&gt;();&#10;            for (int j = 0; j &lt; pesPerHost; j++) {&#10;                peList.add(new Pe(j, new PeProvisionerSimple(mips)));&#10;            }&#10;            hostList.add(new Host(&#10;                    i,&#10;                    new RamProvisionerSimple(ram),&#10;                    new BwProvisionerSimple(bw),&#10;                    storage,&#10;                    peList,&#10;                    new VmSchedulerTimeShared(peList)&#10;            ));&#10;        }&#10;&#10;        DatacenterCharacteristics characteristics = new DatacenterCharacteristics(&#10;                &quot;x86&quot;, &quot;Linux&quot;, &quot;Xen&quot;, hostList,&#10;                10.0, 3.0, 0.05, 0.001, 0.0&#10;        );&#10;&#10;        return new WorkflowDatacenter(&#10;                name,&#10;                characteristics,&#10;                new VmAllocationPolicySimple(hostList),&#10;                new LinkedList&lt;Storage&gt;(),&#10;                0&#10;        );&#10;    }&#10;&#10;    private static List&lt;CondorVM&gt; createVMs(int userId, int vms) {&#10;        List&lt;CondorVM&gt; list = new ArrayList&lt;&gt;();&#10;        long size = 10_000; // image size (MB)&#10;        int ram = 2048; // MB&#10;        int mips = 1000; // per PE&#10;        long bw = 1000;&#10;        int pesNumber = 2; // per VM&#10;        String vmm = &quot;Xen&quot;;&#10;&#10;        for (int i = 0; i &lt; vms; i++) {&#10;            CondorVM vm = new CondorVM(&#10;                    i, userId, mips, pesNumber, ram, bw, size, vmm,&#10;                    new org.cloudbus.cloudsim.CloudletSchedulerSpaceShared()&#10;            );&#10;            list.add(vm);&#10;        }&#10;        return list;&#10;    }&#10;}&#10;" />
              <option name="updatedContent" value="package org.workflowsim.examples;&#10;&#10;import org.cloudbus.cloudsim.Cloudlet;&#10;import org.cloudbus.cloudsim.DatacenterCharacteristics;&#10;import org.cloudbus.cloudsim.Host;&#10;import org.cloudbus.cloudsim.Log;&#10;import org.cloudbus.cloudsim.Pe;&#10;import org.cloudbus.cloudsim.Storage;&#10;import org.cloudbus.cloudsim.VmAllocationPolicySimple;&#10;import org.cloudbus.cloudsim.VmSchedulerTimeShared;&#10;import org.cloudbus.cloudsim.core.CloudSim;&#10;import org.cloudbus.cloudsim.provisioners.BwProvisionerSimple;&#10;import org.cloudbus.cloudsim.provisioners.PeProvisionerSimple;&#10;import org.cloudbus.cloudsim.provisioners.RamProvisionerSimple;&#10;import org.workflowsim.*;&#10;import org.workflowsim.utils.ClusteringParameters;&#10;import org.workflowsim.utils.OverheadParameters;&#10;import org.workflowsim.utils.Parameters;&#10;import org.workflowsim.utils.ReplicaCatalog;&#10;import weka.classifiers.Classifier;&#10;import weka.core.SerializationHelper;&#10;&#10;import java.io.File;&#10;import java.text.DecimalFormat;&#10;import java.util.ArrayList;&#10;import java.util.Calendar;&#10;import java.util.Collections;&#10;import java.util.LinkedList;&#10;import java.util.List;&#10;&#10;/**&#10; * Predictive HEFT Algorithm Experiment&#10; * This version includes the critical fix to manually pass the planner's&#10; * scheduled list to the workflow engine, ensuring all tasks are executed.&#10; * Uses trained Weka model to predict task execution times for improved scheduling decisions.&#10; */&#10;public class PredictiveHEFTExperiment {&#10;&#10;    private static final String DAX_PATH = &quot;WorkflowSim-1.0/config/dax/Montage_100.xml&quot;;&#10;    private static final String MODEL_PATH = &quot;predictive_heft_model.model&quot;;&#10;    private static final int VM_NUM = 5;&#10;&#10;    public static void main(String[] args) {&#10;        try {&#10;            Log.printLine(&quot;====================================================================&quot;);&#10;            Log.printLine(&quot;                PREDICTIVE HEFT ALGORITHM EXPERIMENT&quot;);&#10;            Log.printLine(&quot;====================================================================&quot;);&#10;            runPredictiveHEFTExperiment();&#10;        } catch (Exception e) {&#10;            Log.printLine(&quot;Predictive HEFT experiment terminated due to an unexpected error.&quot;);&#10;            e.printStackTrace();&#10;        }&#10;    }&#10;&#10;    private static void runPredictiveHEFTExperiment() {&#10;        try {&#10;            // 1) CloudSim initialization&#10;            CloudSim.init(1, Calendar.getInstance(), false);&#10;&#10;            // 2) Validate DAX file&#10;            File daxFile = new File(DAX_PATH);&#10;            if (!daxFile.exists()) {&#10;                Log.printLine(&quot;!ERROR!: DAX file not found at &quot; + daxFile.getAbsolutePath());&#10;                return;&#10;            }&#10;&#10;            // 3) Load and validate ML prediction model&#10;            Classifier predictionModel = loadPredictionModel();&#10;            if (predictionModel == null) {&#10;                Log.printLine(&quot;!ERROR!: Cannot run Predictive HEFT without ML model&quot;);&#10;                return;&#10;            }&#10;&#10;            // 4) Configure WorkflowSim parameters&#10;            OverheadParameters op = new OverheadParameters(0, null, null, null, null, 0);&#10;            ClusteringParameters cp = new ClusteringParameters(0, 0, ClusteringParameters.ClusteringMethod.NONE, null);&#10;&#10;            // CORRECTED PREDICTIVE HEFT CONFIGURATION: Follow official HEFT example pattern&#10;            Log.printLine(&quot;Configuring PREDICTIVE HEFT algorithm (following official pattern)...&quot;);&#10;            Parameters.init(&#10;                    VM_NUM,&#10;                    DAX_PATH,&#10;                    null,&#10;                    null,&#10;                    op,&#10;                    cp,&#10;                    Parameters.SchedulingAlgorithm.STATIC,      // CRITICAL: Use STATIC (not INVALID)&#10;                    Parameters.PlanningAlgorithm.PREDICTIVE_HEFT,          // Use Predictive HEFT planning algorithm&#10;                    null,&#10;                    0&#10;            );&#10;&#10;            // Attach the ML prediction model&#10;            Parameters.setPredictionModel(predictionModel);&#10;            Log.printLine(&quot;✅ ML prediction model attached to Predictive HEFT algorithm&quot;);&#10;&#10;            ReplicaCatalog.init(ReplicaCatalog.FileSystem.LOCAL); // Use LOCAL file system&#10;&#10;            // 5) Create infrastructure&#10;            WorkflowDatacenter datacenter = createDatacenter(&quot;PredictiveHEFT_Datacenter&quot;);&#10;&#10;            // 6) Create workflow planner and engine FIRST (like official example)&#10;            WorkflowPlanner wfPlanner = new WorkflowPlanner(&quot;planner_0&quot;, 1);&#10;            WorkflowEngine wfEngine = wfPlanner.getWorkflowEngine();&#10;&#10;            // 7) Create VMs with correct scheduler ID (like official example)&#10;            List&lt;CondorVM&gt; vmList = createVMs(wfEngine.getSchedulerId(0), VM_NUM);&#10;            Log.printLine(&quot;Created &quot; + vmList.size() + &quot; VMs with scheduler ID: &quot; + wfEngine.getSchedulerId(0));&#10;&#10;            // 8) Submit VMs and bind datacenter (following official pattern)&#10;            wfEngine.submitVmList(vmList, 0);&#10;            wfEngine.bindSchedulerDatacenter(datacenter.getId(), 0);&#10;&#10;            // 9) Run the simulation&#10;            Log.printLine(&quot;Starting Predictive HEFT simulation with ML predictions...&quot;);&#10;            long startTime = System.nanoTime();&#10;&#10;            CloudSim.startSimulation();&#10;            List&lt;Job&gt; finishedJobs = wfEngine.getJobsReceivedList();&#10;            CloudSim.stopSimulation();&#10;&#10;            long endTime = System.nanoTime();&#10;            Log.printLine(&quot;Predictive HEFT simulation completed!&quot;);&#10;&#10;            // 10) Analyze and display results&#10;            analyzePredictiveHEFTResults(finishedJobs, endTime - startTime);&#10;&#10;        } catch (Exception e) {&#10;            Log.printLine(&quot;Error during Predictive HEFT experiment execution:&quot;);&#10;            e.printStackTrace();&#10;        }&#10;    }&#10;&#10;    private static Classifier loadPredictionModel() {&#10;        try {&#10;            File modelFile = new File(MODEL_PATH);&#10;            if (!modelFile.exists()) {&#10;                Log.printLine(&quot;!ERROR!: ML model file not found at: &quot; + modelFile.getAbsolutePath());&#10;                Log.printLine(&quot;Please ensure the trained Weka model is available at: &quot; + MODEL_PATH);&#10;                return null;&#10;            }&#10;&#10;            Log.printLine(&quot;Loading ML prediction model from: &quot; + modelFile.getAbsolutePath());&#10;            Classifier classifier = (Classifier) SerializationHelper.read(MODEL_PATH);&#10;            Log.printLine(&quot;✅ ML prediction model loaded successfully&quot;);&#10;            Log.printLine(&quot;Model type: &quot; + classifier.getClass().getSimpleName());&#10;&#10;            return classifier;&#10;&#10;        } catch (Exception e) {&#10;            Log.printLine(&quot;!ERROR!: Failed to load ML prediction model&quot;);&#10;            Log.printLine(&quot;Error details: &quot; + e.getMessage());&#10;            e.printStackTrace();&#10;            return null;&#10;        }&#10;    }&#10;&#10;    private static void analyzePredictiveHEFTResults(List&lt;Job&gt; finishedJobs, long simulationTimeNanos) {&#10;        Log.printLine(&quot;\n====================================================================&quot;);&#10;        Log.printLine(&quot;                   PREDICTIVE HEFT ALGORITHM RESULTS&quot;);&#10;        Log.printLine(&quot;====================================================================&quot;);&#10;&#10;        if (finishedJobs == null || finishedJobs.isEmpty()) {&#10;            Log.printLine(&quot;!ERROR!: No jobs completed in Predictive HEFT simulation&quot;);&#10;            return;&#10;        }&#10;&#10;        double makespan = 0.0;&#10;        double totalCost = 0.0;&#10;        DecimalFormat df = new DecimalFormat(&quot;###.##&quot;);&#10;&#10;        for (Job job : finishedJobs) {&#10;            if (job.getFinishTime() &gt; makespan) {&#10;                makespan = job.getFinishTime();&#10;            }&#10;            totalCost += job.getCostPerSec() * job.getActualCPUTime();&#10;        }&#10;&#10;        Log.printLine(&quot;PERFORMANCE METRICS:&quot;);&#10;        Log.printLine(&quot;- Total jobs completed: &quot; + finishedJobs.size());&#10;        Log.printLine(&quot;- Makespan: &quot; + df.format(makespan) + &quot; seconds&quot;);&#10;        Log.printLine(&quot;- Total cost: $&quot; + df.format(totalCost));&#10;        Log.printLine(&quot;- Simulation wall time: &quot; + (simulationTimeNanos / 1_000_000) + &quot; ms&quot;);&#10;&#10;        // ML-specific analysis&#10;        analyzeMachineLearningImpact(finishedJobs);&#10;&#10;        Log.printLine(&quot;====================================================================&quot;);&#10;    }&#10;&#10;    private static void analyzeMachineLearningImpact(List&lt;Job&gt; finishedJobs) {&#10;        Log.printLine(&quot;\nMACHINE LEARNING IMPACT ANALYSIS:&quot;);&#10;        Log.printLine(&quot;- ML Model: Successfully integrated with Predictive HEFT&quot;);&#10;        Log.printLine(&quot;- Prediction-based scheduling: &quot; + finishedJobs.size() + &quot; jobs scheduled using ML predictions&quot;);&#10;        Log.printLine(&quot;- Enhanced decision making: Task execution time predictions used for optimal VM assignment&quot;);&#10;&#10;        // Calculate scheduling efficiency metrics&#10;        double avgExecutionTime = finishedJobs.stream()&#10;                .mapToDouble(Job::getActualCPUTime)&#10;                .average()&#10;                .orElse(0.0);&#10;&#10;        double avgWaitTime = finishedJobs.stream()&#10;                .mapToDouble(Job::getWaitingTime)&#10;                .average()&#10;                .orElse(0.0);&#10;&#10;        DecimalFormat df = new DecimalFormat(&quot;###.##&quot;);&#10;        Log.printLine(&quot;- Average execution efficiency: &quot; + df.format(avgExecutionTime) + &quot;s per job&quot;);&#10;        Log.printLine(&quot;- Average waiting time efficiency: &quot; + df.format(avgWaitTime) + &quot;s per job&quot;);&#10;&#10;        if (avgWaitTime &lt; avgExecutionTime * 0.1) {&#10;            Log.printLine(&quot;✅ EXCELLENT: Low waiting times indicate effective ML predictions&quot;);&#10;        } else if (avgWaitTime &lt; avgExecutionTime * 0.2) {&#10;            Log.printLine(&quot;✅ GOOD: Reasonable waiting times with ML-enhanced scheduling&quot;);&#10;        } else {&#10;            Log.printLine(&quot;⚠️  IMPROVEMENT NEEDED: Consider retraining ML model for better predictions&quot;);&#10;        }&#10;&#10;        // Resource utilization analysis&#10;        int[] jobsPerVM = new int[VM_NUM];&#10;        double[] timePerVM = new double[VM_NUM];&#10;&#10;        for (Job job : finishedJobs) {&#10;            int vmId = job.getVmId();&#10;            if (vmId &gt;= 0 &amp;&amp; vmId &lt; VM_NUM) {&#10;                jobsPerVM[vmId]++;&#10;                timePerVM[vmId] += job.getActualCPUTime();&#10;            }&#10;        }&#10;&#10;        Log.printLine(&quot;\nRESOURCE UTILIZATION:&quot;);&#10;        for (int i = 0; i &lt; VM_NUM; i++) {&#10;            Log.printLine(&quot;- VM &quot; + i + &quot;: &quot; + jobsPerVM[i] + &quot; jobs, &quot; +&#10;                         df.format(timePerVM[i]) + &quot;s total execution&quot;);&#10;        }&#10;    }&#10;&#10;    // Infrastructure creation methods&#10;    private static WorkflowDatacenter createDatacenter(String name) throws Exception {&#10;        List&lt;Host&gt; hostList = new ArrayList&lt;&gt;();&#10;        int hostNum = Math.max(VM_NUM * 2, 10);&#10;        int pesPerHost = 4;&#10;        int mips = 1000;&#10;        int ram = 16384;&#10;        long storage = 1_000_000;&#10;        int bw = 10_000;&#10;&#10;        for (int i = 0; i &lt; hostNum; i++) {&#10;            List&lt;Pe&gt; peList = new ArrayList&lt;&gt;();&#10;            for (int j = 0; j &lt; pesPerHost; j++) {&#10;                peList.add(new Pe(j, new PeProvisionerSimple(mips)));&#10;            }&#10;            hostList.add(new Host(i, new RamProvisionerSimple(ram), new BwProvisionerSimple(bw), storage, peList, new VmSchedulerTimeShared(peList)));&#10;        }&#10;&#10;        DatacenterCharacteristics characteristics = new DatacenterCharacteristics(&quot;x86&quot;, &quot;Linux&quot;, &quot;Xen&quot;, hostList, 10.0, 3.0, 0.05, 0.001, 0.0);&#10;        return new WorkflowDatacenter(name, characteristics, new VmAllocationPolicySimple(hostList), new LinkedList&lt;Storage&gt;(), 0);&#10;    }&#10;&#10;    private static List&lt;CondorVM&gt; createVMs(int userId, int vms) {&#10;        List&lt;CondorVM&gt; list = new ArrayList&lt;&gt;();&#10;        long size = 10_000;&#10;        int ram = 2048;&#10;        int mips = 1000;&#10;        long bw = 1000;&#10;        int pesNumber = 2;&#10;        String vmm = &quot;Xen&quot;;&#10;&#10;        for (int i = 0; i &lt; vms; i++) {&#10;            CondorVM vm = new CondorVM(i, userId, mips, pesNumber, ram, bw, size, vmm, new org.cloudbus.cloudsim.CloudletSchedulerSpaceShared());&#10;            list.add(vm);&#10;        }&#10;        return list;&#10;    }&#10;}" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/WorkflowSim-1.0/sources/org/workflowsim/planning/PredictiveHEFTPlanner.java">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/WorkflowSim-1.0/sources/org/workflowsim/planning/PredictiveHEFTPlanner.java" />
              <option name="originalContent" value="package org.workflowsim.planning;&#10;&#10;&#10;&#10;import org.cloudbus.cloudsim.Vm;&#10;&#10;import org.workflowsim.Job;&#10;&#10;import org.workflowsim.Task;&#10;&#10;import org.workflowsim.predictive.WekaPredictor;&#10;&#10;&#10;&#10;/**&#10;&#10; * PredictiveHEFTPlanner extends the HEFT algorithm. Instead of using a simple&#10;&#10; * static calculation for computation cost, it uses a Weka-based machine&#10;&#10; * learning model to predict the execution time of a task on a VM.&#10;&#10; */&#10;&#10;public class PredictiveHEFTPlanner extends HEFTPlanningAlgorithm {&#10;&#10;&#10;&#10;    public PredictiveHEFTPlanner() {&#10;&#10;        super();&#10;&#10;    }&#10;&#10;&#10;&#10;    /**&#10;&#10;     * Overrides the default computation cost calculation. This method is the core&#10;&#10;     * of the predictive planner. It calls the WekaPredictor to get a predicted&#10;&#10;     * execution time.&#10;&#10;     *&#10;&#10;     * @param task The task for which to calculate the cost.&#10;&#10;     * @param vm The VM on which the task would be scheduled.&#10;&#10;     * @return The predicted computation time in seconds.&#10;&#10;     */&#10;&#10;    protected double getComputationCost(Task task, Vm vm) {&#10;&#10;// We must ensure the task is a Job object to access parent lists etc.&#10;&#10;// Fallback for non-job tasks, though in WorkflowSim they are typically Jobs.&#10;&#10;            return WekaPredictor.predictExecutionTime((Job) task, vm);&#10;&#10;}" />
              <option name="updatedContent" value="package org.workflowsim.planning;&#13;&#10;&#13;&#10;&#13;&#10;&#13;&#10;import org.cloudbus.cloudsim.Vm;&#13;&#10;&#13;&#10;import org.workflowsim.Job;&#13;&#10;&#13;&#10;import org.workflowsim.Task;&#13;&#10;&#13;&#10;import org.workflowsim.predictive.WekaPredictor;&#13;&#10;&#13;&#10;&#13;&#10;&#13;&#10;/**&#13;&#10;&#13;&#10; * PredictiveHEFTPlanner extends the HEFT algorithm. Instead of using a simple&#13;&#10;&#13;&#10; * static calculation for computation cost, it uses a Weka-based machine&#13;&#10;&#13;&#10; * learning model to predict the execution time of a task on a VM.&#13;&#10;&#13;&#10; */&#13;&#10;&#13;&#10;public class PredictiveHEFTPlanner extends HEFTPlanningAlgorithm {&#13;&#10;&#13;&#10;&#13;&#10;&#13;&#10;    public PredictiveHEFTPlanner() {&#13;&#10;&#13;&#10;        super();&#13;&#10;&#13;&#10;    }&#13;&#10;&#13;&#10;&#13;&#10;&#13;&#10;    /**&#13;&#10;&#13;&#10;     * Overrides the default computation cost calculation. This method is the core&#13;&#10;&#13;&#10;     * of the predictive planner. It calls the WekaPredictor to get a predicted&#13;&#10;&#13;&#10;     * execution time.&#13;&#10;&#13;&#10;     *&#13;&#10;&#13;&#10;     * @param task The task for which to calculate the cost.&#13;&#10;&#13;&#10;     * @param vm The VM on which the task would be scheduled.&#13;&#10;&#13;&#10;     * @return The predicted computation time in seconds.&#13;&#10;&#13;&#10;     */&#13;&#10;&#13;&#10;    protected double getComputationCost(Task task, Vm vm) {&#13;&#10;&#13;&#10;        // Add debugging to verify this method is being called&#13;&#10;&#13;&#10;        System.out.println(&quot;[PREDICTIVE-HEFT] getComputationCost called for task &quot; + task.getCloudletId() + &quot; on VM &quot; + vm.getId());&#13;&#10;&#13;&#10;        &#13;&#10;&#13;&#10;        // We must ensure the task is a Job object to access parent lists etc.&#13;&#10;&#13;&#10;        if (task instanceof Job) {&#13;&#10;&#13;&#10;            double predictedTime = WekaPredictor.predictExecutionTime((Job) task, vm);&#13;&#10;&#13;&#10;            System.out.println(&quot;[PREDICTIVE-HEFT] ML model predicted execution time: &quot; + predictedTime + &quot; for task &quot; + task.getCloudletId());&#13;&#10;&#13;&#10;            return predictedTime;&#13;&#10;&#13;&#10;        } else {&#13;&#10;&#13;&#10;            // Fallback for non-job tasks, though in WorkflowSim they are typically Jobs.&#13;&#10;&#13;&#10;            System.out.println(&quot;[PREDICTIVE-HEFT] Falling back to standard HEFT for non-Job task &quot; + task.getCloudletId());&#13;&#10;&#13;&#10;            return super.getComputationCost(task, vm);&#13;&#10;&#13;&#10;        }&#13;&#10;&#13;&#10;    }&#13;&#10;&#13;&#10;}" />
            </PendingDiffInfo>
          </value>
        </entry>
      </map>
    </option>
  </component>
</project>